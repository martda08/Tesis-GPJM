{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martda08/Tesis-GPJM/blob/main/4_2_2_Selecci%C3%B3n_del_modelo_(Validaci%C3%B3n_cruzada)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54h-9O1Uxu0A"
      },
      "source": [
        "#Subsección 4.2.1- Selección del modelo-Modelo Conjunto basado en Campos Gaussianos.\n",
        "\n",
        "*Fecha de última modificación*: 10-oct-22.\n",
        "\n",
        "*Tesis*: Modelación de la relación entre el cerebro y el comportamiento mediante campos Gaussianos.\n",
        "\n",
        "*Autor*: Giwon Bahg\n",
        "\n",
        "*Modificado por*: Daniela Martínez Aguirre\n",
        "\n",
        "*Descripción*: Código extraido y modificado del repositorio: https://github.com/MbCN-lab/gpjm y https://github.com/rodrigo-carnier/gpjm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW4P6Hvxxu0G",
        "outputId": "69e9ac03-1070-434d-b864-152a846dfd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpflow==2.5.2 in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (0.8.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (57.4.0)\n",
            "Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (1.7.3)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: lark>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (1.1.2)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from gpflow==2.5.2) (1.2.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch>=0.6->gpflow==2.5.2) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.48.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (14.0.6)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (2.0.7)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow==2.5.2) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow==2.5.2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.4.0->gpflow==2.5.2) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow==2.5.2) (3.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.5.2) (1.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.5.2) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.5.2) (0.1.7)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gpflow==2.5.2) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "#Librerías a utilizar\n",
        "import numpy as np\n",
        "#Versión utilizada agosto 2022 2.5.2\n",
        "!pip install gpflow==2.5.2\n",
        "import gpflow\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from gpflow.utilities import ops, print_summary  \n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qihupfNyy8vw",
        "outputId": "77949ce8-bdaf-4b8c-b456-332b8eaa7bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Conectar con Google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGQnsKIhxu0H"
      },
      "outputs": [],
      "source": [
        "#Time indices\n",
        "#Importa los y los tiempos\n",
        "time_block = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/time_block.npy\")# Duración de cada bloque(son 4 bloques y cada bloque tiene 20 ensayos)\n",
        "ts_dense = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/ts_dense.npy\") # Time indices for behavioral data-cada medio segundo hasta 784\n",
        "ts_sparse = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/ts_sparse.npy\") # Time indices for neural data -cada segundo desde 0 hasta 783"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdCr6iwxu0H",
        "outputId": "a69f60d9-01df-4d73-a63b-f3bd71c16a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "#Time indices for the fixation onset (i.e., beginning of new trials)\n",
        "#Cada cuando empieza cada uno de los 20 ensayos\n",
        "#Va de 39 en 39 hasta 744\n",
        "fix_onset = np.array([  0.,  39.,  78., 117., 156., 195., 235., 274., 313., 353., 393., 431., 469., 508., 547., 587., 627., 665., 705., 744.], dtype=np.int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Ml0hpCxu0I"
      },
      "outputs": [],
      "source": [
        "#Load the coherence information\n",
        "#Sólamente se importa un bloque\n",
        "#Only the variable 'coh1', which corresponds to the first run of the experiment, is used.\n",
        "coh1 = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/coherence_scaled_013_run1.npy\")\n",
        "\n",
        "#Upscale the coherence\n",
        "coh1_ext = np.column_stack([coh1, coh1]).ravel()\n",
        "coh1_ext = coh1_ext[0:(len(coh1_ext)-1)]\n",
        "\n",
        "#La coherencia es reescalada de -1 a 1 en lugar de -.35 a .35\n",
        "coh1 = coh1_ext.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w77d1X6Txu0I",
        "outputId": "66001e47-d26d-44ff-9740-ffd9ba04de22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "#Importar datos de comportamiento\n",
        "# Load the mouse trajectory\n",
        "# Only the variable 'Y_B1' is used.\n",
        "Y_B1_ori = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/mouse_trajectory_centered_scaled_013_run1.npy\")\n",
        "\n",
        "#Logit-transform the behavioral data\n",
        "temp = Y_B1_ori/2 + 0.5\n",
        "temp[np.where(temp == 0)[0]] = 1e-5\n",
        "Y_B1 = np.log(temp/(1-temp))\n",
        "\n",
        "#Logit-transform the coherence\n",
        "temp = coh1/2 + 0.5\n",
        "temp[np.where(temp == 0)[0]] = 1e-5\n",
        "coh1_transformada= np.log(temp/(1-temp))\n",
        "\n",
        "Y_B1_ori = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/mouse_trajectory_centered_scaled_013_run1.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN2VrtiLxu0J"
      },
      "outputs": [],
      "source": [
        "# Load the neural data-sólo el primer bloque\n",
        "Y_N1 = np.load(\"/content/drive/MyDrive/Tesis/Cap4/fMRI_DATOS/Timing013_meanTS_block1.npy\")\n",
        "# Y_N2 = np.load(\"Timing013_meanTS_block2.npy\") # Not used\n",
        "# Y_N3 = np.load(\"Timing013_meanTS_block3.npy\")\n",
        "# Y_N4 = np.load(\"Timing013_meanTS_block4.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROI information: Not used for fitting the model.\n",
        "## Coherence-related regions\n",
        "ss_RC = np.array([[-54, 23, 0], [32, -23, 64], [29, -69, -41], [-48, 48, -14], [-9, -88, 26], [49, -40, 9], [7, -72, -9]], dtype = np.float64)\n",
        "## Behavioral-response-related regions\n",
        "ss_resp = np.array([[66, -3, 22], [54, -14, 6], [-44, -64, 11], [10, -6, 72], [-44, -3, 61], [-52, 0, 32], [60, -25, 13], [-53, -21, 10], [-10, -56, 68]], dtype = np.float64)\n",
        "\n",
        "ss = np.vstack([ss_RC, ss_resp])\n",
        "\n",
        "# ROI labels\n",
        "label_ROI = np.array(['RC1_IFG', 'RC2_PCG', 'RC3_CRUS2', 'RC4_FP', 'RC5_OP', 'RC6_PSG', 'RC7_LG',\n",
        "            'resp1_SSC1', 'resp2_AC1', 'resp3_V5', 'resp4_PMC(SFG)', 'resp5_PMC(MFG)','resp6_MC1', 'resp7_IPL', 'resp8_AC1', 'resp9_SPL'])"
      ],
      "metadata": {
        "id": "xknZfESm3DyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plm5rpBhxu0L"
      },
      "source": [
        "#Definir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzxm7FpFyfLm"
      },
      "outputs": [],
      "source": [
        "#Creación de modelo: \n",
        "#Establece las funciones kernel a utilizar\n",
        "#Inicializa la variable latente\n",
        "#Construye la función objetivo\n",
        "class GPJMv3(gpflow.models.GPR):\n",
        "    def __init__(self, Y_N, Y_B, ts_N, ts_B, n_latent, ss,\n",
        "                 kern_tX = None, mean_tX = None, kern_XN = None, mean_XN = None, kern_XB = None, mean_XB = None, name=None):\n",
        "        if kern_tX is None:\n",
        "            kern_tX = gpflow.kernels.Matern12() \n",
        "        if mean_tX is None:\n",
        "            mean_tX = gpflow.mean_functions.Zero(output_dim =n_latent)\n",
        "        if kern_XN is None:\n",
        "            kern_XN =gpflow.kernels.SquaredExponential() \n",
        "        if mean_XN is None:\n",
        "            mean_XN = gpflow.mean_functions.Zero(output_dim = Y_N.shape[1]) \n",
        "        if kern_XB is None:\n",
        "            kern_XB = gpflow.kernels.Matern12()\n",
        "        if mean_XB is None:\n",
        "            mean_XB = gpflow.mean_functions.Zero(output_dim = Y_B.shape[1])\n",
        "       # super().__init__(name=name)\n",
        "        \n",
        "        def cubic_interpolation(ts_N, Y_N, ts_B, ss):\n",
        "            from scipy import interpolate\n",
        "            yn_new = np.zeros((ts_B.shape[0], ss.shape[0]))\n",
        "            yn_array = Y_N.reshape(ss.shape[0], ts_N.shape[0]).T\n",
        "            for i in range(ss.shape[0]):\n",
        "                temp = interpolate.interp1d(np.squeeze(ts_N), yn_array[:,i], kind='cubic',  fill_value=\"extrapolate\")\n",
        "                yn_new[:,i] = temp(np.squeeze(ts_B))\n",
        "            return yn_new\n",
        "        \n",
        "        def downsizing_scheme_nearest(ts_N, ts_B):\n",
        "            M = np.zeros((ts_B.shape[0], ts_N.shape[0]))\n",
        "            ts = np.squeeze(ts_B)\n",
        "            for i in range(ts_N.shape[0]):\n",
        "                argmin_idx = np.argmin(np.abs(ts - ts_N[i,0]))\n",
        "                M[argmin_idx,i] = 1\n",
        "            return M\n",
        "        \n",
        "        def HRF_filter(ts_B):\n",
        "            ts = np.squeeze(ts_B)\n",
        "            unit_ts = ts[ts <= 30]\n",
        "            def HRFunit(t):\n",
        "                from scipy.special import gamma\n",
        "                a1 = 6 # b1=1\n",
        "                a2 = 16 # b2=1\n",
        "                c = 1./6\n",
        "                part1 = t**(a1-1) * np.exp(-t) / gamma(a1)\n",
        "                part2 = t**(a2-1) * np.exp(-t) / gamma(a2)\n",
        "                return part1 - c * part2\n",
        "            hrf = HRFunit(unit_ts)\n",
        "            return(hrf)\n",
        "        \n",
        "        if len(ts_N) > len(ts_B):\n",
        "            print(\"Neural: Dense / Behavioral: Sparse\")\n",
        "            self.ts = tf.constant(ts_N.copy())\n",
        "        elif len(ts_N) < len(ts_B):\n",
        "            print(\"Neural: Sparse / Behavioral: Dense\")\n",
        "            self.ts = tf.constant(ts_B.copy())\n",
        "            self.ts_B = ts_B.copy()\n",
        "            self.Y_N_interp = cubic_interpolation(ts_N, Y_N, ts_B, ss)\n",
        "            self.M = downsizing_scheme_nearest(ts_N, ts_B)\n",
        "        \n",
        "        # Data\n",
        "        self.Y_N = tf.constant(Y_N.copy())\n",
        "        self.Y_B = tf.constant(Y_B.copy())\n",
        "        self.ts_N = tf.constant(ts_N.copy())\n",
        "        self.ts_B = tf.constant(ts_B.copy())\n",
        "        self.n_Nsample = Y_N.shape[0]\n",
        "        self.n_Nfeature = Y_N.shape[1]\n",
        "        self.n_Bsample = Y_B.shape[0]\n",
        "        self.n_Bfeature = Y_B.shape[1]\n",
        "        \n",
        "        # latent dynamics kernel + downsizing scheme\n",
        "        self.kern_tX = kern_tX\n",
        "        self.mean_tX = mean_tX\n",
        "        self.n_latent = n_latent\n",
        "        self.N_pca = ops.pca_reduce(self.Y_N_interp, n_latent)\n",
        "        self.X = gpflow.Parameter(ops.pca_reduce(self.Y_N_interp, n_latent))\n",
        "        self.X_sparse = ops.pca_reduce(Y_N, n_latent)\n",
        "\n",
        "        # Neural data kernel\n",
        "        self.kern_XN = kern_XN\n",
        "        self.mean_XN = mean_XN\n",
        "        self.hrf = tf.constant(HRF_filter(self.ts_B))\n",
        "        \n",
        "        # Behavioral data kernel\n",
        "        self.kern_XB = kern_XB\n",
        "        self.mean_XB = mean_XB\n",
        "\n",
        "        # Likelihood\n",
        "        self.likelihood_tX = gpflow.likelihoods.Gaussian()\n",
        "        self.likelihood_XN = gpflow.likelihoods.Gaussian()\n",
        "        self.likelihood_XB = gpflow.likelihoods.Gaussian() # Can differ according to the model you rely on.\n",
        "    \n",
        "    #@gpflow.params_as_tensors\n",
        "    def _build_likelihood_tX(self): # Zero-noise model is not supported by GPflow ==> Need to add an infinitesimal noise when initializing the model.\n",
        "        Ktx = self.kern_tX.K(self.ts, self.ts) + tf.eye(tf.shape(self.ts)[0], dtype =tf.dtypes.float64) * self.likelihood_tX.variance\n",
        "        Ltx = tf.linalg.cholesky(Ktx)\n",
        "        mtx = self.mean_tX(self.ts)\n",
        "        logpdf_tx = gpflow.logdensities.multivariate_normal(self.X, mtx, Ltx)\n",
        "        return tf.reduce_sum(logpdf_tx)\n",
        "    \n",
        "    #@gpflow.params_as_tensors\n",
        "    def _build_likelihood_XN(self):\n",
        "        kernel_completo=self.kern_XN.K(self.X, self.X) \n",
        "        kernel_incompleto=tf.matmul(tf.transpose(self.M), tf.matmul(kernel_completo, self.M))\n",
        "        Kxn = kernel_incompleto + tf.eye(self.n_Nsample, dtype =tf.dtypes.float64) * self.likelihood_XN.variance\n",
        "        Lxn = tf.linalg.cholesky(Kxn)  \n",
        "        mxn = self.mean_XN(self.X_sparse)\n",
        "        logpdf_xn = gpflow.logdensities.multivariate_normal(self.Y_N, mxn, Lxn)\n",
        "        return tf.reduce_sum(logpdf_xn)\n",
        "\n",
        "    #@gpflow.params_as_tensors\n",
        "    def _build_likelihood_XB(self):\n",
        "        Kxb = self.kern_XB.K(self.X, self.X) + tf.eye(tf.shape(self.X)[0], dtype = tf.dtypes.float64) * self.likelihood_XB.variance\n",
        "        Lxb = tf.linalg.cholesky(Kxb)\n",
        "        mxb = self.mean_XB(self.X)\n",
        "        logpdf_xb = gpflow.logdensities.multivariate_normal(self.Y_B, mxb, Lxb)\n",
        "        return tf.reduce_sum(logpdf_xb)\n",
        "\n",
        "\n",
        "    def maximum_log_likelihood_objective(self):     # 2022-02 RMC upd03: Abstract function for calculating log_likelihood now is named like this. (Was this the purpose of this function \"_build_likelihood\"?)\n",
        "        with tf.name_scope('likelihood') as scope:  # 2022-02 RMC upd02: This is how name_scopes are defined nowaways.\n",
        "            logpdf_tx = self._build_likelihood_tX()\n",
        "            logpdf_xn = self._build_likelihood_XN()\n",
        "            logpdf_xb = self._build_likelihood_XB()\n",
        "            return tf.reduce_sum(logpdf_tx + logpdf_xn + logpdf_xb)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiylkB79xu0N"
      },
      "outputs": [],
      "source": [
        "#Funciones para hacer predicciones de datos nuevos\n",
        "\n",
        "def recover_Kxn(m, ts_new):\n",
        "    X_new = recover_latent(m, ts_new)\n",
        "    X = m.X.numpy()\n",
        "    ss =m.ss.numpy()\n",
        "    kern_temporal = m.kern_XN.kernel_t(X, X_new).numpy()\n",
        "    kern_spatial = m.kern_XN.kernel_s(ss, ss).numpy()\n",
        "    ts_N =m.ts_N.numpy()\n",
        "        \n",
        "    i, k, s = ss.shape[0], ts_N.shape[0], ts_N.shape[0]\n",
        "    o = s * (i - 1) + k\n",
        "  \n",
        "    Kss = tf.reshape(kern_spatial, [1, i, i, 1])\n",
        "    Ktt = tf.reshape(kern_temporal, [k, k, 1, 1])\n",
        "    Kst = tf.squeeze(tf.nn.conv2d_transpose(Kss, Ktt, (1, o, o, 1), [1, s, s, 1], \"VALID\"))\n",
        "    return Kst\n",
        "\n",
        "#Recuperar la variable latente\n",
        "def recover_latent(m, ts_new):\n",
        "    import tensorflow as tf\n",
        "    from numpy.linalg import inv, cholesky\n",
        "    ts = m.ts.numpy()\n",
        "    Kstar = m.kern_tX(ts, ts_new).numpy()\n",
        "    KttI = (m.kern_tX(ts, ts) + np.eye(ts.shape[0], dtype=np.float64) * m.likelihood_tX.variance).numpy()\n",
        "    X = m.X.numpy()\n",
        "    L = cholesky(KttI)\n",
        "    return Kstar.T.dot(inv(L.T).dot((inv(L)).dot(X)))\n",
        "\n",
        "#Recuperar los datos neuronales\n",
        "def recover_neural(m, ts_new):\n",
        "    import tensorflow as tf\n",
        "    from numpy.linalg import inv, cholesky\n",
        "    m=test\n",
        "    ts_N =m.ts_N.numpy()\n",
        "    Y_N = m.Y_N.numpy()\n",
        "    X_new = recover_latent(m, ts_new)\n",
        "    X = m.X.numpy()[train_index % 2 == 0]\n",
        "    Kstar=m.kern_XN(X, X_new).numpy()\n",
        "    KttI = (m.kern_XN(X, X) + np.eye(len(tss_train), dtype = np.float64) * m.likelihood_XN.variance).numpy()\n",
        "    L = cholesky(KttI)\n",
        "    fmean = Kstar.T.dot(inv(L.T).dot((inv(L)).dot(Y_N)))\n",
        "    v = inv(L).dot(Kstar)\n",
        "    Vstar = m.kern_XN(X_new, X_new).numpy() - v.T.dot(v)+np.eye(X_new.shape[0], dtype = np.float64) * m.likelihood_XN.variance\n",
        "    sd = np.sqrt(np.diag(Vstar))\n",
        "    return fmean, Vstar, sd\n",
        "    \n",
        "#Recuperar los datos de comportamiento\n",
        "def recover_behavioral(m, ts_new):\n",
        "    import tensorflow as tf\n",
        "    from numpy.linalg import inv, cholesky\n",
        "    X_new = recover_latent(m, ts_new)\n",
        "    X = m.X.numpy()\n",
        "    Kstar = m.kern_XB(X, X_new).numpy()\n",
        "    KttI = (m.kern_XB(X, X) + np.eye(len(tsd_train), dtype = np.float64) * m.likelihood_XB.variance).numpy()\n",
        "    Y_B = m.Y_B.numpy()\n",
        "    L = cholesky(KttI)\n",
        "    fmean = Kstar.T.dot(inv(L.T).dot((inv(L)).dot(Y_B)))\n",
        "    v = inv(L).dot(Kstar)\n",
        "    Vstar = m.kern_XB(X_new, X_new).numpy() - v.T.dot(v)+np.eye(X_new.shape[0], dtype = np.float64) * m.likelihood_XB.variance\n",
        "    sd= np.sqrt(np.diag(Vstar))\n",
        "    ci95 = np.column_stack([fmean - 1.96 * np.sqrt(np.diag(Vstar)).reshape(-1,1), fmean + 1.96 * np.sqrt(np.diag(Vstar)).reshape(-1,1)])\n",
        "    return fmean, Vstar, ci95\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_IatAHzxu0O"
      },
      "source": [
        "#Ajuste del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9-Dj83bsR4X"
      },
      "outputs": [],
      "source": [
        "#Centrar y escalar los datos\n",
        "\n",
        "#Hacemos una copia de los datos\n",
        "Y_N1_es=Y_N1.copy()\n",
        "Y_B1_es=Y_B1.copy()\n",
        "\n",
        "#Estandarizamos los datos neuronales\n",
        "Y_N1_es-= np.mean(Y_N1_es, axis=0)\n",
        "Y_N1_es /= np.std(Y_N1_es, axis=0)\n",
        "\n",
        "#Estandarizamos los datos de comportamiento\n",
        "Y_B1_es-= np.mean(Y_B1_es, axis=0)\n",
        "Y_B1_es /= np.std(Y_B1_es, axis=0)\n",
        "\n",
        "#with open('ECM', 'rb') as ECM:\n",
        "#    dataset = pickle.load(ECM)\n",
        "\n",
        "#with open('log', 'rb') as log:\n",
        " #   dataset = pickle.load(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VFr-fR8U0DI",
        "outputId": "a0578168-3411-4940-ac98-2595bc553c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 664 -kfold 0-n_latent 3 Loss:  6761.9543\n",
            "Epoch 665 -kfold 0-n_latent 3 Loss:  6740.4040\n",
            "Epoch 666 -kfold 0-n_latent 3 Loss:  6749.2907\n",
            "Epoch 667 -kfold 0-n_latent 3 Loss:  6802.2947\n",
            "Epoch 668 -kfold 0-n_latent 3 Loss:  6840.1441\n",
            "Epoch 669 -kfold 0-n_latent 3 Loss:  6796.6891\n",
            "Epoch 670 -kfold 0-n_latent 3 Loss:  6766.4414\n",
            "Epoch 671 -kfold 0-n_latent 3 Loss:  6684.0386\n",
            "Epoch 672 -kfold 0-n_latent 3 Loss:  6629.2447\n",
            "Epoch 673 -kfold 0-n_latent 3 Loss:  6597.4525\n",
            "Epoch 674 -kfold 0-n_latent 3 Loss:  6489.8220\n",
            "Epoch 675 -kfold 0-n_latent 3 Loss:  6496.9930\n",
            "Epoch 676 -kfold 0-n_latent 3 Loss:  6483.0923\n",
            "Epoch 677 -kfold 0-n_latent 3 Loss:  6533.1598\n",
            "Epoch 678 -kfold 0-n_latent 3 Loss:  6594.6803\n",
            "Epoch 679 -kfold 0-n_latent 3 Loss:  6550.4849\n",
            "Epoch 680 -kfold 0-n_latent 3 Loss:  6553.5324\n",
            "Epoch 681 -kfold 0-n_latent 3 Loss:  6552.5662\n",
            "Epoch 682 -kfold 0-n_latent 3 Loss:  6434.2920\n",
            "Epoch 683 -kfold 0-n_latent 3 Loss:  6381.4567\n",
            "Epoch 684 -kfold 0-n_latent 3 Loss:  6303.3548\n",
            "Epoch 685 -kfold 0-n_latent 3 Loss:  6336.8547\n",
            "Epoch 686 -kfold 0-n_latent 3 Loss:  6330.6112\n",
            "Epoch 687 -kfold 0-n_latent 3 Loss:  6291.6174\n",
            "Epoch 688 -kfold 0-n_latent 3 Loss:  6317.9724\n",
            "Epoch 689 -kfold 0-n_latent 3 Loss:  6446.3442\n",
            "Epoch 690 -kfold 0-n_latent 3 Loss:  6378.3215\n",
            "Epoch 691 -kfold 0-n_latent 3 Loss:  6398.3652\n",
            "Epoch 692 -kfold 0-n_latent 3 Loss:  6436.7523\n",
            "Epoch 693 -kfold 0-n_latent 3 Loss:  6695.6817\n",
            "Epoch 694 -kfold 0-n_latent 3 Loss:  6672.0412\n",
            "Epoch 695 -kfold 0-n_latent 3 Loss:  6613.2366\n",
            "Epoch 696 -kfold 0-n_latent 3 Loss:  6652.8387\n",
            "Epoch 697 -kfold 0-n_latent 3 Loss:  6621.8535\n",
            "Epoch 698 -kfold 0-n_latent 3 Loss:  6603.1724\n",
            "Epoch 699 -kfold 0-n_latent 3 Loss:  6627.6108\n",
            "4\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 0-n_latent 4 Loss:  23380.2846\n",
            "Epoch 2 -kfold 0-n_latent 4 Loss:  21905.7713\n",
            "Epoch 3 -kfold 0-n_latent 4 Loss:  20611.4174\n",
            "Epoch 4 -kfold 0-n_latent 4 Loss:  19402.0743\n",
            "Epoch 5 -kfold 0-n_latent 4 Loss:  18161.1245\n",
            "Epoch 6 -kfold 0-n_latent 4 Loss:  16870.2043\n",
            "Epoch 7 -kfold 0-n_latent 4 Loss:  15673.7442\n",
            "Epoch 8 -kfold 0-n_latent 4 Loss:  14583.2120\n",
            "Epoch 9 -kfold 0-n_latent 4 Loss:  13622.0859\n",
            "Epoch 10 -kfold 0-n_latent 4 Loss:  12656.6887\n",
            "Epoch 11 -kfold 0-n_latent 4 Loss:  11826.6306\n",
            "Epoch 12 -kfold 0-n_latent 4 Loss:  11152.9450\n",
            "Epoch 13 -kfold 0-n_latent 4 Loss:  10528.6903\n",
            "Epoch 14 -kfold 0-n_latent 4 Loss:  10002.3380\n",
            "Epoch 15 -kfold 0-n_latent 4 Loss:  9599.4217\n",
            "Epoch 16 -kfold 0-n_latent 4 Loss:  9254.6534\n",
            "Epoch 17 -kfold 0-n_latent 4 Loss:  8938.8112\n",
            "Epoch 18 -kfold 0-n_latent 4 Loss:  8573.0799\n",
            "Epoch 19 -kfold 0-n_latent 4 Loss:  8211.1252\n",
            "Epoch 20 -kfold 0-n_latent 4 Loss:  7837.5688\n",
            "Epoch 21 -kfold 0-n_latent 4 Loss:  7546.4170\n",
            "Epoch 22 -kfold 0-n_latent 4 Loss:  7267.7086\n",
            "Epoch 23 -kfold 0-n_latent 4 Loss:  6992.0993\n",
            "Epoch 24 -kfold 0-n_latent 4 Loss:  6706.6319\n",
            "Epoch 25 -kfold 0-n_latent 4 Loss:  6422.5142\n",
            "Epoch 26 -kfold 0-n_latent 4 Loss:  6179.2116\n",
            "Epoch 27 -kfold 0-n_latent 4 Loss:  5922.7980\n",
            "Epoch 28 -kfold 0-n_latent 4 Loss:  5675.8931\n",
            "Epoch 29 -kfold 0-n_latent 4 Loss:  5469.7905\n",
            "Epoch 30 -kfold 0-n_latent 4 Loss:  5246.4879\n",
            "Epoch 31 -kfold 0-n_latent 4 Loss:  5052.5815\n",
            "Epoch 32 -kfold 0-n_latent 4 Loss:  4852.8378\n",
            "Epoch 33 -kfold 0-n_latent 4 Loss:  4725.0542\n",
            "Epoch 34 -kfold 0-n_latent 4 Loss:  4562.7129\n",
            "Epoch 35 -kfold 0-n_latent 4 Loss:  4392.4822\n",
            "Epoch 36 -kfold 0-n_latent 4 Loss:  4250.2377\n",
            "Epoch 37 -kfold 0-n_latent 4 Loss:  4106.9343\n",
            "Epoch 38 -kfold 0-n_latent 4 Loss:  3867.8095\n",
            "Epoch 39 -kfold 0-n_latent 4 Loss:  3787.1697\n",
            "Epoch 40 -kfold 0-n_latent 4 Loss:  3556.0699\n",
            "Epoch 41 -kfold 0-n_latent 4 Loss:  3417.1375\n",
            "Epoch 42 -kfold 0-n_latent 4 Loss:  3249.6865\n",
            "Epoch 43 -kfold 0-n_latent 4 Loss:  3108.4574\n",
            "Epoch 44 -kfold 0-n_latent 4 Loss:  3039.4761\n",
            "Epoch 45 -kfold 0-n_latent 4 Loss:  2840.9885\n",
            "Epoch 46 -kfold 0-n_latent 4 Loss:  2699.7460\n",
            "Epoch 47 -kfold 0-n_latent 4 Loss:  2476.8823\n",
            "Epoch 48 -kfold 0-n_latent 4 Loss:  2370.0102\n",
            "Epoch 49 -kfold 0-n_latent 4 Loss:  2215.8568\n",
            "Epoch 50 -kfold 0-n_latent 4 Loss:  2051.6768\n",
            "Epoch 51 -kfold 0-n_latent 4 Loss:  1970.9418\n",
            "Epoch 52 -kfold 0-n_latent 4 Loss:  1795.8418\n",
            "Epoch 53 -kfold 0-n_latent 4 Loss:  1741.6312\n",
            "Epoch 54 -kfold 0-n_latent 4 Loss:  1641.7212\n",
            "Epoch 55 -kfold 0-n_latent 4 Loss:  1556.0451\n",
            "Epoch 56 -kfold 0-n_latent 4 Loss:  1490.1284\n",
            "Epoch 57 -kfold 0-n_latent 4 Loss:  1462.4827\n",
            "Epoch 58 -kfold 0-n_latent 4 Loss:  1273.4428\n",
            "Epoch 59 -kfold 0-n_latent 4 Loss:  1226.2949\n",
            "Epoch 60 -kfold 0-n_latent 4 Loss:  1078.4445\n",
            "Epoch 61 -kfold 0-n_latent 4 Loss:  1075.3908\n",
            "Epoch 62 -kfold 0-n_latent 4 Loss:  1043.0505\n",
            "Epoch 63 -kfold 0-n_latent 4 Loss:  797.9500\n",
            "Epoch 64 -kfold 0-n_latent 4 Loss:  757.4841\n",
            "Epoch 65 -kfold 0-n_latent 4 Loss:  783.3555\n",
            "Epoch 66 -kfold 0-n_latent 4 Loss:  638.8705\n",
            "Epoch 67 -kfold 0-n_latent 4 Loss:  567.5430\n",
            "Epoch 68 -kfold 0-n_latent 4 Loss:  420.8418\n",
            "Epoch 69 -kfold 0-n_latent 4 Loss:  167.6905\n",
            "Epoch 70 -kfold 0-n_latent 4 Loss: -20.0205\n",
            "Epoch 71 -kfold 0-n_latent 4 Loss: -96.3307\n",
            "Epoch 72 -kfold 0-n_latent 4 Loss: -218.2620\n",
            "Epoch 73 -kfold 0-n_latent 4 Loss: -297.1139\n",
            "Epoch 74 -kfold 0-n_latent 4 Loss: -359.7771\n",
            "Epoch 75 -kfold 0-n_latent 4 Loss: -494.8387\n",
            "Epoch 76 -kfold 0-n_latent 4 Loss: -438.1534\n",
            "Epoch 77 -kfold 0-n_latent 4 Loss: -407.2306\n",
            "Epoch 78 -kfold 0-n_latent 4 Loss: -414.1143\n",
            "Epoch 79 -kfold 0-n_latent 4 Loss: -590.0133\n",
            "Epoch 80 -kfold 0-n_latent 4 Loss: -710.9071\n",
            "Epoch 81 -kfold 0-n_latent 4 Loss: -793.9485\n",
            "Epoch 82 -kfold 0-n_latent 4 Loss: -913.2320\n",
            "Epoch 83 -kfold 0-n_latent 4 Loss: -1045.0459\n",
            "Epoch 84 -kfold 0-n_latent 4 Loss: -1162.6175\n",
            "Epoch 85 -kfold 0-n_latent 4 Loss: -1287.5406\n",
            "Epoch 86 -kfold 0-n_latent 4 Loss: -1242.3304\n",
            "Epoch 87 -kfold 0-n_latent 4 Loss: -1391.2199\n",
            "Epoch 88 -kfold 0-n_latent 4 Loss: -1384.6920\n",
            "Epoch 89 -kfold 0-n_latent 4 Loss: -1523.2515\n",
            "Epoch 90 -kfold 0-n_latent 4 Loss: -1546.5104\n",
            "Epoch 91 -kfold 0-n_latent 4 Loss: -1550.3078\n",
            "Epoch 92 -kfold 0-n_latent 4 Loss: -1740.3613\n",
            "Epoch 93 -kfold 0-n_latent 4 Loss: -1805.5893\n",
            "Epoch 94 -kfold 0-n_latent 4 Loss: -1867.2448\n",
            "Epoch 95 -kfold 0-n_latent 4 Loss: -1971.7616\n",
            "Epoch 96 -kfold 0-n_latent 4 Loss: -2312.1454\n",
            "Epoch 97 -kfold 0-n_latent 4 Loss: -2395.7486\n",
            "Epoch 98 -kfold 0-n_latent 4 Loss: -2512.3240\n",
            "Epoch 99 -kfold 0-n_latent 4 Loss: -2769.3184\n",
            "Epoch 100 -kfold 0-n_latent 4 Loss: -2942.8531\n",
            "Epoch 101 -kfold 0-n_latent 4 Loss: -2846.0179\n",
            "Epoch 102 -kfold 0-n_latent 4 Loss: -2820.7801\n",
            "Epoch 103 -kfold 0-n_latent 4 Loss: -2925.1291\n",
            "Epoch 104 -kfold 0-n_latent 4 Loss: -2849.2359\n",
            "Epoch 105 -kfold 0-n_latent 4 Loss: -3014.1751\n",
            "Epoch 106 -kfold 0-n_latent 4 Loss: -3117.2079\n",
            "Epoch 107 -kfold 0-n_latent 4 Loss: -3036.5001\n",
            "Epoch 108 -kfold 0-n_latent 4 Loss: -3258.4423\n",
            "Epoch 109 -kfold 0-n_latent 4 Loss: -3178.1761\n",
            "Epoch 110 -kfold 0-n_latent 4 Loss: -3287.6290\n",
            "Epoch 111 -kfold 0-n_latent 4 Loss: -3543.4889\n",
            "Epoch 112 -kfold 0-n_latent 4 Loss: -3607.8801\n",
            "Epoch 113 -kfold 0-n_latent 4 Loss: -3709.0685\n",
            "Epoch 114 -kfold 0-n_latent 4 Loss: -3817.2178\n",
            "Epoch 115 -kfold 0-n_latent 4 Loss: -3942.1867\n",
            "Epoch 116 -kfold 0-n_latent 4 Loss: -3958.6194\n",
            "Epoch 117 -kfold 0-n_latent 4 Loss: -4041.2592\n",
            "Epoch 118 -kfold 0-n_latent 4 Loss: -3640.0109\n",
            "Epoch 119 -kfold 0-n_latent 4 Loss: -3385.0752\n",
            "Epoch 120 -kfold 0-n_latent 4 Loss: -3238.1960\n",
            "Epoch 121 -kfold 0-n_latent 4 Loss: -3190.6692\n",
            "Epoch 122 -kfold 0-n_latent 4 Loss: -2979.7665\n",
            "Epoch 123 -kfold 0-n_latent 4 Loss: -3057.8624\n",
            "Epoch 124 -kfold 0-n_latent 4 Loss: -3010.8553\n",
            "Epoch 125 -kfold 0-n_latent 4 Loss: -3170.0633\n",
            "Epoch 126 -kfold 0-n_latent 4 Loss: -3116.0415\n",
            "Epoch 127 -kfold 0-n_latent 4 Loss: -3269.2661\n",
            "Epoch 128 -kfold 0-n_latent 4 Loss: -3305.1353\n",
            "Epoch 129 -kfold 0-n_latent 4 Loss: -3316.9028\n",
            "Epoch 130 -kfold 0-n_latent 4 Loss: -3444.8228\n",
            "Epoch 131 -kfold 0-n_latent 4 Loss: -3603.3772\n",
            "Epoch 132 -kfold 0-n_latent 4 Loss: -3903.4090\n",
            "Epoch 133 -kfold 0-n_latent 4 Loss: -4080.0115\n",
            "Epoch 134 -kfold 0-n_latent 4 Loss: -4212.6678\n",
            "Epoch 135 -kfold 0-n_latent 4 Loss: -4452.3012\n",
            "Epoch 136 -kfold 0-n_latent 4 Loss: -4489.8999\n",
            "Epoch 137 -kfold 0-n_latent 4 Loss: -4663.4113\n",
            "Epoch 138 -kfold 0-n_latent 4 Loss: -4578.4140\n",
            "Epoch 139 -kfold 0-n_latent 4 Loss: -4728.9027\n",
            "Epoch 140 -kfold 0-n_latent 4 Loss: -4735.2880\n",
            "Epoch 141 -kfold 0-n_latent 4 Loss: -4599.6356\n",
            "Epoch 142 -kfold 0-n_latent 4 Loss: -4774.6476\n",
            "Epoch 143 -kfold 0-n_latent 4 Loss: -4886.8949\n",
            "Epoch 144 -kfold 0-n_latent 4 Loss: -4869.6915\n",
            "Epoch 145 -kfold 0-n_latent 4 Loss: -4847.9924\n",
            "Epoch 146 -kfold 0-n_latent 4 Loss: -4854.5546\n",
            "Epoch 147 -kfold 0-n_latent 4 Loss: -4810.0091\n",
            "Epoch 148 -kfold 0-n_latent 4 Loss: -4789.7174\n",
            "Epoch 149 -kfold 0-n_latent 4 Loss: -4777.6773\n",
            "Epoch 150 -kfold 0-n_latent 4 Loss: -4782.5763\n",
            "Epoch 151 -kfold 0-n_latent 4 Loss: -4850.4134\n",
            "Epoch 152 -kfold 0-n_latent 4 Loss: -4756.0819\n",
            "Epoch 153 -kfold 0-n_latent 4 Loss: -4908.6068\n",
            "Epoch 154 -kfold 0-n_latent 4 Loss: -4899.7629\n",
            "Epoch 155 -kfold 0-n_latent 4 Loss: -4975.7937\n",
            "Epoch 156 -kfold 0-n_latent 4 Loss: -5152.8812\n",
            "Epoch 157 -kfold 0-n_latent 4 Loss: -5123.7008\n",
            "Epoch 158 -kfold 0-n_latent 4 Loss: -4999.1479\n",
            "Epoch 159 -kfold 0-n_latent 4 Loss: -5254.9118\n",
            "Epoch 160 -kfold 0-n_latent 4 Loss: -5209.0056\n",
            "Epoch 161 -kfold 0-n_latent 4 Loss: -5207.1818\n",
            "Epoch 162 -kfold 0-n_latent 4 Loss: -5289.3967\n",
            "Epoch 163 -kfold 0-n_latent 4 Loss: -5168.8122\n",
            "Epoch 164 -kfold 0-n_latent 4 Loss: -4991.0925\n",
            "Epoch 165 -kfold 0-n_latent 4 Loss: -4975.4592\n",
            "Epoch 166 -kfold 0-n_latent 4 Loss: -4979.6044\n",
            "Epoch 167 -kfold 0-n_latent 4 Loss: -4995.2220\n",
            "Epoch 168 -kfold 0-n_latent 4 Loss: -4895.2475\n",
            "Epoch 169 -kfold 0-n_latent 4 Loss: -4998.7011\n",
            "Epoch 170 -kfold 0-n_latent 4 Loss: -4887.3769\n",
            "Epoch 171 -kfold 0-n_latent 4 Loss: -4872.3247\n",
            "Epoch 172 -kfold 0-n_latent 4 Loss: -4789.9913\n",
            "Epoch 173 -kfold 0-n_latent 4 Loss: -4873.5044\n",
            "Epoch 174 -kfold 0-n_latent 4 Loss: -5031.4016\n",
            "Epoch 175 -kfold 0-n_latent 4 Loss: -5143.9827\n",
            "Epoch 176 -kfold 0-n_latent 4 Loss: -4970.2709\n",
            "Epoch 177 -kfold 0-n_latent 4 Loss: -4959.8503\n",
            "Epoch 178 -kfold 0-n_latent 4 Loss: -4785.3346\n",
            "Epoch 179 -kfold 0-n_latent 4 Loss: -4896.3659\n",
            "Epoch 180 -kfold 0-n_latent 4 Loss: -4784.4881\n",
            "Epoch 181 -kfold 0-n_latent 4 Loss: -4947.7478\n",
            "Epoch 182 -kfold 0-n_latent 4 Loss: -5087.8969\n",
            "Epoch 183 -kfold 0-n_latent 4 Loss: -5355.7154\n",
            "Epoch 184 -kfold 0-n_latent 4 Loss: -5469.1515\n",
            "Epoch 185 -kfold 0-n_latent 4 Loss: -5635.4332\n",
            "Epoch 186 -kfold 0-n_latent 4 Loss: -5523.9914\n",
            "Epoch 187 -kfold 0-n_latent 4 Loss: -5242.8655\n",
            "Epoch 188 -kfold 0-n_latent 4 Loss: -5056.5107\n",
            "Epoch 189 -kfold 0-n_latent 4 Loss: -4619.8405\n",
            "Epoch 190 -kfold 0-n_latent 4 Loss: -4519.2293\n",
            "Epoch 191 -kfold 0-n_latent 4 Loss: -4568.2665\n",
            "Epoch 192 -kfold 0-n_latent 4 Loss: -4727.6692\n",
            "Epoch 193 -kfold 0-n_latent 4 Loss: -4917.4679\n",
            "Epoch 194 -kfold 0-n_latent 4 Loss: -5280.7473\n",
            "Epoch 195 -kfold 0-n_latent 4 Loss: -5557.8431\n",
            "Epoch 196 -kfold 0-n_latent 4 Loss: -5641.6700\n",
            "Epoch 197 -kfold 0-n_latent 4 Loss: -5518.7177\n",
            "Epoch 198 -kfold 0-n_latent 4 Loss: -5059.2578\n",
            "Epoch 199 -kfold 0-n_latent 4 Loss: -4848.7983\n",
            "Epoch 200 -kfold 0-n_latent 4 Loss: -4922.2278\n",
            "Epoch 201 -kfold 0-n_latent 4 Loss: -4741.4205\n",
            "Epoch 202 -kfold 0-n_latent 4 Loss: -4691.9130\n",
            "Epoch 203 -kfold 0-n_latent 4 Loss: -4615.1803\n",
            "Epoch 204 -kfold 0-n_latent 4 Loss: -4863.0996\n",
            "Epoch 205 -kfold 0-n_latent 4 Loss: -5094.9140\n",
            "Epoch 206 -kfold 0-n_latent 4 Loss: -5335.2256\n",
            "Epoch 207 -kfold 0-n_latent 4 Loss: -5622.6413\n",
            "Epoch 208 -kfold 0-n_latent 4 Loss: -5633.6346\n",
            "Epoch 209 -kfold 0-n_latent 4 Loss: -5637.2133\n",
            "Epoch 210 -kfold 0-n_latent 4 Loss: -4972.4127\n",
            "Epoch 211 -kfold 0-n_latent 4 Loss: -4299.7966\n",
            "Epoch 212 -kfold 0-n_latent 4 Loss: -4067.8865\n",
            "Epoch 213 -kfold 0-n_latent 4 Loss: -3903.5231\n",
            "Epoch 214 -kfold 0-n_latent 4 Loss: -4057.7471\n",
            "Epoch 215 -kfold 0-n_latent 4 Loss: -3988.9648\n",
            "Epoch 216 -kfold 0-n_latent 4 Loss: -4091.1038\n",
            "Epoch 217 -kfold 0-n_latent 4 Loss: -4295.6797\n",
            "Epoch 218 -kfold 0-n_latent 4 Loss: -4622.3619\n",
            "Epoch 219 -kfold 0-n_latent 4 Loss: -5131.9554\n",
            "Epoch 220 -kfold 0-n_latent 4 Loss: -5433.6612\n",
            "Epoch 221 -kfold 0-n_latent 4 Loss: -5731.1199\n",
            "Epoch 222 -kfold 0-n_latent 4 Loss: -5536.2084\n",
            "Epoch 223 -kfold 0-n_latent 4 Loss: -5274.6783\n",
            "Epoch 224 -kfold 0-n_latent 4 Loss: -4967.8323\n",
            "Epoch 225 -kfold 0-n_latent 4 Loss: -4910.6294\n",
            "Epoch 226 -kfold 0-n_latent 4 Loss: -5027.0892\n",
            "Epoch 227 -kfold 0-n_latent 4 Loss: -5111.4750\n",
            "Epoch 228 -kfold 0-n_latent 4 Loss: -5179.6773\n",
            "Epoch 229 -kfold 0-n_latent 4 Loss: -5304.8158\n",
            "Epoch 230 -kfold 0-n_latent 4 Loss: -5568.4770\n",
            "Epoch 231 -kfold 0-n_latent 4 Loss: -5802.4226\n",
            "Epoch 232 -kfold 0-n_latent 4 Loss: -6079.5030\n",
            "Epoch 233 -kfold 0-n_latent 4 Loss: -6358.9023\n",
            "Epoch 234 -kfold 0-n_latent 4 Loss: -6534.3419\n",
            "Epoch 235 -kfold 0-n_latent 4 Loss: -6364.0124\n",
            "Epoch 236 -kfold 0-n_latent 4 Loss: -5580.5122\n",
            "Epoch 237 -kfold 0-n_latent 4 Loss: -4977.8359\n",
            "Epoch 238 -kfold 0-n_latent 4 Loss: -4794.9759\n",
            "Epoch 239 -kfold 0-n_latent 4 Loss: -4698.4326\n",
            "Epoch 240 -kfold 0-n_latent 4 Loss: -4386.2599\n",
            "Epoch 241 -kfold 0-n_latent 4 Loss: -4389.2593\n",
            "Epoch 242 -kfold 0-n_latent 4 Loss: -4303.1056\n",
            "Epoch 243 -kfold 0-n_latent 4 Loss: -4296.3492\n",
            "Epoch 244 -kfold 0-n_latent 4 Loss: -4425.5267\n",
            "Epoch 245 -kfold 0-n_latent 4 Loss: -4644.1425\n",
            "Epoch 246 -kfold 0-n_latent 4 Loss: -5020.2978\n",
            "Epoch 247 -kfold 0-n_latent 4 Loss: -5373.3874\n",
            "Epoch 248 -kfold 0-n_latent 4 Loss: -5558.9151\n",
            "Epoch 249 -kfold 0-n_latent 4 Loss: -5627.5175\n",
            "Epoch 250 -kfold 0-n_latent 4 Loss: -5420.0362\n",
            "Epoch 251 -kfold 0-n_latent 4 Loss: -4971.2147\n",
            "Epoch 252 -kfold 0-n_latent 4 Loss: -4224.5546\n",
            "Epoch 253 -kfold 0-n_latent 4 Loss: -3934.3212\n",
            "Epoch 254 -kfold 0-n_latent 4 Loss: -4287.8186\n",
            "Epoch 255 -kfold 0-n_latent 4 Loss: -4306.7587\n",
            "Epoch 256 -kfold 0-n_latent 4 Loss: -4445.9774\n",
            "Epoch 257 -kfold 0-n_latent 4 Loss: -4517.0523\n",
            "Epoch 258 -kfold 0-n_latent 4 Loss: -4655.3884\n",
            "Epoch 259 -kfold 0-n_latent 4 Loss: -4909.3209\n",
            "Epoch 260 -kfold 0-n_latent 4 Loss: -5259.7996\n",
            "Epoch 261 -kfold 0-n_latent 4 Loss: -5672.1303\n",
            "Epoch 262 -kfold 0-n_latent 4 Loss: -5955.2836\n",
            "Epoch 263 -kfold 0-n_latent 4 Loss: -5967.8555\n",
            "Epoch 264 -kfold 0-n_latent 4 Loss: -5987.6404\n",
            "Epoch 265 -kfold 0-n_latent 4 Loss: -5643.9427\n",
            "Epoch 266 -kfold 0-n_latent 4 Loss: -5205.9473\n",
            "Epoch 267 -kfold 0-n_latent 4 Loss: -4721.7983\n",
            "Epoch 268 -kfold 0-n_latent 4 Loss: -5232.4964\n",
            "Epoch 269 -kfold 0-n_latent 4 Loss: -5275.1144\n",
            "Epoch 270 -kfold 0-n_latent 4 Loss: -5609.0072\n",
            "Epoch 271 -kfold 0-n_latent 4 Loss: -5539.1949\n",
            "Epoch 272 -kfold 0-n_latent 4 Loss: -5670.8343\n",
            "Epoch 273 -kfold 0-n_latent 4 Loss: -5813.0152\n",
            "Epoch 274 -kfold 0-n_latent 4 Loss: -6007.2578\n",
            "Epoch 275 -kfold 0-n_latent 4 Loss: -6202.2536\n",
            "Epoch 276 -kfold 0-n_latent 4 Loss: -6505.5353\n",
            "Epoch 277 -kfold 0-n_latent 4 Loss: -6588.3898\n",
            "Epoch 278 -kfold 0-n_latent 4 Loss: -6501.0113\n",
            "Epoch 279 -kfold 0-n_latent 4 Loss: -6107.3637\n",
            "Epoch 280 -kfold 0-n_latent 4 Loss: -4999.6949\n",
            "Epoch 281 -kfold 0-n_latent 4 Loss: -4348.7892\n",
            "Epoch 282 -kfold 0-n_latent 4 Loss: -3848.1690\n",
            "Epoch 283 -kfold 0-n_latent 4 Loss: -3880.7638\n",
            "Epoch 284 -kfold 0-n_latent 4 Loss: -4213.2288\n",
            "Epoch 285 -kfold 0-n_latent 4 Loss: -4501.7897\n",
            "Epoch 286 -kfold 0-n_latent 4 Loss: -4388.3593\n",
            "Epoch 287 -kfold 0-n_latent 4 Loss: -4142.2348\n",
            "Epoch 288 -kfold 0-n_latent 4 Loss: -4239.3990\n",
            "Epoch 289 -kfold 0-n_latent 4 Loss: -4377.1418\n",
            "Epoch 290 -kfold 0-n_latent 4 Loss: -4699.4123\n",
            "Epoch 291 -kfold 0-n_latent 4 Loss: -5110.9457\n",
            "Epoch 292 -kfold 0-n_latent 4 Loss: -5438.9402\n",
            "Epoch 293 -kfold 0-n_latent 4 Loss: -5671.1430\n",
            "Epoch 294 -kfold 0-n_latent 4 Loss: -5898.4309\n",
            "Epoch 295 -kfold 0-n_latent 4 Loss: -5770.8707\n",
            "Epoch 296 -kfold 0-n_latent 4 Loss: -5480.8026\n",
            "Epoch 297 -kfold 0-n_latent 4 Loss: -5434.5247\n",
            "Epoch 298 -kfold 0-n_latent 4 Loss: -5515.0929\n",
            "Epoch 299 -kfold 0-n_latent 4 Loss: -5613.7701\n",
            "Epoch 300 -kfold 0-n_latent 4 Loss: -5590.1077\n",
            "Epoch 301 -kfold 0-n_latent 4 Loss: -5267.2633\n",
            "Epoch 302 -kfold 0-n_latent 4 Loss: -5242.2830\n",
            "Epoch 303 -kfold 0-n_latent 4 Loss: -5630.3127\n",
            "Epoch 304 -kfold 0-n_latent 4 Loss: -5749.2670\n",
            "Epoch 305 -kfold 0-n_latent 4 Loss: -5848.6078\n",
            "Epoch 306 -kfold 0-n_latent 4 Loss: -6014.5997\n",
            "Epoch 307 -kfold 0-n_latent 4 Loss: -6247.2454\n",
            "Epoch 308 -kfold 0-n_latent 4 Loss: -6370.4865\n",
            "Epoch 309 -kfold 0-n_latent 4 Loss: -6297.7122\n",
            "Epoch 310 -kfold 0-n_latent 4 Loss: -6242.4034\n",
            "Epoch 311 -kfold 0-n_latent 4 Loss: -5997.8993\n",
            "Epoch 312 -kfold 0-n_latent 4 Loss: -5735.4065\n",
            "Epoch 313 -kfold 0-n_latent 4 Loss: -5193.5814\n",
            "Epoch 314 -kfold 0-n_latent 4 Loss: -5073.0126\n",
            "Epoch 315 -kfold 0-n_latent 4 Loss: -5095.7740\n",
            "Epoch 316 -kfold 0-n_latent 4 Loss: -5112.1477\n",
            "Epoch 317 -kfold 0-n_latent 4 Loss: -5311.3181\n",
            "Epoch 318 -kfold 0-n_latent 4 Loss: -5281.2495\n",
            "Epoch 319 -kfold 0-n_latent 4 Loss: -5342.3381\n",
            "Epoch 320 -kfold 0-n_latent 4 Loss: -5643.0782\n",
            "Epoch 321 -kfold 0-n_latent 4 Loss: -5764.4268\n",
            "Epoch 322 -kfold 0-n_latent 4 Loss: -5994.1140\n",
            "Epoch 323 -kfold 0-n_latent 4 Loss: -6211.9225\n",
            "Epoch 324 -kfold 0-n_latent 4 Loss: -6344.0033\n",
            "Epoch 325 -kfold 0-n_latent 4 Loss: -6523.3848\n",
            "Epoch 326 -kfold 0-n_latent 4 Loss: -6566.6681\n",
            "Epoch 327 -kfold 0-n_latent 4 Loss: -6459.6075\n",
            "Epoch 328 -kfold 0-n_latent 4 Loss: -6125.2850\n",
            "Epoch 329 -kfold 0-n_latent 4 Loss: -5872.4216\n",
            "Epoch 330 -kfold 0-n_latent 4 Loss: -5811.5595\n",
            "Epoch 331 -kfold 0-n_latent 4 Loss: -6077.0208\n",
            "Epoch 332 -kfold 0-n_latent 4 Loss: -6149.3475\n",
            "Epoch 333 -kfold 0-n_latent 4 Loss: -6074.0829\n",
            "Epoch 334 -kfold 0-n_latent 4 Loss: -6231.2361\n",
            "Epoch 335 -kfold 0-n_latent 4 Loss: -6460.9508\n",
            "Epoch 336 -kfold 0-n_latent 4 Loss: -6374.6380\n",
            "Epoch 337 -kfold 0-n_latent 4 Loss: -6478.5195\n",
            "Epoch 338 -kfold 0-n_latent 4 Loss: -6663.0644\n",
            "Epoch 339 -kfold 0-n_latent 4 Loss: -6854.4311\n",
            "Epoch 340 -kfold 0-n_latent 4 Loss: -6801.4455\n",
            "Epoch 341 -kfold 0-n_latent 4 Loss: -6512.8477\n",
            "Epoch 342 -kfold 0-n_latent 4 Loss: -6254.9024\n",
            "Epoch 343 -kfold 0-n_latent 4 Loss: -6118.8572\n",
            "Epoch 344 -kfold 0-n_latent 4 Loss: -6003.3513\n",
            "Epoch 345 -kfold 0-n_latent 4 Loss: -5755.7053\n",
            "Epoch 346 -kfold 0-n_latent 4 Loss: -5957.4643\n",
            "Epoch 347 -kfold 0-n_latent 4 Loss: -6153.2732\n",
            "Epoch 348 -kfold 0-n_latent 4 Loss: -5778.8621\n",
            "Epoch 349 -kfold 0-n_latent 4 Loss: -5894.7340\n",
            "Epoch 350 -kfold 0-n_latent 4 Loss: -6130.3796\n",
            "Epoch 351 -kfold 0-n_latent 4 Loss: -6262.6767\n",
            "Epoch 352 -kfold 0-n_latent 4 Loss: -6332.8014\n",
            "Epoch 353 -kfold 0-n_latent 4 Loss: -6426.1689\n",
            "Epoch 354 -kfold 0-n_latent 4 Loss: -6675.0045\n",
            "Epoch 355 -kfold 0-n_latent 4 Loss: -6678.8043\n",
            "Epoch 356 -kfold 0-n_latent 4 Loss: -6631.1201\n",
            "Epoch 357 -kfold 0-n_latent 4 Loss: -6497.7605\n",
            "Epoch 358 -kfold 0-n_latent 4 Loss: -5961.3926\n",
            "Epoch 359 -kfold 0-n_latent 4 Loss: -5368.1932\n",
            "Epoch 360 -kfold 0-n_latent 4 Loss: -5488.9011\n",
            "Epoch 361 -kfold 0-n_latent 4 Loss: -5458.7955\n",
            "Epoch 362 -kfold 0-n_latent 4 Loss: -5554.4803\n",
            "Epoch 363 -kfold 0-n_latent 4 Loss: -5657.3499\n",
            "Epoch 364 -kfold 0-n_latent 4 Loss: -5811.2942\n",
            "Epoch 365 -kfold 0-n_latent 4 Loss: -5809.0277\n",
            "Epoch 366 -kfold 0-n_latent 4 Loss: -5971.3557\n",
            "Epoch 367 -kfold 0-n_latent 4 Loss: -6250.2398\n",
            "Epoch 368 -kfold 0-n_latent 4 Loss: -6462.6308\n",
            "Epoch 369 -kfold 0-n_latent 4 Loss: -6604.3535\n",
            "Epoch 370 -kfold 0-n_latent 4 Loss: -6692.6384\n",
            "Epoch 371 -kfold 0-n_latent 4 Loss: -6679.8924\n",
            "Epoch 372 -kfold 0-n_latent 4 Loss: -6499.6256\n",
            "Epoch 373 -kfold 0-n_latent 4 Loss: -6029.0146\n",
            "Epoch 374 -kfold 0-n_latent 4 Loss: -5031.1056\n",
            "Epoch 375 -kfold 0-n_latent 4 Loss: -4427.2540\n",
            "Epoch 376 -kfold 0-n_latent 4 Loss: -4939.5769\n",
            "Epoch 377 -kfold 0-n_latent 4 Loss: -5151.0935\n",
            "Epoch 378 -kfold 0-n_latent 4 Loss: -5124.2012\n",
            "Epoch 379 -kfold 0-n_latent 4 Loss: -5195.1616\n",
            "Epoch 380 -kfold 0-n_latent 4 Loss: -5271.5594\n",
            "Epoch 381 -kfold 0-n_latent 4 Loss: -5422.4968\n",
            "Epoch 382 -kfold 0-n_latent 4 Loss: -5585.7540\n",
            "Epoch 383 -kfold 0-n_latent 4 Loss: -5825.0014\n",
            "Epoch 384 -kfold 0-n_latent 4 Loss: -6041.0920\n",
            "Epoch 385 -kfold 0-n_latent 4 Loss: -6318.1404\n",
            "Epoch 386 -kfold 0-n_latent 4 Loss: -6479.3887\n",
            "Epoch 387 -kfold 0-n_latent 4 Loss: -6458.7835\n",
            "Epoch 388 -kfold 0-n_latent 4 Loss: -6375.9585\n",
            "Epoch 389 -kfold 0-n_latent 4 Loss: -6103.2523\n",
            "Epoch 390 -kfold 0-n_latent 4 Loss: -5872.5476\n",
            "Epoch 391 -kfold 0-n_latent 4 Loss: -5929.7606\n",
            "Epoch 392 -kfold 0-n_latent 4 Loss: -6140.2734\n",
            "Epoch 393 -kfold 0-n_latent 4 Loss: -6351.8001\n",
            "Epoch 394 -kfold 0-n_latent 4 Loss: -6227.2588\n",
            "Epoch 395 -kfold 0-n_latent 4 Loss: -6151.6383\n",
            "Epoch 396 -kfold 0-n_latent 4 Loss: -6089.5672\n",
            "Epoch 397 -kfold 0-n_latent 4 Loss: -6340.6860\n",
            "Epoch 398 -kfold 0-n_latent 4 Loss: -6674.6765\n",
            "Epoch 399 -kfold 0-n_latent 4 Loss: -6856.2596\n",
            "Epoch 400 -kfold 0-n_latent 4 Loss: -6727.1792\n",
            "Epoch 401 -kfold 0-n_latent 4 Loss: -6883.1490\n",
            "Epoch 402 -kfold 0-n_latent 4 Loss: -6757.4670\n",
            "Epoch 403 -kfold 0-n_latent 4 Loss: -6577.8802\n",
            "Epoch 404 -kfold 0-n_latent 4 Loss: -6334.9921\n",
            "Epoch 405 -kfold 0-n_latent 4 Loss: -5831.0801\n",
            "Epoch 406 -kfold 0-n_latent 4 Loss: -5491.9707\n",
            "Epoch 407 -kfold 0-n_latent 4 Loss: -5497.6201\n",
            "Epoch 408 -kfold 0-n_latent 4 Loss: -5816.0647\n",
            "Epoch 409 -kfold 0-n_latent 4 Loss: -6002.9172\n",
            "Epoch 410 -kfold 0-n_latent 4 Loss: -6179.2506\n",
            "Epoch 411 -kfold 0-n_latent 4 Loss: -6263.3433\n",
            "Epoch 412 -kfold 0-n_latent 4 Loss: -6507.1443\n",
            "Epoch 413 -kfold 0-n_latent 4 Loss: -6609.1089\n",
            "Epoch 414 -kfold 0-n_latent 4 Loss: -6827.5788\n",
            "Epoch 415 -kfold 0-n_latent 4 Loss: -6990.4119\n",
            "Epoch 416 -kfold 0-n_latent 4 Loss: -7075.8264\n",
            "Epoch 417 -kfold 0-n_latent 4 Loss: -7070.3547\n",
            "Epoch 418 -kfold 0-n_latent 4 Loss: -6902.5398\n",
            "Epoch 419 -kfold 0-n_latent 4 Loss: -6507.8133\n",
            "Epoch 420 -kfold 0-n_latent 4 Loss: -6053.9558\n",
            "Epoch 421 -kfold 0-n_latent 4 Loss: -5658.7561\n",
            "Epoch 422 -kfold 0-n_latent 4 Loss: -5752.8299\n",
            "Epoch 423 -kfold 0-n_latent 4 Loss: -5778.5220\n",
            "Epoch 424 -kfold 0-n_latent 4 Loss: -6007.5104\n",
            "Epoch 425 -kfold 0-n_latent 4 Loss: -6109.0808\n",
            "Epoch 426 -kfold 0-n_latent 4 Loss: -6305.2730\n",
            "Epoch 427 -kfold 0-n_latent 4 Loss: -6383.5159\n",
            "Epoch 428 -kfold 0-n_latent 4 Loss: -6553.7716\n",
            "Epoch 429 -kfold 0-n_latent 4 Loss: -6642.8741\n",
            "Epoch 430 -kfold 0-n_latent 4 Loss: -6790.3971\n",
            "Epoch 431 -kfold 0-n_latent 4 Loss: -6955.8123\n",
            "Epoch 432 -kfold 0-n_latent 4 Loss: -6954.8034\n",
            "Epoch 433 -kfold 0-n_latent 4 Loss: -6754.5840\n",
            "Epoch 434 -kfold 0-n_latent 4 Loss: -6313.3193\n",
            "Epoch 435 -kfold 0-n_latent 4 Loss: -6116.1847\n",
            "Epoch 436 -kfold 0-n_latent 4 Loss: -5889.2217\n",
            "Epoch 437 -kfold 0-n_latent 4 Loss: -5833.4137\n",
            "Epoch 438 -kfold 0-n_latent 4 Loss: -5925.2030\n",
            "Epoch 439 -kfold 0-n_latent 4 Loss: -6110.5575\n",
            "Epoch 440 -kfold 0-n_latent 4 Loss: -6220.3365\n",
            "Epoch 441 -kfold 0-n_latent 4 Loss: -6231.5899\n",
            "Epoch 442 -kfold 0-n_latent 4 Loss: -6308.8747\n",
            "Epoch 443 -kfold 0-n_latent 4 Loss: -6295.1506\n",
            "Epoch 444 -kfold 0-n_latent 4 Loss: -6508.8216\n",
            "Epoch 445 -kfold 0-n_latent 4 Loss: -6715.8284\n",
            "Epoch 446 -kfold 0-n_latent 4 Loss: -6799.7124\n",
            "Epoch 447 -kfold 0-n_latent 4 Loss: -6709.1542\n",
            "Epoch 448 -kfold 0-n_latent 4 Loss: -6587.9129\n",
            "Epoch 449 -kfold 0-n_latent 4 Loss: -6471.0947\n",
            "Epoch 450 -kfold 0-n_latent 4 Loss: -6371.8254\n",
            "Epoch 451 -kfold 0-n_latent 4 Loss: -6492.6019\n",
            "Epoch 452 -kfold 0-n_latent 4 Loss: -6480.3857\n",
            "Epoch 453 -kfold 0-n_latent 4 Loss: -6424.5454\n",
            "Epoch 454 -kfold 0-n_latent 4 Loss: -6605.4873\n",
            "Epoch 455 -kfold 0-n_latent 4 Loss: -6651.2875\n",
            "Epoch 456 -kfold 0-n_latent 4 Loss: -6900.5885\n",
            "Epoch 457 -kfold 0-n_latent 4 Loss: -7030.7158\n",
            "Epoch 458 -kfold 0-n_latent 4 Loss: -7064.6050\n",
            "Epoch 459 -kfold 0-n_latent 4 Loss: -7093.2108\n",
            "Epoch 460 -kfold 0-n_latent 4 Loss: -7038.0599\n",
            "Epoch 461 -kfold 0-n_latent 4 Loss: -7078.9257\n",
            "Epoch 462 -kfold 0-n_latent 4 Loss: -7027.9918\n",
            "Epoch 463 -kfold 0-n_latent 4 Loss: -6745.0932\n",
            "Epoch 464 -kfold 0-n_latent 4 Loss: -6547.4331\n",
            "Epoch 465 -kfold 0-n_latent 4 Loss: -5946.6358\n",
            "Epoch 466 -kfold 0-n_latent 4 Loss: -5288.2358\n",
            "Epoch 467 -kfold 0-n_latent 4 Loss: -5271.0322\n",
            "Epoch 468 -kfold 0-n_latent 4 Loss: -5679.4398\n",
            "Epoch 469 -kfold 0-n_latent 4 Loss: -5918.0759\n",
            "Epoch 470 -kfold 0-n_latent 4 Loss: -6054.4594\n",
            "Epoch 471 -kfold 0-n_latent 4 Loss: -5994.9903\n",
            "Epoch 472 -kfold 0-n_latent 4 Loss: -6148.6587\n",
            "Epoch 473 -kfold 0-n_latent 4 Loss: -6337.4743\n",
            "Epoch 474 -kfold 0-n_latent 4 Loss: -6663.6916\n",
            "Epoch 475 -kfold 0-n_latent 4 Loss: -6863.1540\n",
            "Epoch 476 -kfold 0-n_latent 4 Loss: -7039.6642\n",
            "Epoch 477 -kfold 0-n_latent 4 Loss: -6994.5985\n",
            "Epoch 478 -kfold 0-n_latent 4 Loss: -6899.5024\n",
            "Epoch 479 -kfold 0-n_latent 4 Loss: -6722.7059\n",
            "Epoch 480 -kfold 0-n_latent 4 Loss: -6171.7417\n",
            "Epoch 481 -kfold 0-n_latent 4 Loss: -5650.3816\n",
            "Epoch 482 -kfold 0-n_latent 4 Loss: -5886.9805\n",
            "Epoch 483 -kfold 0-n_latent 4 Loss: -6090.9211\n",
            "Epoch 484 -kfold 0-n_latent 4 Loss: -5980.2650\n",
            "Epoch 485 -kfold 0-n_latent 4 Loss: -6304.1171\n",
            "Epoch 486 -kfold 0-n_latent 4 Loss: -6393.8308\n",
            "Epoch 487 -kfold 0-n_latent 4 Loss: -6526.6914\n",
            "Epoch 488 -kfold 0-n_latent 4 Loss: -6686.0097\n",
            "Epoch 489 -kfold 0-n_latent 4 Loss: -6820.2273\n",
            "Epoch 490 -kfold 0-n_latent 4 Loss: -6986.9672\n",
            "Epoch 491 -kfold 0-n_latent 4 Loss: -7207.0044\n",
            "Epoch 492 -kfold 0-n_latent 4 Loss: -7270.1136\n",
            "Epoch 493 -kfold 0-n_latent 4 Loss: -7101.2988\n",
            "Epoch 494 -kfold 0-n_latent 4 Loss: -6617.4241\n",
            "Epoch 495 -kfold 0-n_latent 4 Loss: -6160.6617\n",
            "Epoch 496 -kfold 0-n_latent 4 Loss: -5708.4666\n",
            "Epoch 497 -kfold 0-n_latent 4 Loss: -5505.7673\n",
            "Epoch 498 -kfold 0-n_latent 4 Loss: -5755.2541\n",
            "Epoch 499 -kfold 0-n_latent 4 Loss: -5969.7178\n",
            "Epoch 500 -kfold 0-n_latent 4 Loss: -6163.6942\n",
            "Epoch 501 -kfold 0-n_latent 4 Loss: -6174.5157\n",
            "Epoch 502 -kfold 0-n_latent 4 Loss: -6122.6719\n",
            "Epoch 503 -kfold 0-n_latent 4 Loss: -6185.6791\n",
            "Epoch 504 -kfold 0-n_latent 4 Loss: -6488.2813\n",
            "Epoch 505 -kfold 0-n_latent 4 Loss: -6718.9970\n",
            "Epoch 506 -kfold 0-n_latent 4 Loss: -6834.5750\n",
            "Epoch 507 -kfold 0-n_latent 4 Loss: -6929.4226\n",
            "Epoch 508 -kfold 0-n_latent 4 Loss: -7010.8449\n",
            "Epoch 509 -kfold 0-n_latent 4 Loss: -6786.8083\n",
            "Epoch 510 -kfold 0-n_latent 4 Loss: -6440.0471\n",
            "Epoch 511 -kfold 0-n_latent 4 Loss: -6052.5760\n",
            "Epoch 512 -kfold 0-n_latent 4 Loss: -6063.3208\n",
            "Epoch 513 -kfold 0-n_latent 4 Loss: -5833.4886\n",
            "Epoch 514 -kfold 0-n_latent 4 Loss: -6007.8021\n",
            "Epoch 515 -kfold 0-n_latent 4 Loss: -5934.5601\n",
            "Epoch 516 -kfold 0-n_latent 4 Loss: -6302.2714\n",
            "Epoch 517 -kfold 0-n_latent 4 Loss: -6335.2827\n",
            "Epoch 518 -kfold 0-n_latent 4 Loss: -6561.7564\n",
            "Epoch 519 -kfold 0-n_latent 4 Loss: -6683.1509\n",
            "Epoch 520 -kfold 0-n_latent 4 Loss: -6599.5964\n",
            "Epoch 521 -kfold 0-n_latent 4 Loss: -6330.4223\n",
            "Epoch 522 -kfold 0-n_latent 4 Loss: -5903.9163\n",
            "Epoch 523 -kfold 0-n_latent 4 Loss: -6059.9657\n",
            "Epoch 524 -kfold 0-n_latent 4 Loss: -6218.8397\n",
            "Epoch 525 -kfold 0-n_latent 4 Loss: -6288.5747\n",
            "Epoch 526 -kfold 0-n_latent 4 Loss: -6010.4221\n",
            "Epoch 527 -kfold 0-n_latent 4 Loss: -5965.9236\n",
            "Epoch 528 -kfold 0-n_latent 4 Loss: -6134.1088\n",
            "Epoch 529 -kfold 0-n_latent 4 Loss: -6408.4228\n",
            "Epoch 530 -kfold 0-n_latent 4 Loss: -6501.3503\n",
            "Epoch 531 -kfold 0-n_latent 4 Loss: -6783.0257\n",
            "Epoch 532 -kfold 0-n_latent 4 Loss: -6893.6823\n",
            "Epoch 533 -kfold 0-n_latent 4 Loss: -7043.7485\n",
            "Epoch 534 -kfold 0-n_latent 4 Loss: -7122.0973\n",
            "Epoch 535 -kfold 0-n_latent 4 Loss: -7234.8072\n",
            "Epoch 536 -kfold 0-n_latent 4 Loss: -7321.5095\n",
            "Epoch 537 -kfold 0-n_latent 4 Loss: -7212.7805\n",
            "Epoch 538 -kfold 0-n_latent 4 Loss: -7008.6662\n",
            "Epoch 539 -kfold 0-n_latent 4 Loss: -6932.0630\n",
            "Epoch 540 -kfold 0-n_latent 4 Loss: -6421.7618\n",
            "Epoch 541 -kfold 0-n_latent 4 Loss: -6393.1886\n",
            "Epoch 542 -kfold 0-n_latent 4 Loss: -6723.2019\n",
            "Epoch 543 -kfold 0-n_latent 4 Loss: -6594.9501\n",
            "Epoch 544 -kfold 0-n_latent 4 Loss: -6726.2604\n",
            "Epoch 545 -kfold 0-n_latent 4 Loss: -6517.1970\n",
            "Epoch 546 -kfold 0-n_latent 4 Loss: -6547.3673\n",
            "Epoch 547 -kfold 0-n_latent 4 Loss: -6708.7157\n",
            "Epoch 548 -kfold 0-n_latent 4 Loss: -6777.9202\n",
            "Epoch 549 -kfold 0-n_latent 4 Loss: -6887.7588\n",
            "Epoch 550 -kfold 0-n_latent 4 Loss: -7005.0027\n",
            "Epoch 551 -kfold 0-n_latent 4 Loss: -7066.6549\n",
            "Epoch 552 -kfold 0-n_latent 4 Loss: -7158.5607\n",
            "Epoch 553 -kfold 0-n_latent 4 Loss: -7021.3378\n",
            "Epoch 554 -kfold 0-n_latent 4 Loss: -7012.9088\n",
            "Epoch 555 -kfold 0-n_latent 4 Loss: -6840.1100\n",
            "Epoch 556 -kfold 0-n_latent 4 Loss: -6733.6976\n",
            "Epoch 557 -kfold 0-n_latent 4 Loss: -6693.2707\n",
            "Epoch 558 -kfold 0-n_latent 4 Loss: -6801.8920\n",
            "Epoch 559 -kfold 0-n_latent 4 Loss: -6882.1428\n",
            "Epoch 560 -kfold 0-n_latent 4 Loss: -6674.6642\n",
            "Epoch 561 -kfold 0-n_latent 4 Loss: -6927.2228\n",
            "Epoch 562 -kfold 0-n_latent 4 Loss: -6836.0353\n",
            "Epoch 563 -kfold 0-n_latent 4 Loss: -6945.0534\n",
            "Epoch 564 -kfold 0-n_latent 4 Loss: -7097.4488\n",
            "Epoch 565 -kfold 0-n_latent 4 Loss: -7376.7699\n",
            "Epoch 566 -kfold 0-n_latent 4 Loss: -7316.7776\n",
            "Epoch 567 -kfold 0-n_latent 4 Loss: -7393.4334\n",
            "Epoch 568 -kfold 0-n_latent 4 Loss: -7352.8811\n",
            "Epoch 569 -kfold 0-n_latent 4 Loss: -7198.8602\n",
            "Epoch 570 -kfold 0-n_latent 4 Loss: -6955.6889\n",
            "Epoch 571 -kfold 0-n_latent 4 Loss: -6718.4145\n",
            "Epoch 572 -kfold 0-n_latent 4 Loss: -6551.1408\n",
            "Epoch 573 -kfold 0-n_latent 4 Loss: -6557.9906\n",
            "Epoch 574 -kfold 0-n_latent 4 Loss: -6663.5134\n",
            "Epoch 575 -kfold 0-n_latent 4 Loss: -6759.2329\n",
            "Epoch 576 -kfold 0-n_latent 4 Loss: -7104.2739\n",
            "Epoch 577 -kfold 0-n_latent 4 Loss: -7075.5474\n",
            "Epoch 578 -kfold 0-n_latent 4 Loss: -7129.7734\n",
            "Epoch 579 -kfold 0-n_latent 4 Loss: -7245.8663\n",
            "Epoch 580 -kfold 0-n_latent 4 Loss: -7223.9829\n",
            "Epoch 581 -kfold 0-n_latent 4 Loss: -7100.7679\n",
            "Epoch 582 -kfold 0-n_latent 4 Loss: -7009.5177\n",
            "Epoch 583 -kfold 0-n_latent 4 Loss: -6993.9751\n",
            "Epoch 584 -kfold 0-n_latent 4 Loss: -6826.4360\n",
            "Epoch 585 -kfold 0-n_latent 4 Loss: -6553.7326\n",
            "Epoch 586 -kfold 0-n_latent 4 Loss: -6525.8080\n",
            "Epoch 587 -kfold 0-n_latent 4 Loss: -6716.0519\n",
            "Epoch 588 -kfold 0-n_latent 4 Loss: -6698.5042\n",
            "Epoch 589 -kfold 0-n_latent 4 Loss: -6840.7325\n",
            "Epoch 590 -kfold 0-n_latent 4 Loss: -6776.6757\n",
            "Epoch 591 -kfold 0-n_latent 4 Loss: -6936.8225\n",
            "Epoch 592 -kfold 0-n_latent 4 Loss: -6920.3401\n",
            "Epoch 593 -kfold 0-n_latent 4 Loss: -6682.2611\n",
            "Epoch 594 -kfold 0-n_latent 4 Loss: -6614.7314\n",
            "Epoch 595 -kfold 0-n_latent 4 Loss: -6648.8392\n",
            "Epoch 596 -kfold 0-n_latent 4 Loss: -6843.6519\n",
            "Epoch 597 -kfold 0-n_latent 4 Loss: -6794.0855\n",
            "Epoch 598 -kfold 0-n_latent 4 Loss: -6619.7337\n",
            "Epoch 599 -kfold 0-n_latent 4 Loss: -6613.4729\n",
            "Epoch 600 -kfold 0-n_latent 4 Loss: -6727.0306\n",
            "Epoch 601 -kfold 0-n_latent 4 Loss: -6931.5203\n",
            "Epoch 602 -kfold 0-n_latent 4 Loss: -6892.3871\n",
            "Epoch 603 -kfold 0-n_latent 4 Loss: -6802.8528\n",
            "Epoch 604 -kfold 0-n_latent 4 Loss: -7146.7271\n",
            "Epoch 605 -kfold 0-n_latent 4 Loss: -7112.1819\n",
            "Epoch 606 -kfold 0-n_latent 4 Loss: -7311.8594\n",
            "Epoch 607 -kfold 0-n_latent 4 Loss: -7242.7501\n",
            "Epoch 608 -kfold 0-n_latent 4 Loss: -7324.0252\n",
            "Epoch 609 -kfold 0-n_latent 4 Loss: -7292.0062\n",
            "Epoch 610 -kfold 0-n_latent 4 Loss: -7320.9901\n",
            "Epoch 611 -kfold 0-n_latent 4 Loss: -7136.3982\n",
            "Epoch 612 -kfold 0-n_latent 4 Loss: -6921.0481\n",
            "Epoch 613 -kfold 0-n_latent 4 Loss: -6857.3030\n",
            "Epoch 614 -kfold 0-n_latent 4 Loss: -6904.5477\n",
            "Epoch 615 -kfold 0-n_latent 4 Loss: -6827.4966\n",
            "Epoch 616 -kfold 0-n_latent 4 Loss: -6840.9672\n",
            "Epoch 617 -kfold 0-n_latent 4 Loss: -6801.8184\n",
            "Epoch 618 -kfold 0-n_latent 4 Loss: -6804.5439\n",
            "Epoch 619 -kfold 0-n_latent 4 Loss: -6759.2078\n",
            "Epoch 620 -kfold 0-n_latent 4 Loss: -6937.9021\n",
            "Epoch 621 -kfold 0-n_latent 4 Loss: -6751.5565\n",
            "Epoch 622 -kfold 0-n_latent 4 Loss: -6887.9773\n",
            "Epoch 623 -kfold 0-n_latent 4 Loss: -6811.6012\n",
            "Epoch 624 -kfold 0-n_latent 4 Loss: -6674.9123\n",
            "Epoch 625 -kfold 0-n_latent 4 Loss: -6576.0502\n",
            "Epoch 626 -kfold 0-n_latent 4 Loss: -6688.2479\n",
            "Epoch 627 -kfold 0-n_latent 4 Loss: -6725.4241\n",
            "Epoch 628 -kfold 0-n_latent 4 Loss: -6651.2513\n",
            "Epoch 629 -kfold 0-n_latent 4 Loss: -6780.3157\n",
            "Epoch 630 -kfold 0-n_latent 4 Loss: -6846.0096\n",
            "Epoch 631 -kfold 0-n_latent 4 Loss: -7034.6119\n",
            "Epoch 632 -kfold 0-n_latent 4 Loss: -7053.1913\n",
            "Epoch 633 -kfold 0-n_latent 4 Loss: -7129.0916\n",
            "Epoch 634 -kfold 0-n_latent 4 Loss: -7141.8116\n",
            "Epoch 635 -kfold 0-n_latent 4 Loss: -7146.6674\n",
            "Epoch 636 -kfold 0-n_latent 4 Loss: -7247.2038\n",
            "Epoch 637 -kfold 0-n_latent 4 Loss: -7308.1535\n",
            "Epoch 638 -kfold 0-n_latent 4 Loss: -7238.7009\n",
            "Epoch 639 -kfold 0-n_latent 4 Loss: -7131.2149\n",
            "Epoch 640 -kfold 0-n_latent 4 Loss: -7109.7658\n",
            "Epoch 641 -kfold 0-n_latent 4 Loss: -7061.9316\n",
            "Epoch 642 -kfold 0-n_latent 4 Loss: -6857.8844\n",
            "Epoch 643 -kfold 0-n_latent 4 Loss: -6149.1205\n",
            "Epoch 644 -kfold 0-n_latent 4 Loss: -5663.7769\n",
            "Epoch 645 -kfold 0-n_latent 4 Loss: -5741.8414\n",
            "Epoch 646 -kfold 0-n_latent 4 Loss: -5843.5594\n",
            "Epoch 647 -kfold 0-n_latent 4 Loss: -5866.7120\n",
            "Epoch 648 -kfold 0-n_latent 4 Loss: -5960.4256\n",
            "Epoch 649 -kfold 0-n_latent 4 Loss: -6129.1736\n",
            "Epoch 650 -kfold 0-n_latent 4 Loss: -6437.0220\n",
            "Epoch 651 -kfold 0-n_latent 4 Loss: -6604.2802\n",
            "Epoch 652 -kfold 0-n_latent 4 Loss: -6644.2920\n",
            "Epoch 653 -kfold 0-n_latent 4 Loss: -6791.7903\n",
            "Epoch 654 -kfold 0-n_latent 4 Loss: -6735.6892\n",
            "Epoch 655 -kfold 0-n_latent 4 Loss: -6821.9927\n",
            "Epoch 656 -kfold 0-n_latent 4 Loss: -6641.6208\n",
            "Epoch 657 -kfold 0-n_latent 4 Loss: -6457.9864\n",
            "Epoch 658 -kfold 0-n_latent 4 Loss: -6479.1332\n",
            "Epoch 659 -kfold 0-n_latent 4 Loss: -6465.2509\n",
            "Epoch 660 -kfold 0-n_latent 4 Loss: -6343.3615\n",
            "Epoch 661 -kfold 0-n_latent 4 Loss: -6500.8580\n",
            "Epoch 662 -kfold 0-n_latent 4 Loss: -6545.0492\n",
            "Epoch 663 -kfold 0-n_latent 4 Loss: -6901.6644\n",
            "Epoch 664 -kfold 0-n_latent 4 Loss: -7036.9682\n",
            "Epoch 665 -kfold 0-n_latent 4 Loss: -7102.3036\n",
            "Epoch 666 -kfold 0-n_latent 4 Loss: -7184.9128\n",
            "Epoch 667 -kfold 0-n_latent 4 Loss: -7218.1309\n",
            "Epoch 668 -kfold 0-n_latent 4 Loss: -7232.7710\n",
            "Epoch 669 -kfold 0-n_latent 4 Loss: -7004.9331\n",
            "Epoch 670 -kfold 0-n_latent 4 Loss: -6764.7112\n",
            "Epoch 671 -kfold 0-n_latent 4 Loss: -5931.1721\n",
            "Epoch 672 -kfold 0-n_latent 4 Loss: -5978.8555\n",
            "Epoch 673 -kfold 0-n_latent 4 Loss: -6333.8198\n",
            "Epoch 674 -kfold 0-n_latent 4 Loss: -6448.8619\n",
            "Epoch 675 -kfold 0-n_latent 4 Loss: -6406.9380\n",
            "Epoch 676 -kfold 0-n_latent 4 Loss: -6583.4030\n",
            "Epoch 677 -kfold 0-n_latent 4 Loss: -6745.0071\n",
            "Epoch 678 -kfold 0-n_latent 4 Loss: -6874.8123\n",
            "Epoch 679 -kfold 0-n_latent 4 Loss: -6865.0114\n",
            "Epoch 680 -kfold 0-n_latent 4 Loss: -7129.8780\n",
            "Epoch 681 -kfold 0-n_latent 4 Loss: -7183.7462\n",
            "Epoch 682 -kfold 0-n_latent 4 Loss: -7185.4798\n",
            "Epoch 683 -kfold 0-n_latent 4 Loss: -7057.5836\n",
            "Epoch 684 -kfold 0-n_latent 4 Loss: -6791.8507\n",
            "Epoch 685 -kfold 0-n_latent 4 Loss: -6464.7276\n",
            "Epoch 686 -kfold 0-n_latent 4 Loss: -6294.6852\n",
            "Epoch 687 -kfold 0-n_latent 4 Loss: -6228.6489\n",
            "Epoch 688 -kfold 0-n_latent 4 Loss: -6164.8889\n",
            "Epoch 689 -kfold 0-n_latent 4 Loss: -6234.1305\n",
            "Epoch 690 -kfold 0-n_latent 4 Loss: -6349.2646\n",
            "Epoch 691 -kfold 0-n_latent 4 Loss: -6483.6101\n",
            "Epoch 692 -kfold 0-n_latent 4 Loss: -6782.6285\n",
            "Epoch 693 -kfold 0-n_latent 4 Loss: -6761.6233\n",
            "Epoch 694 -kfold 0-n_latent 4 Loss: -7010.9662\n",
            "Epoch 695 -kfold 0-n_latent 4 Loss: -7241.4354\n",
            "Epoch 696 -kfold 0-n_latent 4 Loss: -7367.0118\n",
            "Epoch 697 -kfold 0-n_latent 4 Loss: -7439.4692\n",
            "Epoch 698 -kfold 0-n_latent 4 Loss: -7067.0805\n",
            "Epoch 699 -kfold 0-n_latent 4 Loss: -6786.5590\n",
            "5\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 0-n_latent 5 Loss:  25007.7225\n",
            "Epoch 2 -kfold 0-n_latent 5 Loss:  23357.6902\n",
            "Epoch 3 -kfold 0-n_latent 5 Loss:  21877.8413\n",
            "Epoch 4 -kfold 0-n_latent 5 Loss:  20437.7423\n",
            "Epoch 5 -kfold 0-n_latent 5 Loss:  19002.7527\n",
            "Epoch 6 -kfold 0-n_latent 5 Loss:  17583.5732\n",
            "Epoch 7 -kfold 0-n_latent 5 Loss:  16189.7939\n",
            "Epoch 8 -kfold 0-n_latent 5 Loss:  14817.0275\n",
            "Epoch 9 -kfold 0-n_latent 5 Loss:  13477.7714\n",
            "Epoch 10 -kfold 0-n_latent 5 Loss:  12236.7222\n",
            "Epoch 11 -kfold 0-n_latent 5 Loss:  11105.5102\n",
            "Epoch 12 -kfold 0-n_latent 5 Loss:  10118.2770\n",
            "Epoch 13 -kfold 0-n_latent 5 Loss:  9326.6948\n",
            "Epoch 14 -kfold 0-n_latent 5 Loss:  8672.2057\n",
            "Epoch 15 -kfold 0-n_latent 5 Loss:  8062.7946\n",
            "Epoch 16 -kfold 0-n_latent 5 Loss:  7426.1466\n",
            "Epoch 17 -kfold 0-n_latent 5 Loss:  6967.9872\n",
            "Epoch 18 -kfold 0-n_latent 5 Loss:  6473.2363\n",
            "Epoch 19 -kfold 0-n_latent 5 Loss:  5871.7986\n",
            "Epoch 20 -kfold 0-n_latent 5 Loss:  5371.1906\n",
            "Epoch 21 -kfold 0-n_latent 5 Loss:  4920.1444\n",
            "Epoch 22 -kfold 0-n_latent 5 Loss:  4443.0028\n",
            "Epoch 23 -kfold 0-n_latent 5 Loss:  3962.3352\n",
            "Epoch 24 -kfold 0-n_latent 5 Loss:  3492.8385\n",
            "Epoch 25 -kfold 0-n_latent 5 Loss:  3084.3202\n",
            "Epoch 26 -kfold 0-n_latent 5 Loss:  2703.6060\n",
            "Epoch 27 -kfold 0-n_latent 5 Loss:  2410.0643\n",
            "Epoch 28 -kfold 0-n_latent 5 Loss:  2034.4668\n",
            "Epoch 29 -kfold 0-n_latent 5 Loss:  1737.3769\n",
            "Epoch 30 -kfold 0-n_latent 5 Loss:  1468.6001\n",
            "Epoch 31 -kfold 0-n_latent 5 Loss:  1126.2897\n",
            "Epoch 32 -kfold 0-n_latent 5 Loss:  803.3472\n",
            "Epoch 33 -kfold 0-n_latent 5 Loss:  542.3422\n",
            "Epoch 34 -kfold 0-n_latent 5 Loss:  289.9532\n",
            "Epoch 35 -kfold 0-n_latent 5 Loss:  18.3068\n",
            "Epoch 36 -kfold 0-n_latent 5 Loss: -214.3923\n",
            "Epoch 37 -kfold 0-n_latent 5 Loss: -514.5247\n",
            "Epoch 38 -kfold 0-n_latent 5 Loss: -778.5347\n",
            "Epoch 39 -kfold 0-n_latent 5 Loss: -968.6304\n",
            "Epoch 40 -kfold 0-n_latent 5 Loss: -1165.7291\n",
            "Epoch 41 -kfold 0-n_latent 5 Loss: -1466.5853\n",
            "Epoch 42 -kfold 0-n_latent 5 Loss: -1670.9676\n",
            "Epoch 43 -kfold 0-n_latent 5 Loss: -1795.9400\n",
            "Epoch 44 -kfold 0-n_latent 5 Loss: -2041.7516\n",
            "Epoch 45 -kfold 0-n_latent 5 Loss: -2163.6581\n",
            "Epoch 46 -kfold 0-n_latent 5 Loss: -2251.8669\n",
            "Epoch 47 -kfold 0-n_latent 5 Loss: -2487.4599\n",
            "Epoch 48 -kfold 0-n_latent 5 Loss: -2636.4513\n",
            "Epoch 49 -kfold 0-n_latent 5 Loss: -2963.1581\n",
            "Epoch 50 -kfold 0-n_latent 5 Loss: -3180.9210\n",
            "Epoch 51 -kfold 0-n_latent 5 Loss: -3355.6925\n",
            "Epoch 52 -kfold 0-n_latent 5 Loss: -3622.6136\n",
            "Epoch 53 -kfold 0-n_latent 5 Loss: -3751.5027\n",
            "Epoch 54 -kfold 0-n_latent 5 Loss: -3871.8505\n",
            "Epoch 55 -kfold 0-n_latent 5 Loss: -3991.9485\n",
            "Epoch 56 -kfold 0-n_latent 5 Loss: -4224.8501\n",
            "Epoch 57 -kfold 0-n_latent 5 Loss: -4320.2542\n",
            "Epoch 58 -kfold 0-n_latent 5 Loss: -4519.8351\n",
            "Epoch 59 -kfold 0-n_latent 5 Loss: -4700.1437\n",
            "Epoch 60 -kfold 0-n_latent 5 Loss: -5031.9026\n",
            "Epoch 61 -kfold 0-n_latent 5 Loss: -5087.6484\n",
            "Epoch 62 -kfold 0-n_latent 5 Loss: -5133.8247\n",
            "Epoch 63 -kfold 0-n_latent 5 Loss: -5129.2902\n",
            "Epoch 64 -kfold 0-n_latent 5 Loss: -5272.5697\n",
            "Epoch 65 -kfold 0-n_latent 5 Loss: -5344.7582\n",
            "Epoch 66 -kfold 0-n_latent 5 Loss: -5355.8138\n",
            "Epoch 67 -kfold 0-n_latent 5 Loss: -5395.4922\n",
            "Epoch 68 -kfold 0-n_latent 5 Loss: -5371.6458\n",
            "Epoch 69 -kfold 0-n_latent 5 Loss: -5456.2083\n",
            "Epoch 70 -kfold 0-n_latent 5 Loss: -5636.9553\n",
            "Epoch 71 -kfold 0-n_latent 5 Loss: -5823.9484\n",
            "Epoch 72 -kfold 0-n_latent 5 Loss: -5941.7896\n",
            "Epoch 73 -kfold 0-n_latent 5 Loss: -6190.3999\n",
            "Epoch 74 -kfold 0-n_latent 5 Loss: -6391.6475\n",
            "Epoch 75 -kfold 0-n_latent 5 Loss: -6433.6304\n",
            "Epoch 76 -kfold 0-n_latent 5 Loss: -6543.9920\n",
            "Epoch 77 -kfold 0-n_latent 5 Loss: -6671.7911\n",
            "Epoch 78 -kfold 0-n_latent 5 Loss: -6756.1634\n",
            "Epoch 79 -kfold 0-n_latent 5 Loss: -6710.8204\n",
            "Epoch 80 -kfold 0-n_latent 5 Loss: -6705.8985\n",
            "Epoch 81 -kfold 0-n_latent 5 Loss: -6639.2503\n",
            "Epoch 82 -kfold 0-n_latent 5 Loss: -6603.4675\n",
            "Epoch 83 -kfold 0-n_latent 5 Loss: -6527.9467\n",
            "Epoch 84 -kfold 0-n_latent 5 Loss: -6666.2794\n",
            "Epoch 85 -kfold 0-n_latent 5 Loss: -6926.2549\n",
            "Epoch 86 -kfold 0-n_latent 5 Loss: -7053.8514\n",
            "Epoch 87 -kfold 0-n_latent 5 Loss: -7210.4850\n",
            "Epoch 88 -kfold 0-n_latent 5 Loss: -7403.1933\n",
            "Epoch 89 -kfold 0-n_latent 5 Loss: -7301.4074\n",
            "Epoch 90 -kfold 0-n_latent 5 Loss: -7210.6007\n",
            "Epoch 91 -kfold 0-n_latent 5 Loss: -7028.8859\n",
            "Epoch 92 -kfold 0-n_latent 5 Loss: -7159.3658\n",
            "Epoch 93 -kfold 0-n_latent 5 Loss: -7233.7884\n",
            "Epoch 94 -kfold 0-n_latent 5 Loss: -7193.4645\n",
            "Epoch 95 -kfold 0-n_latent 5 Loss: -7374.6207\n",
            "Epoch 96 -kfold 0-n_latent 5 Loss: -7593.8461\n",
            "Epoch 97 -kfold 0-n_latent 5 Loss: -7722.9187\n",
            "Epoch 98 -kfold 0-n_latent 5 Loss: -7888.9250\n",
            "Epoch 99 -kfold 0-n_latent 5 Loss: -7851.8614\n",
            "Epoch 100 -kfold 0-n_latent 5 Loss: -7946.2308\n",
            "Epoch 101 -kfold 0-n_latent 5 Loss: -8116.0222\n",
            "Epoch 102 -kfold 0-n_latent 5 Loss: -8001.4408\n",
            "Epoch 103 -kfold 0-n_latent 5 Loss: -7981.1791\n",
            "Epoch 104 -kfold 0-n_latent 5 Loss: -7823.3590\n",
            "Epoch 105 -kfold 0-n_latent 5 Loss: -7718.4662\n",
            "Epoch 106 -kfold 0-n_latent 5 Loss: -7610.1889\n",
            "Epoch 107 -kfold 0-n_latent 5 Loss: -7523.0889\n",
            "Epoch 108 -kfold 0-n_latent 5 Loss: -7632.2067\n",
            "Epoch 109 -kfold 0-n_latent 5 Loss: -7723.9276\n",
            "Epoch 110 -kfold 0-n_latent 5 Loss: -7768.5413\n",
            "Epoch 111 -kfold 0-n_latent 5 Loss: -7946.8653\n",
            "Epoch 112 -kfold 0-n_latent 5 Loss: -8063.0542\n",
            "Epoch 113 -kfold 0-n_latent 5 Loss: -8281.9000\n",
            "Epoch 114 -kfold 0-n_latent 5 Loss: -8407.4046\n",
            "Epoch 115 -kfold 0-n_latent 5 Loss: -8254.9229\n",
            "Epoch 116 -kfold 0-n_latent 5 Loss: -8403.7556\n",
            "Epoch 117 -kfold 0-n_latent 5 Loss: -8525.5824\n",
            "Epoch 118 -kfold 0-n_latent 5 Loss: -8562.2229\n",
            "Epoch 119 -kfold 0-n_latent 5 Loss: -8771.1066\n",
            "Epoch 120 -kfold 0-n_latent 5 Loss: -8777.3216\n",
            "Epoch 121 -kfold 0-n_latent 5 Loss: -8717.3341\n",
            "Epoch 122 -kfold 0-n_latent 5 Loss: -8814.8852\n",
            "Epoch 123 -kfold 0-n_latent 5 Loss: -8659.8702\n",
            "Epoch 124 -kfold 0-n_latent 5 Loss: -8702.6422\n",
            "Epoch 125 -kfold 0-n_latent 5 Loss: -8639.1173\n",
            "Epoch 126 -kfold 0-n_latent 5 Loss: -8660.0871\n",
            "Epoch 127 -kfold 0-n_latent 5 Loss: -8747.2659\n",
            "Epoch 128 -kfold 0-n_latent 5 Loss: -8639.4517\n",
            "Epoch 129 -kfold 0-n_latent 5 Loss: -8695.3981\n",
            "Epoch 130 -kfold 0-n_latent 5 Loss: -8799.7608\n",
            "Epoch 131 -kfold 0-n_latent 5 Loss: -8959.5018\n",
            "Epoch 132 -kfold 0-n_latent 5 Loss: -9106.4300\n",
            "Epoch 133 -kfold 0-n_latent 5 Loss: -9055.8117\n",
            "Epoch 134 -kfold 0-n_latent 5 Loss: -8877.1086\n",
            "Epoch 135 -kfold 0-n_latent 5 Loss: -8790.5508\n",
            "Epoch 136 -kfold 0-n_latent 5 Loss: -8596.1743\n",
            "Epoch 137 -kfold 0-n_latent 5 Loss: -8561.3649\n",
            "Epoch 138 -kfold 0-n_latent 5 Loss: -8599.8196\n",
            "Epoch 139 -kfold 0-n_latent 5 Loss: -8606.9409\n",
            "Epoch 140 -kfold 0-n_latent 5 Loss: -8689.1776\n",
            "Epoch 141 -kfold 0-n_latent 5 Loss: -8652.8247\n",
            "Epoch 142 -kfold 0-n_latent 5 Loss: -8724.9356\n",
            "Epoch 143 -kfold 0-n_latent 5 Loss: -8896.3753\n",
            "Epoch 144 -kfold 0-n_latent 5 Loss: -8908.5800\n",
            "Epoch 145 -kfold 0-n_latent 5 Loss: -8944.9904\n",
            "Epoch 146 -kfold 0-n_latent 5 Loss: -8956.2495\n",
            "Epoch 147 -kfold 0-n_latent 5 Loss: -8889.4963\n",
            "Epoch 148 -kfold 0-n_latent 5 Loss: -9034.2357\n",
            "Epoch 149 -kfold 0-n_latent 5 Loss: -9122.9591\n",
            "Epoch 150 -kfold 0-n_latent 5 Loss: -8968.3173\n",
            "Epoch 151 -kfold 0-n_latent 5 Loss: -8930.3687\n",
            "Epoch 152 -kfold 0-n_latent 5 Loss: -9060.2499\n",
            "Epoch 153 -kfold 0-n_latent 5 Loss: -9081.7510\n",
            "Epoch 154 -kfold 0-n_latent 5 Loss: -9260.6221\n",
            "Epoch 155 -kfold 0-n_latent 5 Loss: -9358.8655\n",
            "Epoch 156 -kfold 0-n_latent 5 Loss: -9273.6510\n",
            "Epoch 157 -kfold 0-n_latent 5 Loss: -9228.3172\n",
            "Epoch 158 -kfold 0-n_latent 5 Loss: -9182.2352\n",
            "Epoch 159 -kfold 0-n_latent 5 Loss: -9029.7436\n",
            "Epoch 160 -kfold 0-n_latent 5 Loss: -8937.4801\n",
            "Epoch 161 -kfold 0-n_latent 5 Loss: -9063.7831\n",
            "Epoch 162 -kfold 0-n_latent 5 Loss: -8937.8393\n",
            "Epoch 163 -kfold 0-n_latent 5 Loss: -9167.4324\n",
            "Epoch 164 -kfold 0-n_latent 5 Loss: -9247.8263\n",
            "Epoch 165 -kfold 0-n_latent 5 Loss: -9333.8553\n",
            "Epoch 166 -kfold 0-n_latent 5 Loss: -9329.3739\n",
            "Epoch 167 -kfold 0-n_latent 5 Loss: -9466.6614\n",
            "Epoch 168 -kfold 0-n_latent 5 Loss: -9401.3455\n",
            "Epoch 169 -kfold 0-n_latent 5 Loss: -9432.4089\n",
            "Epoch 170 -kfold 0-n_latent 5 Loss: -9599.1132\n",
            "Epoch 171 -kfold 0-n_latent 5 Loss: -9294.9469\n",
            "Epoch 172 -kfold 0-n_latent 5 Loss: -9286.5804\n",
            "Epoch 173 -kfold 0-n_latent 5 Loss: -9295.7543\n",
            "Epoch 174 -kfold 0-n_latent 5 Loss: -9091.9879\n",
            "Epoch 175 -kfold 0-n_latent 5 Loss: -9377.1946\n",
            "Epoch 176 -kfold 0-n_latent 5 Loss: -9162.8933\n",
            "Epoch 177 -kfold 0-n_latent 5 Loss: -9383.7356\n",
            "Epoch 178 -kfold 0-n_latent 5 Loss: -9559.7300\n",
            "Epoch 179 -kfold 0-n_latent 5 Loss: -9462.8674\n",
            "Epoch 180 -kfold 0-n_latent 5 Loss: -9619.6448\n",
            "Epoch 181 -kfold 0-n_latent 5 Loss: -9719.1197\n",
            "Epoch 182 -kfold 0-n_latent 5 Loss: -9660.0552\n",
            "Epoch 183 -kfold 0-n_latent 5 Loss: -9646.7931\n",
            "Epoch 184 -kfold 0-n_latent 5 Loss: -9405.8182\n",
            "Epoch 185 -kfold 0-n_latent 5 Loss: -9353.1043\n",
            "Epoch 186 -kfold 0-n_latent 5 Loss: -9575.0712\n",
            "Epoch 187 -kfold 0-n_latent 5 Loss: -9541.5169\n",
            "Epoch 188 -kfold 0-n_latent 5 Loss: -9641.2787\n",
            "Epoch 189 -kfold 0-n_latent 5 Loss: -9748.5617\n",
            "Epoch 190 -kfold 0-n_latent 5 Loss: -9804.8297\n",
            "Epoch 191 -kfold 0-n_latent 5 Loss: -9928.6984\n",
            "Epoch 192 -kfold 0-n_latent 5 Loss: -9869.0472\n",
            "Epoch 193 -kfold 0-n_latent 5 Loss: -9832.1991\n",
            "Epoch 194 -kfold 0-n_latent 5 Loss: -9459.7132\n",
            "Epoch 195 -kfold 0-n_latent 5 Loss: -9306.4996\n",
            "Epoch 196 -kfold 0-n_latent 5 Loss: -9403.8527\n",
            "Epoch 197 -kfold 0-n_latent 5 Loss: -9176.2756\n",
            "Epoch 198 -kfold 0-n_latent 5 Loss: -9214.8863\n",
            "Epoch 199 -kfold 0-n_latent 5 Loss: -9098.9820\n",
            "Epoch 200 -kfold 0-n_latent 5 Loss: -9162.5059\n",
            "Epoch 201 -kfold 0-n_latent 5 Loss: -9260.0287\n",
            "Epoch 202 -kfold 0-n_latent 5 Loss: -9375.4837\n",
            "Epoch 203 -kfold 0-n_latent 5 Loss: -9697.7497\n",
            "Epoch 204 -kfold 0-n_latent 5 Loss: -9701.8870\n",
            "Epoch 205 -kfold 0-n_latent 5 Loss: -9846.3737\n",
            "Epoch 206 -kfold 0-n_latent 5 Loss: -9818.8790\n",
            "Epoch 207 -kfold 0-n_latent 5 Loss: -9896.1782\n",
            "Epoch 208 -kfold 0-n_latent 5 Loss: -9549.3973\n",
            "Epoch 209 -kfold 0-n_latent 5 Loss: -9420.5864\n",
            "Epoch 210 -kfold 0-n_latent 5 Loss: -9468.7905\n",
            "Epoch 211 -kfold 0-n_latent 5 Loss: -9546.3440\n",
            "Epoch 212 -kfold 0-n_latent 5 Loss: -9707.3800\n",
            "Epoch 213 -kfold 0-n_latent 5 Loss: -9759.6960\n",
            "Epoch 214 -kfold 0-n_latent 5 Loss: -9756.3131\n",
            "Epoch 215 -kfold 0-n_latent 5 Loss: -10006.8347\n",
            "Epoch 216 -kfold 0-n_latent 5 Loss: -10176.8195\n",
            "Epoch 217 -kfold 0-n_latent 5 Loss: -9948.9764\n",
            "Epoch 218 -kfold 0-n_latent 5 Loss: -9815.4707\n",
            "Epoch 219 -kfold 0-n_latent 5 Loss: -9846.9668\n",
            "Epoch 220 -kfold 0-n_latent 5 Loss: -9749.9172\n",
            "Epoch 221 -kfold 0-n_latent 5 Loss: -9937.8278\n",
            "Epoch 222 -kfold 0-n_latent 5 Loss: -9949.2903\n",
            "Epoch 223 -kfold 0-n_latent 5 Loss: -10252.7644\n",
            "Epoch 224 -kfold 0-n_latent 5 Loss: -10187.7405\n",
            "Epoch 225 -kfold 0-n_latent 5 Loss: -10331.4638\n",
            "Epoch 226 -kfold 0-n_latent 5 Loss: -10226.6511\n",
            "Epoch 227 -kfold 0-n_latent 5 Loss: -9990.2410\n",
            "Epoch 228 -kfold 0-n_latent 5 Loss: -9852.4172\n",
            "Epoch 229 -kfold 0-n_latent 5 Loss: -9701.5260\n",
            "Epoch 230 -kfold 0-n_latent 5 Loss: -9647.5478\n",
            "Epoch 231 -kfold 0-n_latent 5 Loss: -9806.0301\n",
            "Epoch 232 -kfold 0-n_latent 5 Loss: -10022.2984\n",
            "Epoch 233 -kfold 0-n_latent 5 Loss: -10102.2922\n",
            "Epoch 234 -kfold 0-n_latent 5 Loss: -10171.6276\n",
            "Epoch 235 -kfold 0-n_latent 5 Loss: -10161.2114\n",
            "Epoch 236 -kfold 0-n_latent 5 Loss: -9959.7724\n",
            "Epoch 237 -kfold 0-n_latent 5 Loss: -10002.6026\n",
            "Epoch 238 -kfold 0-n_latent 5 Loss: -9935.6978\n",
            "Epoch 239 -kfold 0-n_latent 5 Loss: -9909.0013\n",
            "Epoch 240 -kfold 0-n_latent 5 Loss: -9884.9488\n",
            "Epoch 241 -kfold 0-n_latent 5 Loss: -9767.5242\n",
            "Epoch 242 -kfold 0-n_latent 5 Loss: -9639.9713\n",
            "Epoch 243 -kfold 0-n_latent 5 Loss: -9891.7325\n",
            "Epoch 244 -kfold 0-n_latent 5 Loss: -10001.2986\n",
            "Epoch 245 -kfold 0-n_latent 5 Loss: -9892.3324\n",
            "Epoch 246 -kfold 0-n_latent 5 Loss: -9887.1640\n",
            "Epoch 247 -kfold 0-n_latent 5 Loss: -10036.0116\n",
            "Epoch 248 -kfold 0-n_latent 5 Loss: -10144.0062\n",
            "Epoch 249 -kfold 0-n_latent 5 Loss: -10148.3986\n",
            "Epoch 250 -kfold 0-n_latent 5 Loss: -10023.7418\n",
            "Epoch 251 -kfold 0-n_latent 5 Loss: -10007.2950\n",
            "Epoch 252 -kfold 0-n_latent 5 Loss: -10091.9175\n",
            "Epoch 253 -kfold 0-n_latent 5 Loss: -10194.5936\n",
            "Epoch 254 -kfold 0-n_latent 5 Loss: -10228.5056\n",
            "Epoch 255 -kfold 0-n_latent 5 Loss: -10207.2153\n",
            "Epoch 256 -kfold 0-n_latent 5 Loss: -10376.4438\n",
            "Epoch 257 -kfold 0-n_latent 5 Loss: -10559.4786\n",
            "Epoch 258 -kfold 0-n_latent 5 Loss: -10137.3340\n",
            "Epoch 259 -kfold 0-n_latent 5 Loss: -9952.9317\n",
            "Epoch 260 -kfold 0-n_latent 5 Loss: -9426.1875\n",
            "Epoch 261 -kfold 0-n_latent 5 Loss: -9466.6932\n",
            "Epoch 262 -kfold 0-n_latent 5 Loss: -9347.7814\n",
            "Epoch 263 -kfold 0-n_latent 5 Loss: -9418.9421\n",
            "Epoch 264 -kfold 0-n_latent 5 Loss: -9515.0773\n",
            "Epoch 265 -kfold 0-n_latent 5 Loss: -9444.4553\n",
            "Epoch 266 -kfold 0-n_latent 5 Loss: -9746.5143\n",
            "Epoch 267 -kfold 0-n_latent 5 Loss: -9875.4201\n",
            "Epoch 268 -kfold 0-n_latent 5 Loss: -10056.9356\n",
            "Epoch 269 -kfold 0-n_latent 5 Loss: -10314.6642\n",
            "Epoch 270 -kfold 0-n_latent 5 Loss: -10160.0520\n",
            "Epoch 271 -kfold 0-n_latent 5 Loss: -9933.3627\n",
            "Epoch 272 -kfold 0-n_latent 5 Loss: -9577.9525\n",
            "Epoch 273 -kfold 0-n_latent 5 Loss: -9748.7937\n",
            "Epoch 274 -kfold 0-n_latent 5 Loss: -9982.7491\n",
            "Epoch 275 -kfold 0-n_latent 5 Loss: -9960.2688\n",
            "Epoch 276 -kfold 0-n_latent 5 Loss: -10253.9475\n",
            "Epoch 277 -kfold 0-n_latent 5 Loss: -10568.3874\n",
            "Epoch 278 -kfold 0-n_latent 5 Loss: -10645.1441\n",
            "Epoch 279 -kfold 0-n_latent 5 Loss: -10831.6043\n",
            "Epoch 280 -kfold 0-n_latent 5 Loss: -10678.7740\n",
            "Epoch 281 -kfold 0-n_latent 5 Loss: -10253.1911\n",
            "Epoch 282 -kfold 0-n_latent 5 Loss: -9827.4223\n",
            "Epoch 283 -kfold 0-n_latent 5 Loss: -9606.0689\n",
            "Epoch 284 -kfold 0-n_latent 5 Loss: -9614.4914\n",
            "Epoch 285 -kfold 0-n_latent 5 Loss: -9741.3032\n",
            "Epoch 286 -kfold 0-n_latent 5 Loss: -9700.9770\n",
            "Epoch 287 -kfold 0-n_latent 5 Loss: -9574.5501\n",
            "Epoch 288 -kfold 0-n_latent 5 Loss: -9867.2515\n",
            "Epoch 289 -kfold 0-n_latent 5 Loss: -9992.6223\n",
            "Epoch 290 -kfold 0-n_latent 5 Loss: -9433.3523\n",
            "Epoch 291 -kfold 0-n_latent 5 Loss: -9022.0888\n",
            "Epoch 292 -kfold 0-n_latent 5 Loss: -9237.5702\n",
            "Epoch 293 -kfold 0-n_latent 5 Loss: -9300.2994\n",
            "Epoch 294 -kfold 0-n_latent 5 Loss: -9313.6095\n",
            "Epoch 295 -kfold 0-n_latent 5 Loss: -9240.1610\n",
            "Epoch 296 -kfold 0-n_latent 5 Loss: -9578.1982\n",
            "Epoch 297 -kfold 0-n_latent 5 Loss: -9987.8291\n",
            "Epoch 298 -kfold 0-n_latent 5 Loss: -10201.6812\n",
            "Epoch 299 -kfold 0-n_latent 5 Loss: -10285.0232\n",
            "Epoch 300 -kfold 0-n_latent 5 Loss: -10368.2677\n",
            "Epoch 301 -kfold 0-n_latent 5 Loss: -10536.7447\n",
            "Epoch 302 -kfold 0-n_latent 5 Loss: -10477.0214\n",
            "Epoch 303 -kfold 0-n_latent 5 Loss: -10453.4772\n",
            "Epoch 304 -kfold 0-n_latent 5 Loss: -10318.0535\n",
            "Epoch 305 -kfold 0-n_latent 5 Loss: -10227.5618\n",
            "Epoch 306 -kfold 0-n_latent 5 Loss: -10520.9746\n",
            "Epoch 307 -kfold 0-n_latent 5 Loss: -10534.3379\n",
            "Epoch 308 -kfold 0-n_latent 5 Loss: -10717.0457\n",
            "Epoch 309 -kfold 0-n_latent 5 Loss: -10806.0320\n",
            "Epoch 310 -kfold 0-n_latent 5 Loss: -10776.6759\n",
            "Epoch 311 -kfold 0-n_latent 5 Loss: -10637.6847\n",
            "Epoch 312 -kfold 0-n_latent 5 Loss: -10331.1071\n",
            "Epoch 313 -kfold 0-n_latent 5 Loss: -10002.6065\n",
            "Epoch 314 -kfold 0-n_latent 5 Loss: -9855.5913\n",
            "Epoch 315 -kfold 0-n_latent 5 Loss: -9877.5463\n",
            "Epoch 316 -kfold 0-n_latent 5 Loss: -10079.5028\n",
            "Epoch 317 -kfold 0-n_latent 5 Loss: -10203.3407\n",
            "Epoch 318 -kfold 0-n_latent 5 Loss: -10258.9775\n",
            "Epoch 319 -kfold 0-n_latent 5 Loss: -10473.0232\n",
            "Epoch 320 -kfold 0-n_latent 5 Loss: -10676.0290\n",
            "Epoch 321 -kfold 0-n_latent 5 Loss: -10672.6053\n",
            "Epoch 322 -kfold 0-n_latent 5 Loss: -10490.5360\n",
            "Epoch 323 -kfold 0-n_latent 5 Loss: -9942.3268\n",
            "Epoch 324 -kfold 0-n_latent 5 Loss: -9474.5524\n",
            "Epoch 325 -kfold 0-n_latent 5 Loss: -9965.4481\n",
            "Epoch 326 -kfold 0-n_latent 5 Loss: -10024.3727\n",
            "Epoch 327 -kfold 0-n_latent 5 Loss: -10064.5268\n",
            "Epoch 328 -kfold 0-n_latent 5 Loss: -10333.8635\n",
            "Epoch 329 -kfold 0-n_latent 5 Loss: -10630.3422\n",
            "Epoch 330 -kfold 0-n_latent 5 Loss: -10881.5289\n",
            "Epoch 331 -kfold 0-n_latent 5 Loss: -10924.6745\n",
            "Epoch 332 -kfold 0-n_latent 5 Loss: -10842.4358\n",
            "Epoch 333 -kfold 0-n_latent 5 Loss: -10043.4959\n",
            "Epoch 334 -kfold 0-n_latent 5 Loss: -9097.0873\n",
            "Epoch 335 -kfold 0-n_latent 5 Loss: -8696.4411\n",
            "Epoch 336 -kfold 0-n_latent 5 Loss: -9000.6208\n",
            "Epoch 337 -kfold 0-n_latent 5 Loss: -9119.9290\n",
            "Epoch 338 -kfold 0-n_latent 5 Loss: -9015.4021\n",
            "Epoch 339 -kfold 0-n_latent 5 Loss: -9168.6844\n",
            "Epoch 340 -kfold 0-n_latent 5 Loss: -9493.2693\n",
            "Epoch 341 -kfold 0-n_latent 5 Loss: -10052.2784\n",
            "Epoch 342 -kfold 0-n_latent 5 Loss: -10592.6373\n",
            "Epoch 343 -kfold 0-n_latent 5 Loss: -10985.6755\n",
            "Epoch 344 -kfold 0-n_latent 5 Loss: -10859.9450\n",
            "Epoch 345 -kfold 0-n_latent 5 Loss: -9761.9869\n",
            "Epoch 346 -kfold 0-n_latent 5 Loss: -7901.8431\n",
            "Epoch 347 -kfold 0-n_latent 5 Loss: -7894.8147\n",
            "Epoch 348 -kfold 0-n_latent 5 Loss: -8902.6274\n",
            "Epoch 349 -kfold 0-n_latent 5 Loss: -8684.8681\n",
            "Epoch 350 -kfold 0-n_latent 5 Loss: -8841.3034\n",
            "Epoch 351 -kfold 0-n_latent 5 Loss: -8865.8691\n",
            "Epoch 352 -kfold 0-n_latent 5 Loss: -9125.2348\n",
            "Epoch 353 -kfold 0-n_latent 5 Loss: -9547.2970\n",
            "Epoch 354 -kfold 0-n_latent 5 Loss: -10109.0487\n",
            "Epoch 355 -kfold 0-n_latent 5 Loss: -10761.1914\n",
            "Epoch 356 -kfold 0-n_latent 5 Loss: -11102.1820\n",
            "Epoch 357 -kfold 0-n_latent 5 Loss: -10910.5065\n",
            "Epoch 358 -kfold 0-n_latent 5 Loss: -9711.7017\n",
            "Epoch 359 -kfold 0-n_latent 5 Loss: -8017.7645\n",
            "Epoch 360 -kfold 0-n_latent 5 Loss: -7558.2740\n",
            "Epoch 361 -kfold 0-n_latent 5 Loss: -8179.1591\n",
            "Epoch 362 -kfold 0-n_latent 5 Loss: -8265.9831\n",
            "Epoch 363 -kfold 0-n_latent 5 Loss: -8314.4011\n",
            "Epoch 364 -kfold 0-n_latent 5 Loss: -8336.9282\n",
            "Epoch 365 -kfold 0-n_latent 5 Loss: -8358.2846\n",
            "Epoch 366 -kfold 0-n_latent 5 Loss: -8480.5775\n",
            "Epoch 367 -kfold 0-n_latent 5 Loss: -8873.6735\n",
            "Epoch 368 -kfold 0-n_latent 5 Loss: -9536.8540\n",
            "Epoch 369 -kfold 0-n_latent 5 Loss: -10206.4227\n",
            "Epoch 370 -kfold 0-n_latent 5 Loss: -10700.6345\n",
            "Epoch 371 -kfold 0-n_latent 5 Loss: -10918.3723\n",
            "Epoch 372 -kfold 0-n_latent 5 Loss: -10532.8764\n",
            "Epoch 373 -kfold 0-n_latent 5 Loss: -8935.4517\n",
            "Epoch 374 -kfold 0-n_latent 5 Loss: -7098.3752\n",
            "Epoch 375 -kfold 0-n_latent 5 Loss: -7039.8523\n",
            "Epoch 376 -kfold 0-n_latent 5 Loss: -8164.9286\n",
            "Epoch 377 -kfold 0-n_latent 5 Loss: -8410.9332\n",
            "Epoch 378 -kfold 0-n_latent 5 Loss: -8683.5810\n",
            "Epoch 379 -kfold 0-n_latent 5 Loss: -8639.2584\n",
            "Epoch 380 -kfold 0-n_latent 5 Loss: -8614.5334\n",
            "Epoch 381 -kfold 0-n_latent 5 Loss: -8721.9531\n",
            "Epoch 382 -kfold 0-n_latent 5 Loss: -9139.4956\n",
            "Epoch 383 -kfold 0-n_latent 5 Loss: -9667.2316\n",
            "Epoch 384 -kfold 0-n_latent 5 Loss: -10281.5631\n",
            "Epoch 385 -kfold 0-n_latent 5 Loss: -10802.4165\n",
            "Epoch 386 -kfold 0-n_latent 5 Loss: -11011.9612\n",
            "Epoch 387 -kfold 0-n_latent 5 Loss: -10927.9620\n",
            "Epoch 388 -kfold 0-n_latent 5 Loss: -10285.9558\n",
            "Epoch 389 -kfold 0-n_latent 5 Loss: -9249.7617\n",
            "Epoch 390 -kfold 0-n_latent 5 Loss: -8002.6293\n",
            "Epoch 391 -kfold 0-n_latent 5 Loss: -7720.0573\n",
            "Epoch 392 -kfold 0-n_latent 5 Loss: -9205.9978\n",
            "Epoch 393 -kfold 0-n_latent 5 Loss: -9367.8040\n",
            "Epoch 394 -kfold 0-n_latent 5 Loss: -9519.7594\n",
            "Epoch 395 -kfold 0-n_latent 5 Loss: -9555.7677\n",
            "Epoch 396 -kfold 0-n_latent 5 Loss: -9545.2247\n",
            "Epoch 397 -kfold 0-n_latent 5 Loss: -9821.4115\n",
            "Epoch 398 -kfold 0-n_latent 5 Loss: -10109.7019\n",
            "Epoch 399 -kfold 0-n_latent 5 Loss: -10485.4824\n",
            "Epoch 400 -kfold 0-n_latent 5 Loss: -10949.0473\n",
            "Epoch 401 -kfold 0-n_latent 5 Loss: -11318.4295\n",
            "Epoch 402 -kfold 0-n_latent 5 Loss: -11367.6591\n",
            "Epoch 403 -kfold 0-n_latent 5 Loss: -10926.4514\n",
            "Epoch 404 -kfold 0-n_latent 5 Loss: -9313.7188\n",
            "Epoch 405 -kfold 0-n_latent 5 Loss: -7338.4293\n",
            "Epoch 406 -kfold 0-n_latent 5 Loss: -6495.6096\n",
            "Epoch 407 -kfold 0-n_latent 5 Loss: -7391.7061\n",
            "Epoch 408 -kfold 0-n_latent 5 Loss: -8337.0230\n",
            "Epoch 409 -kfold 0-n_latent 5 Loss: -8539.1477\n",
            "Epoch 410 -kfold 0-n_latent 5 Loss: -8719.7712\n",
            "Epoch 411 -kfold 0-n_latent 5 Loss: -8688.8802\n",
            "Epoch 412 -kfold 0-n_latent 5 Loss: -8815.4287\n",
            "Epoch 413 -kfold 0-n_latent 5 Loss: -9014.3345\n",
            "Epoch 414 -kfold 0-n_latent 5 Loss: -9295.9408\n",
            "Epoch 415 -kfold 0-n_latent 5 Loss: -9688.8772\n",
            "Epoch 416 -kfold 0-n_latent 5 Loss: -10273.5353\n",
            "Epoch 417 -kfold 0-n_latent 5 Loss: -10758.0830\n",
            "Epoch 418 -kfold 0-n_latent 5 Loss: -11003.7171\n",
            "Epoch 419 -kfold 0-n_latent 5 Loss: -10914.3646\n",
            "Epoch 420 -kfold 0-n_latent 5 Loss: -10515.4990\n",
            "Epoch 421 -kfold 0-n_latent 5 Loss: -9815.6322\n",
            "Epoch 422 -kfold 0-n_latent 5 Loss: -8777.5191\n",
            "Epoch 423 -kfold 0-n_latent 5 Loss: -8193.2061\n",
            "Epoch 424 -kfold 0-n_latent 5 Loss: -8194.0895\n",
            "Epoch 425 -kfold 0-n_latent 5 Loss: -9007.5648\n",
            "Epoch 426 -kfold 0-n_latent 5 Loss: -9228.3657\n",
            "Epoch 427 -kfold 0-n_latent 5 Loss: -9459.1432\n",
            "Epoch 428 -kfold 0-n_latent 5 Loss: -9684.0468\n",
            "Epoch 429 -kfold 0-n_latent 5 Loss: -9778.2226\n",
            "Epoch 430 -kfold 0-n_latent 5 Loss: -9947.9512\n",
            "Epoch 431 -kfold 0-n_latent 5 Loss: -10230.7991\n",
            "Epoch 432 -kfold 0-n_latent 5 Loss: -10520.2952\n",
            "Epoch 433 -kfold 0-n_latent 5 Loss: -10977.3086\n",
            "Epoch 434 -kfold 0-n_latent 5 Loss: -11298.1383\n",
            "Epoch 435 -kfold 0-n_latent 5 Loss: -11371.3482\n",
            "Epoch 436 -kfold 0-n_latent 5 Loss: -11234.9845\n",
            "Epoch 437 -kfold 0-n_latent 5 Loss: -10676.6034\n",
            "Epoch 438 -kfold 0-n_latent 5 Loss: -9303.7094\n",
            "Epoch 439 -kfold 0-n_latent 5 Loss: -8347.1254\n",
            "Epoch 440 -kfold 0-n_latent 5 Loss: -8460.4508\n",
            "Epoch 441 -kfold 0-n_latent 5 Loss: -8823.4245\n",
            "Epoch 442 -kfold 0-n_latent 5 Loss: -9300.8169\n",
            "Epoch 443 -kfold 0-n_latent 5 Loss: -9882.1223\n",
            "Epoch 444 -kfold 0-n_latent 5 Loss: -9890.0212\n",
            "Epoch 445 -kfold 0-n_latent 5 Loss: -10090.8745\n",
            "Epoch 446 -kfold 0-n_latent 5 Loss: -10161.9520\n",
            "Epoch 447 -kfold 0-n_latent 5 Loss: -10490.6810\n",
            "Epoch 448 -kfold 0-n_latent 5 Loss: -10713.7116\n",
            "Epoch 449 -kfold 0-n_latent 5 Loss: -11020.8986\n",
            "Epoch 450 -kfold 0-n_latent 5 Loss: -11351.1305\n",
            "Epoch 451 -kfold 0-n_latent 5 Loss: -11405.3231\n",
            "Epoch 452 -kfold 0-n_latent 5 Loss: -11474.8972\n",
            "Epoch 453 -kfold 0-n_latent 5 Loss: -11159.3099\n",
            "Epoch 454 -kfold 0-n_latent 5 Loss: -10176.1433\n",
            "Epoch 455 -kfold 0-n_latent 5 Loss: -9000.9415\n",
            "Epoch 456 -kfold 0-n_latent 5 Loss: -8107.8447\n",
            "Epoch 457 -kfold 0-n_latent 5 Loss: -8016.0507\n",
            "Epoch 458 -kfold 0-n_latent 5 Loss: -8443.1868\n",
            "Epoch 459 -kfold 0-n_latent 5 Loss: -8854.4157\n",
            "Epoch 460 -kfold 0-n_latent 5 Loss: -9101.4692\n",
            "Epoch 461 -kfold 0-n_latent 5 Loss: -9382.5400\n",
            "Epoch 462 -kfold 0-n_latent 5 Loss: -9391.5189\n",
            "Epoch 463 -kfold 0-n_latent 5 Loss: -9652.1875\n",
            "Epoch 464 -kfold 0-n_latent 5 Loss: -9855.7068\n",
            "Epoch 465 -kfold 0-n_latent 5 Loss: -10117.7473\n",
            "Epoch 466 -kfold 0-n_latent 5 Loss: -10527.2721\n",
            "Epoch 467 -kfold 0-n_latent 5 Loss: -10950.1446\n",
            "Epoch 468 -kfold 0-n_latent 5 Loss: -11306.0018\n",
            "Epoch 469 -kfold 0-n_latent 5 Loss: -11356.6115\n",
            "Epoch 470 -kfold 0-n_latent 5 Loss: -11104.4978\n",
            "Epoch 471 -kfold 0-n_latent 5 Loss: -9968.5162\n",
            "Epoch 472 -kfold 0-n_latent 5 Loss: -9108.7834\n",
            "Epoch 473 -kfold 0-n_latent 5 Loss: -8811.9434\n",
            "Epoch 474 -kfold 0-n_latent 5 Loss: -8865.7528\n",
            "Epoch 475 -kfold 0-n_latent 5 Loss: -8944.9738\n",
            "Epoch 476 -kfold 0-n_latent 5 Loss: -9374.8623\n",
            "Epoch 477 -kfold 0-n_latent 5 Loss: -9698.1891\n",
            "Epoch 478 -kfold 0-n_latent 5 Loss: -9586.2885\n",
            "Epoch 479 -kfold 0-n_latent 5 Loss: -9667.6743\n",
            "Epoch 480 -kfold 0-n_latent 5 Loss: -10034.1865\n",
            "Epoch 481 -kfold 0-n_latent 5 Loss: -10207.1541\n",
            "Epoch 482 -kfold 0-n_latent 5 Loss: -10492.2120\n",
            "Epoch 483 -kfold 0-n_latent 5 Loss: -10787.7499\n",
            "Epoch 484 -kfold 0-n_latent 5 Loss: -11009.7659\n",
            "Epoch 485 -kfold 0-n_latent 5 Loss: -11366.0738\n",
            "Epoch 486 -kfold 0-n_latent 5 Loss: -11298.1411\n",
            "Epoch 487 -kfold 0-n_latent 5 Loss: -11003.4330\n",
            "Epoch 488 -kfold 0-n_latent 5 Loss: -10105.4376\n",
            "Epoch 489 -kfold 0-n_latent 5 Loss: -9247.6549\n",
            "Epoch 490 -kfold 0-n_latent 5 Loss: -8358.6520\n",
            "Epoch 491 -kfold 0-n_latent 5 Loss: -8665.0992\n",
            "Epoch 492 -kfold 0-n_latent 5 Loss: -9101.9604\n",
            "Epoch 493 -kfold 0-n_latent 5 Loss: -9433.5778\n",
            "Epoch 494 -kfold 0-n_latent 5 Loss: -9978.8426\n",
            "Epoch 495 -kfold 0-n_latent 5 Loss: -10030.5649\n",
            "Epoch 496 -kfold 0-n_latent 5 Loss: -10247.8494\n",
            "Epoch 497 -kfold 0-n_latent 5 Loss: -10425.9482\n",
            "Epoch 498 -kfold 0-n_latent 5 Loss: -10607.8150\n",
            "Epoch 499 -kfold 0-n_latent 5 Loss: -10837.1814\n",
            "Epoch 500 -kfold 0-n_latent 5 Loss: -11131.9499\n",
            "Epoch 501 -kfold 0-n_latent 5 Loss: -11395.4770\n",
            "Epoch 502 -kfold 0-n_latent 5 Loss: -11584.4562\n",
            "Epoch 503 -kfold 0-n_latent 5 Loss: -11392.0680\n",
            "Epoch 504 -kfold 0-n_latent 5 Loss: -10941.3095\n",
            "Epoch 505 -kfold 0-n_latent 5 Loss: -10117.2580\n",
            "Epoch 506 -kfold 0-n_latent 5 Loss: -9134.6003\n",
            "Epoch 507 -kfold 0-n_latent 5 Loss: -8632.3140\n",
            "Epoch 508 -kfold 0-n_latent 5 Loss: -8316.3287\n",
            "Epoch 509 -kfold 0-n_latent 5 Loss: -7771.5256\n",
            "Epoch 510 -kfold 0-n_latent 5 Loss: -8639.8448\n",
            "Epoch 511 -kfold 0-n_latent 5 Loss: -9249.7600\n",
            "Epoch 512 -kfold 0-n_latent 5 Loss: -8509.4621\n",
            "Epoch 513 -kfold 0-n_latent 5 Loss: -7968.8952\n",
            "Epoch 514 -kfold 0-n_latent 5 Loss: -8414.0248\n",
            "Epoch 515 -kfold 0-n_latent 5 Loss: -8974.8939\n",
            "Epoch 516 -kfold 0-n_latent 5 Loss: -9268.8677\n",
            "Epoch 517 -kfold 0-n_latent 5 Loss: -9439.1660\n",
            "Epoch 518 -kfold 0-n_latent 5 Loss: -9685.6676\n",
            "Epoch 519 -kfold 0-n_latent 5 Loss: -9950.8435\n",
            "Epoch 520 -kfold 0-n_latent 5 Loss: -10200.9096\n",
            "Epoch 521 -kfold 0-n_latent 5 Loss: -10327.6234\n",
            "Epoch 522 -kfold 0-n_latent 5 Loss: -9822.6367\n",
            "Epoch 523 -kfold 0-n_latent 5 Loss: -9494.7340\n",
            "Epoch 524 -kfold 0-n_latent 5 Loss: -9319.2425\n",
            "Epoch 525 -kfold 0-n_latent 5 Loss: -9264.1377\n",
            "Epoch 526 -kfold 0-n_latent 5 Loss: -9696.3839\n",
            "Epoch 527 -kfold 0-n_latent 5 Loss: -9974.9522\n",
            "Epoch 528 -kfold 0-n_latent 5 Loss: -10256.2571\n",
            "Epoch 529 -kfold 0-n_latent 5 Loss: -10352.8657\n",
            "Epoch 530 -kfold 0-n_latent 5 Loss: -10479.5446\n",
            "Epoch 531 -kfold 0-n_latent 5 Loss: -10605.9051\n",
            "Epoch 532 -kfold 0-n_latent 5 Loss: -10662.9772\n",
            "Epoch 533 -kfold 0-n_latent 5 Loss: -10602.3900\n",
            "Epoch 534 -kfold 0-n_latent 5 Loss: -10718.6588\n",
            "Epoch 535 -kfold 0-n_latent 5 Loss: -10988.0625\n",
            "Epoch 536 -kfold 0-n_latent 5 Loss: -11355.9294\n",
            "Epoch 537 -kfold 0-n_latent 5 Loss: -11424.8712\n",
            "Epoch 538 -kfold 0-n_latent 5 Loss: -11531.1074\n",
            "Epoch 539 -kfold 0-n_latent 5 Loss: -11362.9571\n",
            "Epoch 540 -kfold 0-n_latent 5 Loss: -10798.1134\n",
            "Epoch 541 -kfold 0-n_latent 5 Loss: -10649.2615\n",
            "Epoch 542 -kfold 0-n_latent 5 Loss: -10591.0173\n",
            "Epoch 543 -kfold 0-n_latent 5 Loss: -10250.2379\n",
            "Epoch 544 -kfold 0-n_latent 5 Loss: -10176.7539\n",
            "Epoch 545 -kfold 0-n_latent 5 Loss: -10290.3487\n",
            "Epoch 546 -kfold 0-n_latent 5 Loss: -10303.5120\n",
            "Epoch 547 -kfold 0-n_latent 5 Loss: -10597.0249\n",
            "Epoch 548 -kfold 0-n_latent 5 Loss: -10858.1824\n",
            "Epoch 549 -kfold 0-n_latent 5 Loss: -11010.5245\n",
            "Epoch 550 -kfold 0-n_latent 5 Loss: -10979.0425\n",
            "Epoch 551 -kfold 0-n_latent 5 Loss: -11301.2643\n",
            "Epoch 552 -kfold 0-n_latent 5 Loss: -11585.8363\n",
            "Epoch 553 -kfold 0-n_latent 5 Loss: -11624.5503\n",
            "Epoch 554 -kfold 0-n_latent 5 Loss: -11674.2788\n",
            "Epoch 555 -kfold 0-n_latent 5 Loss: -11475.2123\n",
            "Epoch 556 -kfold 0-n_latent 5 Loss: -11088.1832\n",
            "Epoch 557 -kfold 0-n_latent 5 Loss: -10502.2596\n",
            "Epoch 558 -kfold 0-n_latent 5 Loss: -9725.7595\n",
            "Epoch 559 -kfold 0-n_latent 5 Loss: -9178.6389\n",
            "Epoch 560 -kfold 0-n_latent 5 Loss: -9394.8270\n",
            "Epoch 561 -kfold 0-n_latent 5 Loss: -9786.0379\n",
            "Epoch 562 -kfold 0-n_latent 5 Loss: -10068.6126\n",
            "Epoch 563 -kfold 0-n_latent 5 Loss: -10229.1908\n",
            "Epoch 564 -kfold 0-n_latent 5 Loss: -10620.9375\n",
            "Epoch 565 -kfold 0-n_latent 5 Loss: -10629.1921\n",
            "Epoch 566 -kfold 0-n_latent 5 Loss: -10956.8902\n",
            "Epoch 567 -kfold 0-n_latent 5 Loss: -11101.6474\n",
            "Epoch 568 -kfold 0-n_latent 5 Loss: -11313.5738\n",
            "Epoch 569 -kfold 0-n_latent 5 Loss: -11514.7337\n",
            "Epoch 570 -kfold 0-n_latent 5 Loss: -11739.4496\n",
            "Epoch 571 -kfold 0-n_latent 5 Loss: -11588.3123\n",
            "Epoch 572 -kfold 0-n_latent 5 Loss: -11024.6548\n",
            "Epoch 573 -kfold 0-n_latent 5 Loss: -10151.2926\n",
            "Epoch 574 -kfold 0-n_latent 5 Loss: -8858.6710\n",
            "Epoch 575 -kfold 0-n_latent 5 Loss: -8796.3078\n",
            "Epoch 576 -kfold 0-n_latent 5 Loss: -8935.6715\n",
            "Epoch 577 -kfold 0-n_latent 5 Loss: -9211.8545\n",
            "Epoch 578 -kfold 0-n_latent 5 Loss: -9605.6486\n",
            "Epoch 579 -kfold 0-n_latent 5 Loss: -9795.5244\n",
            "Epoch 580 -kfold 0-n_latent 5 Loss: -9888.4629\n",
            "Epoch 581 -kfold 0-n_latent 5 Loss: -10223.2307\n",
            "Epoch 582 -kfold 0-n_latent 5 Loss: -10391.9421\n",
            "Epoch 583 -kfold 0-n_latent 5 Loss: -10636.9391\n",
            "Epoch 584 -kfold 0-n_latent 5 Loss: -10960.8483\n",
            "Epoch 585 -kfold 0-n_latent 5 Loss: -11274.3251\n",
            "Epoch 586 -kfold 0-n_latent 5 Loss: -11505.5303\n",
            "Epoch 587 -kfold 0-n_latent 5 Loss: -11717.8191\n",
            "Epoch 588 -kfold 0-n_latent 5 Loss: -11833.9091\n",
            "Epoch 589 -kfold 0-n_latent 5 Loss: -11409.7564\n",
            "Epoch 590 -kfold 0-n_latent 5 Loss: -10320.0400\n",
            "Epoch 591 -kfold 0-n_latent 5 Loss: -9151.0740\n",
            "Epoch 592 -kfold 0-n_latent 5 Loss: -8592.1809\n",
            "Epoch 593 -kfold 0-n_latent 5 Loss: -8823.3502\n",
            "Epoch 594 -kfold 0-n_latent 5 Loss: -9535.7478\n",
            "Epoch 595 -kfold 0-n_latent 5 Loss: -9694.1415\n",
            "Epoch 596 -kfold 0-n_latent 5 Loss: -10029.9709\n",
            "Epoch 597 -kfold 0-n_latent 5 Loss: -10127.2830\n",
            "Epoch 598 -kfold 0-n_latent 5 Loss: -10356.4596\n",
            "Epoch 599 -kfold 0-n_latent 5 Loss: -10552.8037\n",
            "Epoch 600 -kfold 0-n_latent 5 Loss: -10798.5791\n",
            "Epoch 601 -kfold 0-n_latent 5 Loss: -11100.6295\n",
            "Epoch 602 -kfold 0-n_latent 5 Loss: -11387.0173\n",
            "Epoch 603 -kfold 0-n_latent 5 Loss: -11730.0997\n",
            "Epoch 604 -kfold 0-n_latent 5 Loss: -12064.7851\n",
            "Epoch 605 -kfold 0-n_latent 5 Loss: -12059.0738\n",
            "Epoch 606 -kfold 0-n_latent 5 Loss: -11365.9473\n",
            "Epoch 607 -kfold 0-n_latent 5 Loss: -9982.3009\n",
            "Epoch 608 -kfold 0-n_latent 5 Loss: -8633.3305\n",
            "Epoch 609 -kfold 0-n_latent 5 Loss: -7853.8635\n",
            "Epoch 610 -kfold 0-n_latent 5 Loss: -7628.3109\n",
            "Epoch 611 -kfold 0-n_latent 5 Loss: -8238.8812\n",
            "Epoch 612 -kfold 0-n_latent 5 Loss: -8193.3761\n",
            "Epoch 613 -kfold 0-n_latent 5 Loss: -8596.8990\n",
            "Epoch 614 -kfold 0-n_latent 5 Loss: -8632.7505\n",
            "Epoch 615 -kfold 0-n_latent 5 Loss: -8666.5943\n",
            "Epoch 616 -kfold 0-n_latent 5 Loss: -8899.5879\n",
            "Epoch 617 -kfold 0-n_latent 5 Loss: -9074.2703\n",
            "Epoch 618 -kfold 0-n_latent 5 Loss: -9342.8059\n",
            "Epoch 619 -kfold 0-n_latent 5 Loss: -9701.2696\n",
            "Epoch 620 -kfold 0-n_latent 5 Loss: -10168.4127\n",
            "Epoch 621 -kfold 0-n_latent 5 Loss: -10520.3472\n",
            "Epoch 622 -kfold 0-n_latent 5 Loss: -10899.1546\n",
            "Epoch 623 -kfold 0-n_latent 5 Loss: -11174.9165\n",
            "Epoch 624 -kfold 0-n_latent 5 Loss: -11128.4294\n",
            "Epoch 625 -kfold 0-n_latent 5 Loss: -10668.7049\n",
            "Epoch 626 -kfold 0-n_latent 5 Loss: -10024.4096\n",
            "Epoch 627 -kfold 0-n_latent 5 Loss: -9077.2510\n",
            "Epoch 628 -kfold 0-n_latent 5 Loss: -8629.2265\n",
            "Epoch 629 -kfold 0-n_latent 5 Loss: -8941.2298\n",
            "Epoch 630 -kfold 0-n_latent 5 Loss: -9465.0701\n",
            "Epoch 631 -kfold 0-n_latent 5 Loss: -9853.3607\n",
            "Epoch 632 -kfold 0-n_latent 5 Loss: -10058.1694\n",
            "Epoch 633 -kfold 0-n_latent 5 Loss: -10131.4277\n",
            "Epoch 634 -kfold 0-n_latent 5 Loss: -10449.8555\n",
            "Epoch 635 -kfold 0-n_latent 5 Loss: -10544.5466\n",
            "Epoch 636 -kfold 0-n_latent 5 Loss: -10686.8561\n",
            "Epoch 637 -kfold 0-n_latent 5 Loss: -10952.1521\n",
            "Epoch 638 -kfold 0-n_latent 5 Loss: -11280.0515\n",
            "Epoch 639 -kfold 0-n_latent 5 Loss: -11561.3340\n",
            "Epoch 640 -kfold 0-n_latent 5 Loss: -11740.8661\n",
            "Epoch 641 -kfold 0-n_latent 5 Loss: -11691.1259\n",
            "Epoch 642 -kfold 0-n_latent 5 Loss: -11390.4863\n",
            "Epoch 643 -kfold 0-n_latent 5 Loss: -10650.2743\n",
            "Epoch 644 -kfold 0-n_latent 5 Loss: -9576.8367\n",
            "Epoch 645 -kfold 0-n_latent 5 Loss: -9078.6060\n",
            "Epoch 646 -kfold 0-n_latent 5 Loss: -8816.1346\n",
            "Epoch 647 -kfold 0-n_latent 5 Loss: -8937.5505\n",
            "Epoch 648 -kfold 0-n_latent 5 Loss: -9482.7851\n",
            "Epoch 649 -kfold 0-n_latent 5 Loss: -9727.0050\n",
            "Epoch 650 -kfold 0-n_latent 5 Loss: -9930.4715\n",
            "Epoch 651 -kfold 0-n_latent 5 Loss: -10027.3757\n",
            "Epoch 652 -kfold 0-n_latent 5 Loss: -10292.3515\n",
            "Epoch 653 -kfold 0-n_latent 5 Loss: -10423.1454\n",
            "Epoch 654 -kfold 0-n_latent 5 Loss: -10684.4884\n",
            "Epoch 655 -kfold 0-n_latent 5 Loss: -10900.8293\n",
            "Epoch 656 -kfold 0-n_latent 5 Loss: -11178.9433\n",
            "Epoch 657 -kfold 0-n_latent 5 Loss: -11590.1255\n",
            "Epoch 658 -kfold 0-n_latent 5 Loss: -11797.8178\n",
            "Epoch 659 -kfold 0-n_latent 5 Loss: -11610.8808\n",
            "Epoch 660 -kfold 0-n_latent 5 Loss: -11146.9237\n",
            "Epoch 661 -kfold 0-n_latent 5 Loss: -10650.2540\n",
            "Epoch 662 -kfold 0-n_latent 5 Loss: -9919.0868\n",
            "Epoch 663 -kfold 0-n_latent 5 Loss: -9454.9062\n",
            "Epoch 664 -kfold 0-n_latent 5 Loss: -9525.2160\n",
            "Epoch 665 -kfold 0-n_latent 5 Loss: -9976.0921\n",
            "Epoch 666 -kfold 0-n_latent 5 Loss: -10163.5007\n",
            "Epoch 667 -kfold 0-n_latent 5 Loss: -10273.1837\n",
            "Epoch 668 -kfold 0-n_latent 5 Loss: -10429.4500\n",
            "Epoch 669 -kfold 0-n_latent 5 Loss: -10656.8981\n",
            "Epoch 670 -kfold 0-n_latent 5 Loss: -10765.5247\n",
            "Epoch 671 -kfold 0-n_latent 5 Loss: -11022.2383\n",
            "Epoch 672 -kfold 0-n_latent 5 Loss: -11146.5363\n",
            "Epoch 673 -kfold 0-n_latent 5 Loss: -11421.5307\n",
            "Epoch 674 -kfold 0-n_latent 5 Loss: -11593.2625\n",
            "Epoch 675 -kfold 0-n_latent 5 Loss: -11721.6727\n",
            "Epoch 676 -kfold 0-n_latent 5 Loss: -11702.3876\n",
            "Epoch 677 -kfold 0-n_latent 5 Loss: -11439.6736\n",
            "Epoch 678 -kfold 0-n_latent 5 Loss: -10972.8178\n",
            "Epoch 679 -kfold 0-n_latent 5 Loss: -10106.2853\n",
            "Epoch 680 -kfold 0-n_latent 5 Loss: -9372.8556\n",
            "Epoch 681 -kfold 0-n_latent 5 Loss: -9322.8082\n",
            "Epoch 682 -kfold 0-n_latent 5 Loss: -9548.8497\n",
            "Epoch 683 -kfold 0-n_latent 5 Loss: -9779.2246\n",
            "Epoch 684 -kfold 0-n_latent 5 Loss: -10136.2559\n",
            "Epoch 685 -kfold 0-n_latent 5 Loss: -10267.9847\n",
            "Epoch 686 -kfold 0-n_latent 5 Loss: -10706.2257\n",
            "Epoch 687 -kfold 0-n_latent 5 Loss: -10842.7160\n",
            "Epoch 688 -kfold 0-n_latent 5 Loss: -10878.0835\n",
            "Epoch 689 -kfold 0-n_latent 5 Loss: -10978.9905\n",
            "Epoch 690 -kfold 0-n_latent 5 Loss: -11253.4961\n",
            "Epoch 691 -kfold 0-n_latent 5 Loss: -11567.1336\n",
            "Epoch 692 -kfold 0-n_latent 5 Loss: -11796.1633\n",
            "Epoch 693 -kfold 0-n_latent 5 Loss: -11868.2076\n",
            "Epoch 694 -kfold 0-n_latent 5 Loss: -11514.6687\n",
            "Epoch 695 -kfold 0-n_latent 5 Loss: -11217.9387\n",
            "Epoch 696 -kfold 0-n_latent 5 Loss: -10666.6450\n",
            "Epoch 697 -kfold 0-n_latent 5 Loss: -10335.0644\n",
            "Epoch 698 -kfold 0-n_latent 5 Loss: -10175.1994\n",
            "Epoch 699 -kfold 0-n_latent 5 Loss: -9904.4582\n",
            "6\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 0-n_latent 6 Loss:  26437.3115\n",
            "Epoch 2 -kfold 0-n_latent 6 Loss:  24587.4431\n",
            "Epoch 3 -kfold 0-n_latent 6 Loss:  22939.0646\n",
            "Epoch 4 -kfold 0-n_latent 6 Loss:  21330.0388\n",
            "Epoch 5 -kfold 0-n_latent 6 Loss:  19702.1421\n",
            "Epoch 6 -kfold 0-n_latent 6 Loss:  18062.4067\n",
            "Epoch 7 -kfold 0-n_latent 6 Loss:  16442.0186\n",
            "Epoch 8 -kfold 0-n_latent 6 Loss:  14833.3635\n",
            "Epoch 9 -kfold 0-n_latent 6 Loss:  13233.7772\n",
            "Epoch 10 -kfold 0-n_latent 6 Loss:  11737.3626\n",
            "Epoch 11 -kfold 0-n_latent 6 Loss:  10313.6398\n",
            "Epoch 12 -kfold 0-n_latent 6 Loss:  9023.4833\n",
            "Epoch 13 -kfold 0-n_latent 6 Loss:  7838.1211\n",
            "Epoch 14 -kfold 0-n_latent 6 Loss:  6819.4749\n",
            "Epoch 15 -kfold 0-n_latent 6 Loss:  5986.5459\n",
            "Epoch 16 -kfold 0-n_latent 6 Loss:  5166.7748\n",
            "Epoch 17 -kfold 0-n_latent 6 Loss:  4361.5999\n",
            "Epoch 18 -kfold 0-n_latent 6 Loss:  3591.1533\n",
            "Epoch 19 -kfold 0-n_latent 6 Loss:  2888.5975\n",
            "Epoch 20 -kfold 0-n_latent 6 Loss:  2229.5511\n",
            "Epoch 21 -kfold 0-n_latent 6 Loss:  1600.6103\n",
            "Epoch 22 -kfold 0-n_latent 6 Loss:  988.4833\n",
            "Epoch 23 -kfold 0-n_latent 6 Loss:  460.3160\n",
            "Epoch 24 -kfold 0-n_latent 6 Loss: -117.8866\n",
            "Epoch 25 -kfold 0-n_latent 6 Loss: -606.1286\n",
            "Epoch 26 -kfold 0-n_latent 6 Loss: -1156.1748\n",
            "Epoch 27 -kfold 0-n_latent 6 Loss: -1608.5308\n",
            "Epoch 28 -kfold 0-n_latent 6 Loss: -2098.5514\n",
            "Epoch 29 -kfold 0-n_latent 6 Loss: -2572.7813\n",
            "Epoch 30 -kfold 0-n_latent 6 Loss: -3034.7757\n",
            "Epoch 31 -kfold 0-n_latent 6 Loss: -3456.7755\n",
            "Epoch 32 -kfold 0-n_latent 6 Loss: -3891.3720\n",
            "Epoch 33 -kfold 0-n_latent 6 Loss: -4207.2132\n",
            "Epoch 34 -kfold 0-n_latent 6 Loss: -4617.0287\n",
            "Epoch 35 -kfold 0-n_latent 6 Loss: -4997.6586\n",
            "Epoch 36 -kfold 0-n_latent 6 Loss: -5262.0675\n",
            "Epoch 37 -kfold 0-n_latent 6 Loss: -5624.1276\n",
            "Epoch 38 -kfold 0-n_latent 6 Loss: -5963.1474\n",
            "Epoch 39 -kfold 0-n_latent 6 Loss: -6258.5831\n",
            "Epoch 40 -kfold 0-n_latent 6 Loss: -6459.5306\n",
            "Epoch 41 -kfold 0-n_latent 6 Loss: -6690.9579\n",
            "Epoch 42 -kfold 0-n_latent 6 Loss: -6795.4410\n",
            "Epoch 43 -kfold 0-n_latent 6 Loss: -7008.5429\n",
            "Epoch 44 -kfold 0-n_latent 6 Loss: -7184.3638\n",
            "Epoch 45 -kfold 0-n_latent 6 Loss: -7327.4978\n",
            "Epoch 46 -kfold 0-n_latent 6 Loss: -7372.5852\n",
            "Epoch 47 -kfold 0-n_latent 6 Loss: -7400.4934\n",
            "Epoch 48 -kfold 0-n_latent 6 Loss: -7622.4734\n",
            "Epoch 49 -kfold 0-n_latent 6 Loss: -7856.4062\n",
            "Epoch 50 -kfold 0-n_latent 6 Loss: -8151.5437\n",
            "Epoch 51 -kfold 0-n_latent 6 Loss: -8176.7995\n",
            "Epoch 52 -kfold 0-n_latent 6 Loss: -8449.7369\n",
            "Epoch 53 -kfold 0-n_latent 6 Loss: -8541.3343\n",
            "Epoch 54 -kfold 0-n_latent 6 Loss: -8684.0261\n",
            "Epoch 55 -kfold 0-n_latent 6 Loss: -8827.3563\n",
            "Epoch 56 -kfold 0-n_latent 6 Loss: -9118.9539\n",
            "Epoch 57 -kfold 0-n_latent 6 Loss: -9086.2742\n",
            "Epoch 58 -kfold 0-n_latent 6 Loss: -9244.5139\n",
            "Epoch 59 -kfold 0-n_latent 6 Loss: -9508.2992\n",
            "Epoch 60 -kfold 0-n_latent 6 Loss: -9466.4613\n",
            "Epoch 61 -kfold 0-n_latent 6 Loss: -9429.5985\n",
            "Epoch 62 -kfold 0-n_latent 6 Loss: -9660.9116\n",
            "Epoch 63 -kfold 0-n_latent 6 Loss: -9726.7689\n",
            "Epoch 64 -kfold 0-n_latent 6 Loss: -9749.2278\n",
            "Epoch 65 -kfold 0-n_latent 6 Loss: -9714.9230\n",
            "Epoch 66 -kfold 0-n_latent 6 Loss: -9915.9319\n",
            "Epoch 67 -kfold 0-n_latent 6 Loss: -9958.6911\n",
            "Epoch 68 -kfold 0-n_latent 6 Loss: -10100.5948\n",
            "Epoch 69 -kfold 0-n_latent 6 Loss: -10060.8449\n",
            "Epoch 70 -kfold 0-n_latent 6 Loss: -9993.9864\n",
            "Epoch 71 -kfold 0-n_latent 6 Loss: -10176.2545\n",
            "Epoch 72 -kfold 0-n_latent 6 Loss: -10322.8690\n",
            "Epoch 73 -kfold 0-n_latent 6 Loss: -10572.5123\n",
            "Epoch 74 -kfold 0-n_latent 6 Loss: -10711.7965\n",
            "Epoch 75 -kfold 0-n_latent 6 Loss: -10611.3591\n",
            "Epoch 76 -kfold 0-n_latent 6 Loss: -10647.4116\n",
            "Epoch 77 -kfold 0-n_latent 6 Loss: -10682.5283\n",
            "Epoch 78 -kfold 0-n_latent 6 Loss: -10692.3201\n",
            "Epoch 79 -kfold 0-n_latent 6 Loss: -10763.2245\n",
            "Epoch 80 -kfold 0-n_latent 6 Loss: -10889.7589\n",
            "Epoch 81 -kfold 0-n_latent 6 Loss: -11038.9243\n",
            "Epoch 82 -kfold 0-n_latent 6 Loss: -10973.8639\n",
            "Epoch 83 -kfold 0-n_latent 6 Loss: -11094.2277\n",
            "Epoch 84 -kfold 0-n_latent 6 Loss: -11247.1281\n",
            "Epoch 85 -kfold 0-n_latent 6 Loss: -11357.9346\n",
            "Epoch 86 -kfold 0-n_latent 6 Loss: -11586.7200\n",
            "Epoch 87 -kfold 0-n_latent 6 Loss: -11375.2687\n",
            "Epoch 88 -kfold 0-n_latent 6 Loss: -11209.3277\n",
            "Epoch 89 -kfold 0-n_latent 6 Loss: -11098.7918\n",
            "Epoch 90 -kfold 0-n_latent 6 Loss: -10903.7015\n",
            "Epoch 91 -kfold 0-n_latent 6 Loss: -10940.1571\n",
            "Epoch 92 -kfold 0-n_latent 6 Loss: -11012.4028\n",
            "Epoch 93 -kfold 0-n_latent 6 Loss: -11114.7816\n",
            "Epoch 94 -kfold 0-n_latent 6 Loss: -11458.9286\n",
            "Epoch 95 -kfold 0-n_latent 6 Loss: -11432.3736\n",
            "Epoch 96 -kfold 0-n_latent 6 Loss: -11702.4249\n",
            "Epoch 97 -kfold 0-n_latent 6 Loss: -11819.3215\n",
            "Epoch 98 -kfold 0-n_latent 6 Loss: -11598.6957\n",
            "Epoch 99 -kfold 0-n_latent 6 Loss: -11484.7283\n",
            "Epoch 100 -kfold 0-n_latent 6 Loss: -11677.7663\n",
            "Epoch 101 -kfold 0-n_latent 6 Loss: -11570.1551\n",
            "Epoch 102 -kfold 0-n_latent 6 Loss: -11442.4359\n",
            "Epoch 103 -kfold 0-n_latent 6 Loss: -11562.1303\n",
            "Epoch 104 -kfold 0-n_latent 6 Loss: -11640.6684\n",
            "Epoch 105 -kfold 0-n_latent 6 Loss: -11781.0950\n",
            "Epoch 106 -kfold 0-n_latent 6 Loss: -11832.6051\n",
            "Epoch 107 -kfold 0-n_latent 6 Loss: -12023.2435\n",
            "Epoch 108 -kfold 0-n_latent 6 Loss: -12083.1524\n",
            "Epoch 109 -kfold 0-n_latent 6 Loss: -12120.6381\n",
            "Epoch 110 -kfold 0-n_latent 6 Loss: -12114.9256\n",
            "Epoch 111 -kfold 0-n_latent 6 Loss: -12216.5932\n",
            "Epoch 112 -kfold 0-n_latent 6 Loss: -12255.5150\n",
            "Epoch 113 -kfold 0-n_latent 6 Loss: -12213.5023\n",
            "Epoch 114 -kfold 0-n_latent 6 Loss: -12041.8857\n",
            "Epoch 115 -kfold 0-n_latent 6 Loss: -11865.4627\n",
            "Epoch 116 -kfold 0-n_latent 6 Loss: -11652.1082\n",
            "Epoch 117 -kfold 0-n_latent 6 Loss: -11792.4411\n",
            "Epoch 118 -kfold 0-n_latent 6 Loss: -12186.9290\n",
            "Epoch 119 -kfold 0-n_latent 6 Loss: -12341.4635\n",
            "Epoch 120 -kfold 0-n_latent 6 Loss: -12532.3967\n",
            "Epoch 121 -kfold 0-n_latent 6 Loss: -12426.3171\n",
            "Epoch 122 -kfold 0-n_latent 6 Loss: -12410.6741\n",
            "Epoch 123 -kfold 0-n_latent 6 Loss: -12247.4797\n",
            "Epoch 124 -kfold 0-n_latent 6 Loss: -12466.3059\n",
            "Epoch 125 -kfold 0-n_latent 6 Loss: -12446.8824\n",
            "Epoch 126 -kfold 0-n_latent 6 Loss: -12440.3540\n",
            "Epoch 127 -kfold 0-n_latent 6 Loss: -12627.8181\n",
            "Epoch 128 -kfold 0-n_latent 6 Loss: -12633.6684\n",
            "Epoch 129 -kfold 0-n_latent 6 Loss: -12328.6286\n",
            "Epoch 130 -kfold 0-n_latent 6 Loss: -12437.7839\n",
            "Epoch 131 -kfold 0-n_latent 6 Loss: -12293.3161\n",
            "Epoch 132 -kfold 0-n_latent 6 Loss: -12191.3355\n",
            "Epoch 133 -kfold 0-n_latent 6 Loss: -12209.4247\n",
            "Epoch 134 -kfold 0-n_latent 6 Loss: -12204.1697\n",
            "Epoch 135 -kfold 0-n_latent 6 Loss: -12312.6781\n",
            "Epoch 136 -kfold 0-n_latent 6 Loss: -12629.3436\n",
            "Epoch 137 -kfold 0-n_latent 6 Loss: -12811.0511\n",
            "Epoch 138 -kfold 0-n_latent 6 Loss: -12910.9466\n",
            "Epoch 139 -kfold 0-n_latent 6 Loss: -13040.4832\n",
            "Epoch 140 -kfold 0-n_latent 6 Loss: -12607.3922\n",
            "Epoch 141 -kfold 0-n_latent 6 Loss: -12749.2932\n",
            "Epoch 142 -kfold 0-n_latent 6 Loss: -12764.7074\n",
            "Epoch 143 -kfold 0-n_latent 6 Loss: -12678.9415\n",
            "Epoch 144 -kfold 0-n_latent 6 Loss: -12739.1926\n",
            "Epoch 145 -kfold 0-n_latent 6 Loss: -12888.1106\n",
            "Epoch 146 -kfold 0-n_latent 6 Loss: -12788.3527\n",
            "Epoch 147 -kfold 0-n_latent 6 Loss: -12778.0134\n",
            "Epoch 148 -kfold 0-n_latent 6 Loss: -12912.3489\n",
            "Epoch 149 -kfold 0-n_latent 6 Loss: -13018.4294\n",
            "Epoch 150 -kfold 0-n_latent 6 Loss: -13018.0497\n",
            "Epoch 151 -kfold 0-n_latent 6 Loss: -13073.0474\n",
            "Epoch 152 -kfold 0-n_latent 6 Loss: -13263.4677\n",
            "Epoch 153 -kfold 0-n_latent 6 Loss: -13297.1433\n",
            "Epoch 154 -kfold 0-n_latent 6 Loss: -13514.9602\n",
            "Epoch 155 -kfold 0-n_latent 6 Loss: -13331.7845\n",
            "Epoch 156 -kfold 0-n_latent 6 Loss: -13091.7758\n",
            "Epoch 157 -kfold 0-n_latent 6 Loss: -12910.3474\n",
            "Epoch 158 -kfold 0-n_latent 6 Loss: -12880.1288\n",
            "Epoch 159 -kfold 0-n_latent 6 Loss: -12827.8057\n",
            "Epoch 160 -kfold 0-n_latent 6 Loss: -12884.9541\n",
            "Epoch 161 -kfold 0-n_latent 6 Loss: -13061.8201\n",
            "Epoch 162 -kfold 0-n_latent 6 Loss: -13300.0875\n",
            "Epoch 163 -kfold 0-n_latent 6 Loss: -13396.6113\n",
            "Epoch 164 -kfold 0-n_latent 6 Loss: -13497.4883\n",
            "Epoch 165 -kfold 0-n_latent 6 Loss: -13503.1783\n",
            "Epoch 166 -kfold 0-n_latent 6 Loss: -13192.1191\n",
            "Epoch 167 -kfold 0-n_latent 6 Loss: -13272.9368\n",
            "Epoch 168 -kfold 0-n_latent 6 Loss: -13321.9333\n",
            "Epoch 169 -kfold 0-n_latent 6 Loss: -13166.8825\n",
            "Epoch 170 -kfold 0-n_latent 6 Loss: -13173.9552\n",
            "Epoch 171 -kfold 0-n_latent 6 Loss: -13184.3853\n",
            "Epoch 172 -kfold 0-n_latent 6 Loss: -13262.4754\n",
            "Epoch 173 -kfold 0-n_latent 6 Loss: -13169.9902\n",
            "Epoch 174 -kfold 0-n_latent 6 Loss: -13326.5342\n",
            "Epoch 175 -kfold 0-n_latent 6 Loss: -13395.2065\n",
            "Epoch 176 -kfold 0-n_latent 6 Loss: -13078.8033\n",
            "Epoch 177 -kfold 0-n_latent 6 Loss: -13349.1573\n",
            "Epoch 178 -kfold 0-n_latent 6 Loss: -13137.8539\n",
            "Epoch 179 -kfold 0-n_latent 6 Loss: -13110.1831\n",
            "Epoch 180 -kfold 0-n_latent 6 Loss: -13275.7567\n",
            "Epoch 181 -kfold 0-n_latent 6 Loss: -13180.9532\n",
            "Epoch 182 -kfold 0-n_latent 6 Loss: -13350.9974\n",
            "Epoch 183 -kfold 0-n_latent 6 Loss: -13134.3302\n",
            "Epoch 184 -kfold 0-n_latent 6 Loss: -13143.0857\n",
            "Epoch 185 -kfold 0-n_latent 6 Loss: -13126.5945\n",
            "Epoch 186 -kfold 0-n_latent 6 Loss: -13208.7747\n",
            "Epoch 187 -kfold 0-n_latent 6 Loss: -13439.0564\n",
            "Epoch 188 -kfold 0-n_latent 6 Loss: -13137.8974\n",
            "Epoch 189 -kfold 0-n_latent 6 Loss: -13243.4532\n",
            "Epoch 190 -kfold 0-n_latent 6 Loss: -13573.3053\n",
            "Epoch 191 -kfold 0-n_latent 6 Loss: -13392.7533\n",
            "Epoch 192 -kfold 0-n_latent 6 Loss: -13627.6001\n",
            "Epoch 193 -kfold 0-n_latent 6 Loss: -13766.5317\n",
            "Epoch 194 -kfold 0-n_latent 6 Loss: -13708.1640\n",
            "Epoch 195 -kfold 0-n_latent 6 Loss: -13624.2540\n",
            "Epoch 196 -kfold 0-n_latent 6 Loss: -13460.3114\n",
            "Epoch 197 -kfold 0-n_latent 6 Loss: -13527.0665\n",
            "Epoch 198 -kfold 0-n_latent 6 Loss: -13401.3425\n",
            "Epoch 199 -kfold 0-n_latent 6 Loss: -13514.8871\n",
            "Epoch 200 -kfold 0-n_latent 6 Loss: -13649.0172\n",
            "Epoch 201 -kfold 0-n_latent 6 Loss: -13622.0049\n",
            "Epoch 202 -kfold 0-n_latent 6 Loss: -13761.7923\n",
            "Epoch 203 -kfold 0-n_latent 6 Loss: -13843.4129\n",
            "Epoch 204 -kfold 0-n_latent 6 Loss: -13799.2473\n",
            "Epoch 205 -kfold 0-n_latent 6 Loss: -13624.1030\n",
            "Epoch 206 -kfold 0-n_latent 6 Loss: -12993.4707\n",
            "Epoch 207 -kfold 0-n_latent 6 Loss: -12848.7367\n",
            "Epoch 208 -kfold 0-n_latent 6 Loss: -12965.9537\n",
            "Epoch 209 -kfold 0-n_latent 6 Loss: -12982.0953\n",
            "Epoch 210 -kfold 0-n_latent 6 Loss: -12795.4687\n",
            "Epoch 211 -kfold 0-n_latent 6 Loss: -12715.7451\n",
            "Epoch 212 -kfold 0-n_latent 6 Loss: -12902.9003\n",
            "Epoch 213 -kfold 0-n_latent 6 Loss: -13234.9564\n",
            "Epoch 214 -kfold 0-n_latent 6 Loss: -13768.7933\n",
            "Epoch 215 -kfold 0-n_latent 6 Loss: -14013.7443\n",
            "Epoch 216 -kfold 0-n_latent 6 Loss: -13664.8759\n",
            "Epoch 217 -kfold 0-n_latent 6 Loss: -13348.1664\n",
            "Epoch 218 -kfold 0-n_latent 6 Loss: -13036.1419\n",
            "Epoch 219 -kfold 0-n_latent 6 Loss: -13150.2485\n",
            "Epoch 220 -kfold 0-n_latent 6 Loss: -13445.0760\n",
            "Epoch 221 -kfold 0-n_latent 6 Loss: -13500.5078\n",
            "Epoch 222 -kfold 0-n_latent 6 Loss: -13742.1441\n",
            "Epoch 223 -kfold 0-n_latent 6 Loss: -14235.6382\n",
            "Epoch 224 -kfold 0-n_latent 6 Loss: -14270.5844\n",
            "Epoch 225 -kfold 0-n_latent 6 Loss: -14348.1384\n",
            "Epoch 226 -kfold 0-n_latent 6 Loss: -13985.5478\n",
            "Epoch 227 -kfold 0-n_latent 6 Loss: -12836.2729\n",
            "Epoch 228 -kfold 0-n_latent 6 Loss: -12825.2338\n",
            "Epoch 229 -kfold 0-n_latent 6 Loss: -13035.5996\n",
            "Epoch 230 -kfold 0-n_latent 6 Loss: -12758.4969\n",
            "Epoch 231 -kfold 0-n_latent 6 Loss: -12681.4930\n",
            "Epoch 232 -kfold 0-n_latent 6 Loss: -13045.3207\n",
            "Epoch 233 -kfold 0-n_latent 6 Loss: -13395.2127\n",
            "Epoch 234 -kfold 0-n_latent 6 Loss: -13834.9184\n",
            "Epoch 235 -kfold 0-n_latent 6 Loss: -13904.6647\n",
            "Epoch 236 -kfold 0-n_latent 6 Loss: -13744.2921\n",
            "Epoch 237 -kfold 0-n_latent 6 Loss: -13185.8994\n",
            "Epoch 238 -kfold 0-n_latent 6 Loss: -12781.7497\n",
            "Epoch 239 -kfold 0-n_latent 6 Loss: -13156.7650\n",
            "Epoch 240 -kfold 0-n_latent 6 Loss: -13186.0330\n",
            "Epoch 241 -kfold 0-n_latent 6 Loss: -13303.0544\n",
            "Epoch 242 -kfold 0-n_latent 6 Loss: -13668.0410\n",
            "Epoch 243 -kfold 0-n_latent 6 Loss: -13914.1772\n",
            "Epoch 244 -kfold 0-n_latent 6 Loss: -14210.7396\n",
            "Epoch 245 -kfold 0-n_latent 6 Loss: -14433.8420\n",
            "Epoch 246 -kfold 0-n_latent 6 Loss: -14317.7253\n",
            "Epoch 247 -kfold 0-n_latent 6 Loss: -13829.0135\n",
            "Epoch 248 -kfold 0-n_latent 6 Loss: -12465.6967\n",
            "Epoch 249 -kfold 0-n_latent 6 Loss: -12369.9355\n",
            "Epoch 250 -kfold 0-n_latent 6 Loss: -12470.4955\n",
            "Epoch 251 -kfold 0-n_latent 6 Loss: -12831.9383\n",
            "Epoch 252 -kfold 0-n_latent 6 Loss: -13016.6093\n",
            "Epoch 253 -kfold 0-n_latent 6 Loss: -13124.7560\n",
            "Epoch 254 -kfold 0-n_latent 6 Loss: -13678.3315\n",
            "Epoch 255 -kfold 0-n_latent 6 Loss: -14170.5128\n",
            "Epoch 256 -kfold 0-n_latent 6 Loss: -14706.1942\n",
            "Epoch 257 -kfold 0-n_latent 6 Loss: -14981.9190\n",
            "Epoch 258 -kfold 0-n_latent 6 Loss: -14595.1109\n",
            "Epoch 259 -kfold 0-n_latent 6 Loss: -12871.4429\n",
            "Epoch 260 -kfold 0-n_latent 6 Loss: -10493.2302\n",
            "Epoch 261 -kfold 0-n_latent 6 Loss: -10864.3125\n",
            "Epoch 262 -kfold 0-n_latent 6 Loss: -11637.8648\n",
            "Epoch 263 -kfold 0-n_latent 6 Loss: -11398.4828\n",
            "Epoch 264 -kfold 0-n_latent 6 Loss: -11570.3734\n",
            "Epoch 265 -kfold 0-n_latent 6 Loss: -11486.4911\n",
            "Epoch 266 -kfold 0-n_latent 6 Loss: -11827.7976\n",
            "Epoch 267 -kfold 0-n_latent 6 Loss: -12424.7579\n",
            "Epoch 268 -kfold 0-n_latent 6 Loss: -13124.5858\n",
            "Epoch 269 -kfold 0-n_latent 6 Loss: -13924.4594\n",
            "Epoch 270 -kfold 0-n_latent 6 Loss: -14553.7016\n",
            "Epoch 271 -kfold 0-n_latent 6 Loss: -14769.1636\n",
            "Epoch 272 -kfold 0-n_latent 6 Loss: -14313.6970\n",
            "Epoch 273 -kfold 0-n_latent 6 Loss: -11905.5840\n",
            "Epoch 274 -kfold 0-n_latent 6 Loss: -8326.0438\n",
            "Epoch 275 -kfold 0-n_latent 6 Loss: -7261.7419\n",
            "Epoch 276 -kfold 0-n_latent 6 Loss: -10032.7984\n",
            "Epoch 277 -kfold 0-n_latent 6 Loss: -10077.5956\n",
            "Epoch 278 -kfold 0-n_latent 6 Loss: -10552.0274\n",
            "Epoch 279 -kfold 0-n_latent 6 Loss: -10282.7555\n",
            "Epoch 280 -kfold 0-n_latent 6 Loss: -10036.4884\n",
            "Epoch 281 -kfold 0-n_latent 6 Loss: -10112.6296\n",
            "Epoch 282 -kfold 0-n_latent 6 Loss: -10475.9807\n",
            "Epoch 283 -kfold 0-n_latent 6 Loss: -11074.8677\n",
            "Epoch 284 -kfold 0-n_latent 6 Loss: -11926.9639\n",
            "Epoch 285 -kfold 0-n_latent 6 Loss: -12870.0391\n",
            "Epoch 286 -kfold 0-n_latent 6 Loss: -13688.6204\n",
            "Epoch 287 -kfold 0-n_latent 6 Loss: -14280.7016\n",
            "Epoch 288 -kfold 0-n_latent 6 Loss: -14409.8803\n",
            "Epoch 289 -kfold 0-n_latent 6 Loss: -13494.7439\n",
            "Epoch 290 -kfold 0-n_latent 6 Loss: -11402.9118\n",
            "Epoch 291 -kfold 0-n_latent 6 Loss: -9449.3436\n",
            "Epoch 292 -kfold 0-n_latent 6 Loss: -8664.7801\n",
            "Epoch 293 -kfold 0-n_latent 6 Loss: -9833.1781\n",
            "Epoch 294 -kfold 0-n_latent 6 Loss: -10645.5744\n",
            "Epoch 295 -kfold 0-n_latent 6 Loss: -11068.4445\n",
            "Epoch 296 -kfold 0-n_latent 6 Loss: -11145.3177\n",
            "Epoch 297 -kfold 0-n_latent 6 Loss: -11013.5310\n",
            "Epoch 298 -kfold 0-n_latent 6 Loss: -11081.5111\n",
            "Epoch 299 -kfold 0-n_latent 6 Loss: -11229.9853\n",
            "Epoch 300 -kfold 0-n_latent 6 Loss: -11505.2363\n",
            "Epoch 301 -kfold 0-n_latent 6 Loss: -12030.7450\n",
            "Epoch 302 -kfold 0-n_latent 6 Loss: -12688.5575\n",
            "Epoch 303 -kfold 0-n_latent 6 Loss: -13381.2937\n",
            "Epoch 304 -kfold 0-n_latent 6 Loss: -13993.5226\n",
            "Epoch 305 -kfold 0-n_latent 6 Loss: -14502.6329\n",
            "Epoch 306 -kfold 0-n_latent 6 Loss: -14301.2404\n",
            "Epoch 307 -kfold 0-n_latent 6 Loss: -13186.6751\n",
            "Epoch 308 -kfold 0-n_latent 6 Loss: -11767.0125\n",
            "Epoch 309 -kfold 0-n_latent 6 Loss: -11013.8270\n",
            "Epoch 310 -kfold 0-n_latent 6 Loss: -10503.7003\n",
            "Epoch 311 -kfold 0-n_latent 6 Loss: -10881.3677\n",
            "Epoch 312 -kfold 0-n_latent 6 Loss: -11040.1339\n",
            "Epoch 313 -kfold 0-n_latent 6 Loss: -11790.4476\n",
            "Epoch 314 -kfold 0-n_latent 6 Loss: -12205.4947\n",
            "Epoch 315 -kfold 0-n_latent 6 Loss: -12385.3678\n",
            "Epoch 316 -kfold 0-n_latent 6 Loss: -12367.7250\n",
            "Epoch 317 -kfold 0-n_latent 6 Loss: -12502.5618\n",
            "Epoch 318 -kfold 0-n_latent 6 Loss: -12816.9266\n",
            "Epoch 319 -kfold 0-n_latent 6 Loss: -13301.0853\n",
            "Epoch 320 -kfold 0-n_latent 6 Loss: -13719.1675\n",
            "Epoch 321 -kfold 0-n_latent 6 Loss: -14248.9125\n",
            "Epoch 322 -kfold 0-n_latent 6 Loss: -14765.5938\n",
            "Epoch 323 -kfold 0-n_latent 6 Loss: -15096.1947\n",
            "Epoch 324 -kfold 0-n_latent 6 Loss: -14797.9634\n",
            "Epoch 325 -kfold 0-n_latent 6 Loss: -13591.9265\n",
            "Epoch 326 -kfold 0-n_latent 6 Loss: -12002.8096\n",
            "Epoch 327 -kfold 0-n_latent 6 Loss: -11451.1754\n",
            "Epoch 328 -kfold 0-n_latent 6 Loss: -10790.6153\n",
            "Epoch 329 -kfold 0-n_latent 6 Loss: -10934.2970\n",
            "Epoch 330 -kfold 0-n_latent 6 Loss: -11744.6856\n",
            "Epoch 331 -kfold 0-n_latent 6 Loss: -12255.9155\n",
            "Epoch 332 -kfold 0-n_latent 6 Loss: -12589.9875\n",
            "Epoch 333 -kfold 0-n_latent 6 Loss: -12603.2866\n",
            "Epoch 334 -kfold 0-n_latent 6 Loss: -12804.9389\n",
            "Epoch 335 -kfold 0-n_latent 6 Loss: -13013.3118\n",
            "Epoch 336 -kfold 0-n_latent 6 Loss: -13377.0053\n",
            "Epoch 337 -kfold 0-n_latent 6 Loss: -13821.8747\n",
            "Epoch 338 -kfold 0-n_latent 6 Loss: -14133.0495\n",
            "Epoch 339 -kfold 0-n_latent 6 Loss: -14571.3676\n",
            "Epoch 340 -kfold 0-n_latent 6 Loss: -15084.2773\n",
            "Epoch 341 -kfold 0-n_latent 6 Loss: -15273.0144\n",
            "Epoch 342 -kfold 0-n_latent 6 Loss: -14950.6843\n",
            "Epoch 343 -kfold 0-n_latent 6 Loss: -13881.0640\n",
            "Epoch 344 -kfold 0-n_latent 6 Loss: -12853.4115\n",
            "Epoch 345 -kfold 0-n_latent 6 Loss: -12236.6636\n",
            "Epoch 346 -kfold 0-n_latent 6 Loss: -11392.2961\n",
            "Epoch 347 -kfold 0-n_latent 6 Loss: -11211.5640\n",
            "Epoch 348 -kfold 0-n_latent 6 Loss: -11464.8519\n",
            "Epoch 349 -kfold 0-n_latent 6 Loss: -12213.9257\n",
            "Epoch 350 -kfold 0-n_latent 6 Loss: -12344.3273\n",
            "Epoch 351 -kfold 0-n_latent 6 Loss: -12669.6121\n",
            "Epoch 352 -kfold 0-n_latent 6 Loss: -12847.8877\n",
            "Epoch 353 -kfold 0-n_latent 6 Loss: -13131.3490\n",
            "Epoch 354 -kfold 0-n_latent 6 Loss: -13334.6904\n",
            "Epoch 355 -kfold 0-n_latent 6 Loss: -13571.8093\n",
            "Epoch 356 -kfold 0-n_latent 6 Loss: -14130.6512\n",
            "Epoch 357 -kfold 0-n_latent 6 Loss: -14609.9870\n",
            "Epoch 358 -kfold 0-n_latent 6 Loss: -14918.9535\n",
            "Epoch 359 -kfold 0-n_latent 6 Loss: -15235.1456\n",
            "Epoch 360 -kfold 0-n_latent 6 Loss: -15070.1693\n",
            "Epoch 361 -kfold 0-n_latent 6 Loss: -14235.6799\n",
            "Epoch 362 -kfold 0-n_latent 6 Loss: -12482.4792\n",
            "Epoch 363 -kfold 0-n_latent 6 Loss: -10992.1596\n",
            "Epoch 364 -kfold 0-n_latent 6 Loss: -10430.9550\n",
            "Epoch 365 -kfold 0-n_latent 6 Loss: -10903.3052\n",
            "Epoch 366 -kfold 0-n_latent 6 Loss: -11487.5081\n",
            "Epoch 367 -kfold 0-n_latent 6 Loss: -11884.5087\n",
            "Epoch 368 -kfold 0-n_latent 6 Loss: -12619.9499\n",
            "Epoch 369 -kfold 0-n_latent 6 Loss: -12570.6672\n",
            "Epoch 370 -kfold 0-n_latent 6 Loss: -12890.4192\n",
            "Epoch 371 -kfold 0-n_latent 6 Loss: -13012.9670\n",
            "Epoch 372 -kfold 0-n_latent 6 Loss: -13234.8139\n",
            "Epoch 373 -kfold 0-n_latent 6 Loss: -13596.0871\n",
            "Epoch 374 -kfold 0-n_latent 6 Loss: -13951.0113\n",
            "Epoch 375 -kfold 0-n_latent 6 Loss: -14416.5019\n",
            "Epoch 376 -kfold 0-n_latent 6 Loss: -14946.1970\n",
            "Epoch 377 -kfold 0-n_latent 6 Loss: -15308.4000\n",
            "Epoch 378 -kfold 0-n_latent 6 Loss: -15282.3759\n",
            "Epoch 379 -kfold 0-n_latent 6 Loss: -14912.6303\n",
            "Epoch 380 -kfold 0-n_latent 6 Loss: -13854.4651\n",
            "Epoch 381 -kfold 0-n_latent 6 Loss: -12386.2250\n",
            "Epoch 382 -kfold 0-n_latent 6 Loss: -10674.9144\n",
            "Epoch 383 -kfold 0-n_latent 6 Loss: -10246.6359\n",
            "Epoch 384 -kfold 0-n_latent 6 Loss: -11011.0436\n",
            "Epoch 385 -kfold 0-n_latent 6 Loss: -11751.8742\n",
            "Epoch 386 -kfold 0-n_latent 6 Loss: -12137.8551\n",
            "Epoch 387 -kfold 0-n_latent 6 Loss: -12471.9098\n",
            "Epoch 388 -kfold 0-n_latent 6 Loss: -12742.0081\n",
            "Epoch 389 -kfold 0-n_latent 6 Loss: -12763.0043\n",
            "Epoch 390 -kfold 0-n_latent 6 Loss: -12891.0870\n",
            "Epoch 391 -kfold 0-n_latent 6 Loss: -13173.9020\n",
            "Epoch 392 -kfold 0-n_latent 6 Loss: -13561.9960\n",
            "Epoch 393 -kfold 0-n_latent 6 Loss: -14005.1142\n",
            "Epoch 394 -kfold 0-n_latent 6 Loss: -14431.0473\n",
            "Epoch 395 -kfold 0-n_latent 6 Loss: -14918.4502\n",
            "Epoch 396 -kfold 0-n_latent 6 Loss: -15181.8317\n",
            "Epoch 397 -kfold 0-n_latent 6 Loss: -15253.2800\n",
            "Epoch 398 -kfold 0-n_latent 6 Loss: -14551.1155\n",
            "Epoch 399 -kfold 0-n_latent 6 Loss: -13062.7668\n",
            "Epoch 400 -kfold 0-n_latent 6 Loss: -11738.7725\n",
            "Epoch 401 -kfold 0-n_latent 6 Loss: -11650.6590\n",
            "Epoch 402 -kfold 0-n_latent 6 Loss: -11321.4259\n",
            "Epoch 403 -kfold 0-n_latent 6 Loss: -11635.7990\n",
            "Epoch 404 -kfold 0-n_latent 6 Loss: -12477.8164\n",
            "Epoch 405 -kfold 0-n_latent 6 Loss: -12833.7108\n",
            "Epoch 406 -kfold 0-n_latent 6 Loss: -13155.0610\n",
            "Epoch 407 -kfold 0-n_latent 6 Loss: -13399.4817\n",
            "Epoch 408 -kfold 0-n_latent 6 Loss: -13512.0339\n",
            "Epoch 409 -kfold 0-n_latent 6 Loss: -13722.0621\n",
            "Epoch 410 -kfold 0-n_latent 6 Loss: -14020.5446\n",
            "Epoch 411 -kfold 0-n_latent 6 Loss: -14421.3915\n",
            "Epoch 412 -kfold 0-n_latent 6 Loss: -14819.5888\n",
            "Epoch 413 -kfold 0-n_latent 6 Loss: -15223.9782\n",
            "Epoch 414 -kfold 0-n_latent 6 Loss: -15618.1831\n",
            "Epoch 415 -kfold 0-n_latent 6 Loss: -15721.0740\n",
            "Epoch 416 -kfold 0-n_latent 6 Loss: -15395.9687\n",
            "Epoch 417 -kfold 0-n_latent 6 Loss: -14032.8787\n",
            "Epoch 418 -kfold 0-n_latent 6 Loss: -12331.1821\n",
            "Epoch 419 -kfold 0-n_latent 6 Loss: -11245.9370\n",
            "Epoch 420 -kfold 0-n_latent 6 Loss: -10766.5697\n",
            "Epoch 421 -kfold 0-n_latent 6 Loss: -11187.5203\n",
            "Epoch 422 -kfold 0-n_latent 6 Loss: -11859.1021\n",
            "Epoch 423 -kfold 0-n_latent 6 Loss: -12504.4180\n",
            "Epoch 424 -kfold 0-n_latent 6 Loss: -12757.2058\n",
            "Epoch 425 -kfold 0-n_latent 6 Loss: -13055.5980\n",
            "Epoch 426 -kfold 0-n_latent 6 Loss: -13328.6429\n",
            "Epoch 427 -kfold 0-n_latent 6 Loss: -13549.8317\n",
            "Epoch 428 -kfold 0-n_latent 6 Loss: -13755.6847\n",
            "Epoch 429 -kfold 0-n_latent 6 Loss: -14023.9989\n",
            "Epoch 430 -kfold 0-n_latent 6 Loss: -14433.6764\n",
            "Epoch 431 -kfold 0-n_latent 6 Loss: -15014.1733\n",
            "Epoch 432 -kfold 0-n_latent 6 Loss: -15303.1084\n",
            "Epoch 433 -kfold 0-n_latent 6 Loss: -15505.8823\n",
            "Epoch 434 -kfold 0-n_latent 6 Loss: -15270.9450\n",
            "Epoch 435 -kfold 0-n_latent 6 Loss: -14525.5734\n",
            "Epoch 436 -kfold 0-n_latent 6 Loss: -13671.8167\n",
            "Epoch 437 -kfold 0-n_latent 6 Loss: -12754.0254\n",
            "Epoch 438 -kfold 0-n_latent 6 Loss: -11555.4542\n",
            "Epoch 439 -kfold 0-n_latent 6 Loss: -11311.0239\n",
            "Epoch 440 -kfold 0-n_latent 6 Loss: -11006.9078\n",
            "Epoch 441 -kfold 0-n_latent 6 Loss: -11785.2653\n",
            "Epoch 442 -kfold 0-n_latent 6 Loss: -12197.7659\n",
            "Epoch 443 -kfold 0-n_latent 6 Loss: -12640.3209\n",
            "Epoch 444 -kfold 0-n_latent 6 Loss: -12864.9262\n",
            "Epoch 445 -kfold 0-n_latent 6 Loss: -13058.9031\n",
            "Epoch 446 -kfold 0-n_latent 6 Loss: -13229.1209\n",
            "Epoch 447 -kfold 0-n_latent 6 Loss: -13450.9246\n",
            "Epoch 448 -kfold 0-n_latent 6 Loss: -13824.5197\n",
            "Epoch 449 -kfold 0-n_latent 6 Loss: -14213.6877\n",
            "Epoch 450 -kfold 0-n_latent 6 Loss: -14675.4910\n",
            "Epoch 451 -kfold 0-n_latent 6 Loss: -15163.0611\n",
            "Epoch 452 -kfold 0-n_latent 6 Loss: -15455.1898\n",
            "Epoch 453 -kfold 0-n_latent 6 Loss: -15406.8121\n",
            "Epoch 454 -kfold 0-n_latent 6 Loss: -14788.3263\n",
            "Epoch 455 -kfold 0-n_latent 6 Loss: -13819.8239\n",
            "Epoch 456 -kfold 0-n_latent 6 Loss: -12351.6310\n",
            "Epoch 457 -kfold 0-n_latent 6 Loss: -11118.9559\n",
            "Epoch 458 -kfold 0-n_latent 6 Loss: -10296.2564\n",
            "Epoch 459 -kfold 0-n_latent 6 Loss: -10691.3211\n",
            "Epoch 460 -kfold 0-n_latent 6 Loss: -11281.2949\n",
            "Epoch 461 -kfold 0-n_latent 6 Loss: -12000.2537\n",
            "Epoch 462 -kfold 0-n_latent 6 Loss: -12552.5172\n",
            "Epoch 463 -kfold 0-n_latent 6 Loss: -12862.6077\n",
            "Epoch 464 -kfold 0-n_latent 6 Loss: -12921.1160\n",
            "Epoch 465 -kfold 0-n_latent 6 Loss: -13042.0558\n",
            "Epoch 466 -kfold 0-n_latent 6 Loss: -13353.3600\n",
            "Epoch 467 -kfold 0-n_latent 6 Loss: -13599.7269\n",
            "Epoch 468 -kfold 0-n_latent 6 Loss: -13985.6089\n",
            "Epoch 469 -kfold 0-n_latent 6 Loss: -14429.8111\n",
            "Epoch 470 -kfold 0-n_latent 6 Loss: -14825.9524\n",
            "Epoch 471 -kfold 0-n_latent 6 Loss: -15227.0452\n",
            "Epoch 472 -kfold 0-n_latent 6 Loss: -15423.2531\n",
            "Epoch 473 -kfold 0-n_latent 6 Loss: -15212.7529\n",
            "Epoch 474 -kfold 0-n_latent 6 Loss: -14447.6895\n",
            "Epoch 475 -kfold 0-n_latent 6 Loss: -13404.0888\n",
            "Epoch 476 -kfold 0-n_latent 6 Loss: -12640.5974\n",
            "Epoch 477 -kfold 0-n_latent 6 Loss: -11735.1983\n",
            "Epoch 478 -kfold 0-n_latent 6 Loss: -11463.8288\n",
            "Epoch 479 -kfold 0-n_latent 6 Loss: -11722.9425\n",
            "Epoch 480 -kfold 0-n_latent 6 Loss: -12368.8241\n",
            "Epoch 481 -kfold 0-n_latent 6 Loss: -13003.4642\n",
            "Epoch 482 -kfold 0-n_latent 6 Loss: -13418.0397\n",
            "Epoch 483 -kfold 0-n_latent 6 Loss: -13497.3218\n",
            "Epoch 484 -kfold 0-n_latent 6 Loss: -13726.2540\n",
            "Epoch 485 -kfold 0-n_latent 6 Loss: -13933.6864\n",
            "Epoch 486 -kfold 0-n_latent 6 Loss: -14258.2485\n",
            "Epoch 487 -kfold 0-n_latent 6 Loss: -14466.7085\n",
            "Epoch 488 -kfold 0-n_latent 6 Loss: -14825.4620\n",
            "Epoch 489 -kfold 0-n_latent 6 Loss: -15233.7870\n",
            "Epoch 490 -kfold 0-n_latent 6 Loss: -15545.0845\n",
            "Epoch 491 -kfold 0-n_latent 6 Loss: -15801.3850\n",
            "Epoch 492 -kfold 0-n_latent 6 Loss: -15540.5209\n",
            "Epoch 493 -kfold 0-n_latent 6 Loss: -14746.0900\n",
            "Epoch 494 -kfold 0-n_latent 6 Loss: -13171.2122\n",
            "Epoch 495 -kfold 0-n_latent 6 Loss: -11911.0790\n",
            "Epoch 496 -kfold 0-n_latent 6 Loss: -10798.7953\n",
            "Epoch 497 -kfold 0-n_latent 6 Loss: -11036.3038\n",
            "Epoch 498 -kfold 0-n_latent 6 Loss: -11589.6215\n",
            "Epoch 499 -kfold 0-n_latent 6 Loss: -12398.3483\n",
            "Epoch 500 -kfold 0-n_latent 6 Loss: -12566.7596\n",
            "Epoch 501 -kfold 0-n_latent 6 Loss: -12819.7016\n",
            "Epoch 502 -kfold 0-n_latent 6 Loss: -13102.3902\n",
            "Epoch 503 -kfold 0-n_latent 6 Loss: -13476.3212\n",
            "Epoch 504 -kfold 0-n_latent 6 Loss: -13606.1469\n",
            "Epoch 505 -kfold 0-n_latent 6 Loss: -13784.7288\n",
            "Epoch 506 -kfold 0-n_latent 6 Loss: -14140.5579\n",
            "Epoch 507 -kfold 0-n_latent 6 Loss: -14626.2333\n",
            "Epoch 508 -kfold 0-n_latent 6 Loss: -15045.0931\n",
            "Epoch 509 -kfold 0-n_latent 6 Loss: -15367.3871\n",
            "Epoch 510 -kfold 0-n_latent 6 Loss: -15764.2435\n",
            "Epoch 511 -kfold 0-n_latent 6 Loss: -15657.4291\n",
            "Epoch 512 -kfold 0-n_latent 6 Loss: -14823.2268\n",
            "Epoch 513 -kfold 0-n_latent 6 Loss: -13477.8849\n",
            "Epoch 514 -kfold 0-n_latent 6 Loss: -12401.9116\n",
            "Epoch 515 -kfold 0-n_latent 6 Loss: -11195.0051\n",
            "Epoch 516 -kfold 0-n_latent 6 Loss: -10262.7894\n",
            "Epoch 517 -kfold 0-n_latent 6 Loss: -10428.4422\n",
            "Epoch 518 -kfold 0-n_latent 6 Loss: -11297.1959\n",
            "Epoch 519 -kfold 0-n_latent 6 Loss: -11881.0729\n",
            "Epoch 520 -kfold 0-n_latent 6 Loss: -12464.5312\n",
            "Epoch 521 -kfold 0-n_latent 6 Loss: -12664.8040\n",
            "Epoch 522 -kfold 0-n_latent 6 Loss: -12778.6171\n",
            "Epoch 523 -kfold 0-n_latent 6 Loss: -12861.9628\n",
            "Epoch 524 -kfold 0-n_latent 6 Loss: -13162.6351\n",
            "Epoch 525 -kfold 0-n_latent 6 Loss: -13503.4769\n",
            "Epoch 526 -kfold 0-n_latent 6 Loss: -13848.8043\n",
            "Epoch 527 -kfold 0-n_latent 6 Loss: -14316.1338\n",
            "Epoch 528 -kfold 0-n_latent 6 Loss: -14708.2591\n",
            "Epoch 529 -kfold 0-n_latent 6 Loss: -15123.4946\n",
            "Epoch 530 -kfold 0-n_latent 6 Loss: -15270.0890\n",
            "Epoch 531 -kfold 0-n_latent 6 Loss: -15155.0909\n",
            "Epoch 532 -kfold 0-n_latent 6 Loss: -14579.9789\n",
            "Epoch 533 -kfold 0-n_latent 6 Loss: -13552.5685\n",
            "Epoch 534 -kfold 0-n_latent 6 Loss: -12854.0329\n",
            "Epoch 535 -kfold 0-n_latent 6 Loss: -11954.0965\n",
            "Epoch 536 -kfold 0-n_latent 6 Loss: -11660.3383\n",
            "Epoch 537 -kfold 0-n_latent 6 Loss: -11619.0778\n",
            "Epoch 538 -kfold 0-n_latent 6 Loss: -12509.0936\n",
            "Epoch 539 -kfold 0-n_latent 6 Loss: -13060.9437\n",
            "Epoch 540 -kfold 0-n_latent 6 Loss: -13428.4240\n",
            "Epoch 541 -kfold 0-n_latent 6 Loss: -13795.2493\n",
            "Epoch 542 -kfold 0-n_latent 6 Loss: -14036.8749\n",
            "Epoch 543 -kfold 0-n_latent 6 Loss: -14218.6916\n",
            "Epoch 544 -kfold 0-n_latent 6 Loss: -14409.0596\n",
            "Epoch 545 -kfold 0-n_latent 6 Loss: -14742.5247\n",
            "Epoch 546 -kfold 0-n_latent 6 Loss: -15094.9649\n",
            "Epoch 547 -kfold 0-n_latent 6 Loss: -15419.6169\n",
            "Epoch 548 -kfold 0-n_latent 6 Loss: -15759.5252\n",
            "Epoch 549 -kfold 0-n_latent 6 Loss: -15908.3322\n",
            "Epoch 550 -kfold 0-n_latent 6 Loss: -15758.4945\n",
            "Epoch 551 -kfold 0-n_latent 6 Loss: -15285.6795\n",
            "Epoch 552 -kfold 0-n_latent 6 Loss: -14607.3386\n",
            "Epoch 553 -kfold 0-n_latent 6 Loss: -13799.3031\n",
            "Epoch 554 -kfold 0-n_latent 6 Loss: -12570.3668\n",
            "Epoch 555 -kfold 0-n_latent 6 Loss: -11227.0121\n",
            "Epoch 556 -kfold 0-n_latent 6 Loss: -11502.6962\n",
            "Epoch 557 -kfold 0-n_latent 6 Loss: -12025.5094\n",
            "Epoch 558 -kfold 0-n_latent 6 Loss: -12350.7008\n",
            "Epoch 559 -kfold 0-n_latent 6 Loss: -13046.5244\n",
            "Epoch 560 -kfold 0-n_latent 6 Loss: -13366.7777\n",
            "Epoch 561 -kfold 0-n_latent 6 Loss: -13479.8097\n",
            "Epoch 562 -kfold 0-n_latent 6 Loss: -13707.9799\n",
            "Epoch 563 -kfold 0-n_latent 6 Loss: -13722.9671\n",
            "Epoch 564 -kfold 0-n_latent 6 Loss: -14036.2389\n",
            "Epoch 565 -kfold 0-n_latent 6 Loss: -14401.0302\n",
            "Epoch 566 -kfold 0-n_latent 6 Loss: -14811.1919\n",
            "Epoch 567 -kfold 0-n_latent 6 Loss: -15146.8822\n",
            "Epoch 568 -kfold 0-n_latent 6 Loss: -15553.2625\n",
            "Epoch 569 -kfold 0-n_latent 6 Loss: -15774.9558\n",
            "Epoch 570 -kfold 0-n_latent 6 Loss: -15282.6673\n",
            "Epoch 571 -kfold 0-n_latent 6 Loss: -14234.3080\n",
            "Epoch 572 -kfold 0-n_latent 6 Loss: -13599.9804\n",
            "Epoch 573 -kfold 0-n_latent 6 Loss: -12702.3869\n",
            "Epoch 574 -kfold 0-n_latent 6 Loss: -11243.9844\n",
            "Epoch 575 -kfold 0-n_latent 6 Loss: -11687.2314\n",
            "Epoch 576 -kfold 0-n_latent 6 Loss: -11983.6302\n",
            "Epoch 577 -kfold 0-n_latent 6 Loss: -12270.3605\n",
            "Epoch 578 -kfold 0-n_latent 6 Loss: -12843.2041\n",
            "Epoch 579 -kfold 0-n_latent 6 Loss: -13059.9563\n",
            "Epoch 580 -kfold 0-n_latent 6 Loss: -13200.3828\n",
            "Epoch 581 -kfold 0-n_latent 6 Loss: -13552.9916\n",
            "Epoch 582 -kfold 0-n_latent 6 Loss: -13792.6894\n",
            "Epoch 583 -kfold 0-n_latent 6 Loss: -14125.9550\n",
            "Epoch 584 -kfold 0-n_latent 6 Loss: -14369.6245\n",
            "Epoch 585 -kfold 0-n_latent 6 Loss: -14737.0373\n",
            "Epoch 586 -kfold 0-n_latent 6 Loss: -15098.7490\n",
            "Epoch 587 -kfold 0-n_latent 6 Loss: -15494.2410\n",
            "Epoch 588 -kfold 0-n_latent 6 Loss: -15664.0587\n",
            "Epoch 589 -kfold 0-n_latent 6 Loss: -15568.9334\n",
            "Epoch 590 -kfold 0-n_latent 6 Loss: -14868.3700\n",
            "Epoch 591 -kfold 0-n_latent 6 Loss: -13896.3388\n",
            "Epoch 592 -kfold 0-n_latent 6 Loss: -13054.6257\n",
            "Epoch 593 -kfold 0-n_latent 6 Loss: -12995.7789\n",
            "Epoch 594 -kfold 0-n_latent 6 Loss: -12684.5668\n",
            "Epoch 595 -kfold 0-n_latent 6 Loss: -12671.6736\n",
            "Epoch 596 -kfold 0-n_latent 6 Loss: -12989.6553\n",
            "Epoch 597 -kfold 0-n_latent 6 Loss: -13436.4414\n",
            "Epoch 598 -kfold 0-n_latent 6 Loss: -13816.3488\n",
            "Epoch 599 -kfold 0-n_latent 6 Loss: -14172.6479\n",
            "Epoch 600 -kfold 0-n_latent 6 Loss: -14474.9762\n",
            "Epoch 601 -kfold 0-n_latent 6 Loss: -14766.6036\n",
            "Epoch 602 -kfold 0-n_latent 6 Loss: -14950.3700\n",
            "Epoch 603 -kfold 0-n_latent 6 Loss: -15129.3593\n",
            "Epoch 604 -kfold 0-n_latent 6 Loss: -15406.2649\n",
            "Epoch 605 -kfold 0-n_latent 6 Loss: -15756.0254\n",
            "Epoch 606 -kfold 0-n_latent 6 Loss: -15959.7631\n",
            "Epoch 607 -kfold 0-n_latent 6 Loss: -15909.7781\n",
            "Epoch 608 -kfold 0-n_latent 6 Loss: -15669.5847\n",
            "Epoch 609 -kfold 0-n_latent 6 Loss: -14997.1347\n",
            "Epoch 610 -kfold 0-n_latent 6 Loss: -13762.7008\n",
            "Epoch 611 -kfold 0-n_latent 6 Loss: -12752.2224\n",
            "Epoch 612 -kfold 0-n_latent 6 Loss: -12332.6178\n",
            "Epoch 613 -kfold 0-n_latent 6 Loss: -12371.4267\n",
            "Epoch 614 -kfold 0-n_latent 6 Loss: -12568.4896\n",
            "Epoch 615 -kfold 0-n_latent 6 Loss: -12897.2995\n",
            "Epoch 616 -kfold 0-n_latent 6 Loss: -13160.1083\n",
            "Epoch 617 -kfold 0-n_latent 6 Loss: -13686.8031\n",
            "Epoch 618 -kfold 0-n_latent 6 Loss: -13854.3376\n",
            "Epoch 619 -kfold 0-n_latent 6 Loss: -14195.0525\n",
            "Epoch 620 -kfold 0-n_latent 6 Loss: -14226.5074\n",
            "Epoch 621 -kfold 0-n_latent 6 Loss: -14424.4731\n",
            "Epoch 622 -kfold 0-n_latent 6 Loss: -14730.1749\n",
            "Epoch 623 -kfold 0-n_latent 6 Loss: -15102.1125\n",
            "Epoch 624 -kfold 0-n_latent 6 Loss: -15427.0109\n",
            "Epoch 625 -kfold 0-n_latent 6 Loss: -15418.0473\n",
            "Epoch 626 -kfold 0-n_latent 6 Loss: -15451.5371\n",
            "Epoch 627 -kfold 0-n_latent 6 Loss: -15247.5346\n",
            "Epoch 628 -kfold 0-n_latent 6 Loss: -14719.7685\n",
            "Epoch 629 -kfold 0-n_latent 6 Loss: -13934.6027\n",
            "Epoch 630 -kfold 0-n_latent 6 Loss: -13235.3225\n",
            "Epoch 631 -kfold 0-n_latent 6 Loss: -12745.8791\n",
            "Epoch 632 -kfold 0-n_latent 6 Loss: -12600.8634\n",
            "Epoch 633 -kfold 0-n_latent 6 Loss: -12621.6761\n",
            "Epoch 634 -kfold 0-n_latent 6 Loss: -12983.3963\n",
            "Epoch 635 -kfold 0-n_latent 6 Loss: -13567.9961\n",
            "Epoch 636 -kfold 0-n_latent 6 Loss: -13688.4042\n",
            "Epoch 637 -kfold 0-n_latent 6 Loss: -14239.4308\n",
            "Epoch 638 -kfold 0-n_latent 6 Loss: -14470.2077\n",
            "Epoch 639 -kfold 0-n_latent 6 Loss: -14608.3678\n",
            "Epoch 640 -kfold 0-n_latent 6 Loss: -14497.9805\n",
            "Epoch 641 -kfold 0-n_latent 6 Loss: -14954.5577\n",
            "Epoch 642 -kfold 0-n_latent 6 Loss: -15489.1262\n",
            "Epoch 643 -kfold 0-n_latent 6 Loss: -15706.5353\n",
            "Epoch 644 -kfold 0-n_latent 6 Loss: -15715.5921\n",
            "Epoch 645 -kfold 0-n_latent 6 Loss: -15304.5534\n",
            "Epoch 646 -kfold 0-n_latent 6 Loss: -14768.6710\n",
            "Epoch 647 -kfold 0-n_latent 6 Loss: -14612.3165\n",
            "Epoch 648 -kfold 0-n_latent 6 Loss: -14226.8654\n",
            "Epoch 649 -kfold 0-n_latent 6 Loss: -13987.3266\n",
            "Epoch 650 -kfold 0-n_latent 6 Loss: -13610.1578\n",
            "Epoch 651 -kfold 0-n_latent 6 Loss: -13455.2902\n",
            "Epoch 652 -kfold 0-n_latent 6 Loss: -13768.1380\n",
            "Epoch 653 -kfold 0-n_latent 6 Loss: -13853.1048\n",
            "Epoch 654 -kfold 0-n_latent 6 Loss: -14291.8355\n",
            "Epoch 655 -kfold 0-n_latent 6 Loss: -14480.3066\n",
            "Epoch 656 -kfold 0-n_latent 6 Loss: -14581.1469\n",
            "Epoch 657 -kfold 0-n_latent 6 Loss: -14798.1455\n",
            "Epoch 658 -kfold 0-n_latent 6 Loss: -15022.9119\n",
            "Epoch 659 -kfold 0-n_latent 6 Loss: -15140.3724\n",
            "Epoch 660 -kfold 0-n_latent 6 Loss: -15350.5642\n",
            "Epoch 661 -kfold 0-n_latent 6 Loss: -15638.7816\n",
            "Epoch 662 -kfold 0-n_latent 6 Loss: -15805.9195\n",
            "Epoch 663 -kfold 0-n_latent 6 Loss: -15831.7204\n",
            "Epoch 664 -kfold 0-n_latent 6 Loss: -15668.5454\n",
            "Epoch 665 -kfold 0-n_latent 6 Loss: -15130.6219\n",
            "Epoch 666 -kfold 0-n_latent 6 Loss: -14716.3000\n",
            "Epoch 667 -kfold 0-n_latent 6 Loss: -14162.6106\n",
            "Epoch 668 -kfold 0-n_latent 6 Loss: -13425.4542\n",
            "Epoch 669 -kfold 0-n_latent 6 Loss: -13088.8065\n",
            "Epoch 670 -kfold 0-n_latent 6 Loss: -12163.6742\n",
            "Epoch 671 -kfold 0-n_latent 6 Loss: -11567.0261\n",
            "Epoch 672 -kfold 0-n_latent 6 Loss: -12220.1878\n",
            "Epoch 673 -kfold 0-n_latent 6 Loss: -12842.0597\n",
            "Epoch 674 -kfold 0-n_latent 6 Loss: -13467.6225\n",
            "Epoch 675 -kfold 0-n_latent 6 Loss: -13823.2288\n",
            "Epoch 676 -kfold 0-n_latent 6 Loss: -13791.9085\n",
            "Epoch 677 -kfold 0-n_latent 6 Loss: -13955.3668\n",
            "Epoch 678 -kfold 0-n_latent 6 Loss: -14219.2775\n",
            "Epoch 679 -kfold 0-n_latent 6 Loss: -14496.8446\n",
            "Epoch 680 -kfold 0-n_latent 6 Loss: -14872.9323\n",
            "Epoch 681 -kfold 0-n_latent 6 Loss: -15093.9941\n",
            "Epoch 682 -kfold 0-n_latent 6 Loss: -15312.7602\n",
            "Epoch 683 -kfold 0-n_latent 6 Loss: -15244.7479\n",
            "Epoch 684 -kfold 0-n_latent 6 Loss: -14753.6366\n",
            "Epoch 685 -kfold 0-n_latent 6 Loss: -14246.9230\n",
            "Epoch 686 -kfold 0-n_latent 6 Loss: -13472.1736\n",
            "Epoch 687 -kfold 0-n_latent 6 Loss: -12845.0011\n",
            "Epoch 688 -kfold 0-n_latent 6 Loss: -12338.6009\n",
            "Epoch 689 -kfold 0-n_latent 6 Loss: -12508.4077\n",
            "Epoch 690 -kfold 0-n_latent 6 Loss: -12971.4828\n",
            "Epoch 691 -kfold 0-n_latent 6 Loss: -13470.8187\n",
            "Epoch 692 -kfold 0-n_latent 6 Loss: -13810.7119\n",
            "Epoch 693 -kfold 0-n_latent 6 Loss: -14021.1835\n",
            "Epoch 694 -kfold 0-n_latent 6 Loss: -14284.9612\n",
            "Epoch 695 -kfold 0-n_latent 6 Loss: -14665.0359\n",
            "Epoch 696 -kfold 0-n_latent 6 Loss: -14877.2395\n",
            "Epoch 697 -kfold 0-n_latent 6 Loss: -15150.4686\n",
            "Epoch 698 -kfold 0-n_latent 6 Loss: -15455.0668\n",
            "Epoch 699 -kfold 0-n_latent 6 Loss: -15713.2621\n",
            "TRAIN: [   0    1    2 ... 1564 1565 1566] TEST: [   3   19   22   49   51   56   88   94  104  108  115  119  140  142\n",
            "  159  167  169  190  198  208  211  223  227  228  248  255  267  283\n",
            "  285  286  288  292  299  301  309  325  351  372  375  399  401  404\n",
            "  446  447  460  462  490  512  520  521  531  558  563  572  575  579\n",
            "  592  607  608  612  620  622  623  625  628  638  640  644  659  660\n",
            "  662  669  685  700  736  757  763  766  767  785  786  793  802  807\n",
            "  820  824  826  834  852  853  877  880  890  904  927  942  943  969\n",
            "  970  992 1003 1007 1011 1014 1016 1020 1026 1030 1073 1092 1098 1106\n",
            " 1113 1138 1142 1143 1144 1147 1148 1208 1209 1213 1215 1223 1225 1229\n",
            " 1231 1253 1256 1261 1263 1266 1276 1284 1285 1297 1328 1331 1344 1357\n",
            " 1361 1382 1384 1387 1398 1402 1435 1460 1465 1467 1477 1483 1497 1522\n",
            " 1533 1543 1551]\n",
            "1\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 1-n_latent 1 Loss:  19209.9755\n",
            "Epoch 2 -kfold 1-n_latent 1 Loss:  18367.2432\n",
            "Epoch 3 -kfold 1-n_latent 1 Loss:  17584.8648\n",
            "Epoch 4 -kfold 1-n_latent 1 Loss:  17000.0848\n",
            "Epoch 5 -kfold 1-n_latent 1 Loss:  16524.2677\n",
            "Epoch 6 -kfold 1-n_latent 1 Loss:  16110.5776\n",
            "Epoch 7 -kfold 1-n_latent 1 Loss:  15780.4090\n",
            "Epoch 8 -kfold 1-n_latent 1 Loss:  15474.9402\n",
            "Epoch 9 -kfold 1-n_latent 1 Loss:  15012.8643\n",
            "Epoch 10 -kfold 1-n_latent 1 Loss:  14646.7394\n",
            "Epoch 11 -kfold 1-n_latent 1 Loss:  14397.1797\n",
            "Epoch 12 -kfold 1-n_latent 1 Loss:  14277.7846\n",
            "Epoch 13 -kfold 1-n_latent 1 Loss:  14150.4694\n",
            "Epoch 14 -kfold 1-n_latent 1 Loss:  13922.8265\n",
            "Epoch 15 -kfold 1-n_latent 1 Loss:  13728.3518\n",
            "Epoch 16 -kfold 1-n_latent 1 Loss:  13591.6826\n",
            "Epoch 17 -kfold 1-n_latent 1 Loss:  13450.1968\n",
            "Epoch 18 -kfold 1-n_latent 1 Loss:  13209.8664\n",
            "Epoch 19 -kfold 1-n_latent 1 Loss:  13155.2305\n",
            "Epoch 20 -kfold 1-n_latent 1 Loss:  13019.4533\n",
            "Epoch 21 -kfold 1-n_latent 1 Loss:  12926.0959\n",
            "Epoch 22 -kfold 1-n_latent 1 Loss:  12854.5053\n",
            "Epoch 23 -kfold 1-n_latent 1 Loss:  12779.1857\n",
            "Epoch 24 -kfold 1-n_latent 1 Loss:  12711.9524\n",
            "Epoch 25 -kfold 1-n_latent 1 Loss:  12680.0018\n",
            "Epoch 26 -kfold 1-n_latent 1 Loss:  12612.2985\n",
            "Epoch 27 -kfold 1-n_latent 1 Loss:  12556.9450\n",
            "Epoch 28 -kfold 1-n_latent 1 Loss:  12462.6208\n",
            "Epoch 29 -kfold 1-n_latent 1 Loss:  12420.5185\n",
            "Epoch 30 -kfold 1-n_latent 1 Loss:  12347.3175\n",
            "Epoch 31 -kfold 1-n_latent 1 Loss:  12357.4953\n",
            "Epoch 32 -kfold 1-n_latent 1 Loss:  12277.7388\n",
            "Epoch 33 -kfold 1-n_latent 1 Loss:  12315.0907\n",
            "Epoch 34 -kfold 1-n_latent 1 Loss:  12346.8054\n",
            "Epoch 35 -kfold 1-n_latent 1 Loss:  12368.4201\n",
            "Epoch 36 -kfold 1-n_latent 1 Loss:  12297.7621\n",
            "Epoch 37 -kfold 1-n_latent 1 Loss:  12461.6033\n",
            "Epoch 38 -kfold 1-n_latent 1 Loss:  12413.6174\n",
            "Epoch 39 -kfold 1-n_latent 1 Loss:  12368.1730\n",
            "Epoch 40 -kfold 1-n_latent 1 Loss:  12336.5133\n",
            "Epoch 41 -kfold 1-n_latent 1 Loss:  12308.2767\n",
            "Epoch 42 -kfold 1-n_latent 1 Loss:  12224.3672\n",
            "Epoch 43 -kfold 1-n_latent 1 Loss:  12339.2494\n",
            "Epoch 44 -kfold 1-n_latent 1 Loss:  12202.5367\n",
            "Epoch 45 -kfold 1-n_latent 1 Loss:  12249.4606\n",
            "Epoch 46 -kfold 1-n_latent 1 Loss:  12254.5716\n",
            "Epoch 47 -kfold 1-n_latent 1 Loss:  12101.6947\n",
            "Epoch 48 -kfold 1-n_latent 1 Loss:  12127.0189\n",
            "Epoch 49 -kfold 1-n_latent 1 Loss:  12060.5209\n",
            "Epoch 50 -kfold 1-n_latent 1 Loss:  12067.6145\n",
            "Epoch 51 -kfold 1-n_latent 1 Loss:  12055.9622\n",
            "Epoch 52 -kfold 1-n_latent 1 Loss:  12061.8867\n",
            "Epoch 53 -kfold 1-n_latent 1 Loss:  12047.6899\n",
            "Epoch 54 -kfold 1-n_latent 1 Loss:  12043.3895\n",
            "Epoch 55 -kfold 1-n_latent 1 Loss:  12221.8509\n",
            "Epoch 56 -kfold 1-n_latent 1 Loss:  12142.0185\n",
            "Epoch 57 -kfold 1-n_latent 1 Loss:  12218.2013\n",
            "Epoch 58 -kfold 1-n_latent 1 Loss:  12240.4514\n",
            "Epoch 59 -kfold 1-n_latent 1 Loss:  12201.9642\n",
            "Epoch 60 -kfold 1-n_latent 1 Loss:  12194.0294\n",
            "Epoch 61 -kfold 1-n_latent 1 Loss:  12148.5607\n",
            "Epoch 62 -kfold 1-n_latent 1 Loss:  12153.5830\n",
            "Epoch 63 -kfold 1-n_latent 1 Loss:  12169.1953\n",
            "Epoch 64 -kfold 1-n_latent 1 Loss:  12072.1692\n",
            "Epoch 65 -kfold 1-n_latent 1 Loss:  12065.1834\n",
            "Epoch 66 -kfold 1-n_latent 1 Loss:  11997.2975\n",
            "Epoch 67 -kfold 1-n_latent 1 Loss:  12025.9097\n",
            "Epoch 68 -kfold 1-n_latent 1 Loss:  12079.0052\n",
            "Epoch 69 -kfold 1-n_latent 1 Loss:  12053.6139\n",
            "Epoch 70 -kfold 1-n_latent 1 Loss:  12030.2758\n",
            "Epoch 71 -kfold 1-n_latent 1 Loss:  12043.5271\n",
            "Epoch 72 -kfold 1-n_latent 1 Loss:  12028.8592\n",
            "Epoch 73 -kfold 1-n_latent 1 Loss:  12115.8775\n",
            "Epoch 74 -kfold 1-n_latent 1 Loss:  12035.3779\n",
            "Epoch 75 -kfold 1-n_latent 1 Loss:  12160.5208\n",
            "Epoch 76 -kfold 1-n_latent 1 Loss:  12092.1354\n",
            "Epoch 77 -kfold 1-n_latent 1 Loss:  12220.6064\n",
            "Epoch 78 -kfold 1-n_latent 1 Loss:  12264.4883\n",
            "Epoch 79 -kfold 1-n_latent 1 Loss:  12273.7864\n",
            "Epoch 80 -kfold 1-n_latent 1 Loss:  12329.6675\n",
            "Epoch 81 -kfold 1-n_latent 1 Loss:  12417.8712\n",
            "Epoch 82 -kfold 1-n_latent 1 Loss:  12334.9545\n",
            "Epoch 83 -kfold 1-n_latent 1 Loss:  12435.0198\n",
            "Epoch 84 -kfold 1-n_latent 1 Loss:  12367.2141\n",
            "Epoch 85 -kfold 1-n_latent 1 Loss:  12283.6230\n",
            "Epoch 86 -kfold 1-n_latent 1 Loss:  12334.5996\n",
            "Epoch 87 -kfold 1-n_latent 1 Loss:  12284.4569\n",
            "Epoch 88 -kfold 1-n_latent 1 Loss:  12264.8213\n",
            "Epoch 89 -kfold 1-n_latent 1 Loss:  12255.0360\n",
            "Epoch 90 -kfold 1-n_latent 1 Loss:  12198.9255\n",
            "Epoch 91 -kfold 1-n_latent 1 Loss:  12198.6898\n",
            "Epoch 92 -kfold 1-n_latent 1 Loss:  12159.5244\n",
            "Epoch 93 -kfold 1-n_latent 1 Loss:  12101.0856\n",
            "Epoch 94 -kfold 1-n_latent 1 Loss:  12085.7458\n",
            "Epoch 95 -kfold 1-n_latent 1 Loss:  12016.8110\n",
            "Epoch 96 -kfold 1-n_latent 1 Loss:  12060.5900\n",
            "Epoch 97 -kfold 1-n_latent 1 Loss:  12105.7482\n",
            "Epoch 98 -kfold 1-n_latent 1 Loss:  12177.6740\n",
            "Epoch 99 -kfold 1-n_latent 1 Loss:  12223.6724\n",
            "Epoch 100 -kfold 1-n_latent 1 Loss:  12276.6679\n",
            "Epoch 101 -kfold 1-n_latent 1 Loss:  12268.6692\n",
            "Epoch 102 -kfold 1-n_latent 1 Loss:  12365.5296\n",
            "Epoch 103 -kfold 1-n_latent 1 Loss:  12384.5083\n",
            "Epoch 104 -kfold 1-n_latent 1 Loss:  12337.5143\n",
            "Epoch 105 -kfold 1-n_latent 1 Loss:  12329.3546\n",
            "Epoch 106 -kfold 1-n_latent 1 Loss:  12369.0474\n",
            "Epoch 107 -kfold 1-n_latent 1 Loss:  12277.4379\n",
            "Epoch 108 -kfold 1-n_latent 1 Loss:  12288.9353\n",
            "Epoch 109 -kfold 1-n_latent 1 Loss:  12302.9715\n",
            "Epoch 110 -kfold 1-n_latent 1 Loss:  12303.1803\n",
            "Epoch 111 -kfold 1-n_latent 1 Loss:  12226.3330\n",
            "Epoch 112 -kfold 1-n_latent 1 Loss:  12200.7323\n",
            "Epoch 113 -kfold 1-n_latent 1 Loss:  12162.5578\n",
            "Epoch 114 -kfold 1-n_latent 1 Loss:  12118.5113\n",
            "Epoch 115 -kfold 1-n_latent 1 Loss:  12069.0169\n",
            "Epoch 116 -kfold 1-n_latent 1 Loss:  12147.9092\n",
            "Epoch 117 -kfold 1-n_latent 1 Loss:  12324.6786\n",
            "Epoch 118 -kfold 1-n_latent 1 Loss:  12298.0393\n",
            "Epoch 119 -kfold 1-n_latent 1 Loss:  12398.6071\n",
            "Epoch 120 -kfold 1-n_latent 1 Loss:  12571.8107\n",
            "Epoch 121 -kfold 1-n_latent 1 Loss:  12593.0959\n",
            "Epoch 122 -kfold 1-n_latent 1 Loss:  12705.6984\n",
            "Epoch 123 -kfold 1-n_latent 1 Loss:  12724.4482\n",
            "Epoch 124 -kfold 1-n_latent 1 Loss:  12801.8214\n",
            "Epoch 125 -kfold 1-n_latent 1 Loss:  12737.7023\n",
            "Epoch 126 -kfold 1-n_latent 1 Loss:  12652.4547\n",
            "Epoch 127 -kfold 1-n_latent 1 Loss:  12700.8269\n",
            "Epoch 128 -kfold 1-n_latent 1 Loss:  12610.5401\n",
            "Epoch 129 -kfold 1-n_latent 1 Loss:  12660.5579\n",
            "Epoch 130 -kfold 1-n_latent 1 Loss:  12610.0385\n",
            "Epoch 131 -kfold 1-n_latent 1 Loss:  12642.8306\n",
            "Epoch 132 -kfold 1-n_latent 1 Loss:  12623.3144\n",
            "Epoch 133 -kfold 1-n_latent 1 Loss:  12591.8617\n",
            "Epoch 134 -kfold 1-n_latent 1 Loss:  12530.4187\n",
            "Epoch 135 -kfold 1-n_latent 1 Loss:  12480.5226\n",
            "Epoch 136 -kfold 1-n_latent 1 Loss:  12496.3671\n",
            "Epoch 137 -kfold 1-n_latent 1 Loss:  12431.3895\n",
            "Epoch 138 -kfold 1-n_latent 1 Loss:  12404.4996\n",
            "Epoch 139 -kfold 1-n_latent 1 Loss:  12512.7239\n",
            "Epoch 140 -kfold 1-n_latent 1 Loss:  12497.0971\n",
            "Epoch 141 -kfold 1-n_latent 1 Loss:  12442.6776\n",
            "Epoch 142 -kfold 1-n_latent 1 Loss:  12522.8367\n",
            "Epoch 143 -kfold 1-n_latent 1 Loss:  12579.0096\n",
            "Epoch 144 -kfold 1-n_latent 1 Loss:  12680.0226\n",
            "Epoch 145 -kfold 1-n_latent 1 Loss:  12886.8511\n",
            "Epoch 146 -kfold 1-n_latent 1 Loss:  12799.3835\n",
            "Epoch 147 -kfold 1-n_latent 1 Loss:  13099.3636\n",
            "Epoch 148 -kfold 1-n_latent 1 Loss:  13209.3961\n",
            "Epoch 149 -kfold 1-n_latent 1 Loss:  13322.7907\n",
            "Epoch 150 -kfold 1-n_latent 1 Loss:  13349.8318\n",
            "Epoch 151 -kfold 1-n_latent 1 Loss:  13329.1818\n",
            "Epoch 152 -kfold 1-n_latent 1 Loss:  13141.4575\n",
            "Epoch 153 -kfold 1-n_latent 1 Loss:  13178.9733\n",
            "Epoch 154 -kfold 1-n_latent 1 Loss:  13136.7247\n",
            "Epoch 155 -kfold 1-n_latent 1 Loss:  13157.8907\n",
            "Epoch 156 -kfold 1-n_latent 1 Loss:  13091.6584\n",
            "Epoch 157 -kfold 1-n_latent 1 Loss:  13083.0376\n",
            "Epoch 158 -kfold 1-n_latent 1 Loss:  12951.2659\n",
            "Epoch 159 -kfold 1-n_latent 1 Loss:  12936.8322\n",
            "Epoch 160 -kfold 1-n_latent 1 Loss:  12873.8207\n",
            "Epoch 161 -kfold 1-n_latent 1 Loss:  12893.7219\n",
            "Epoch 162 -kfold 1-n_latent 1 Loss:  12897.4462\n",
            "Epoch 163 -kfold 1-n_latent 1 Loss:  12929.6770\n",
            "Epoch 164 -kfold 1-n_latent 1 Loss:  12880.2331\n",
            "Epoch 165 -kfold 1-n_latent 1 Loss:  12701.3220\n",
            "Epoch 166 -kfold 1-n_latent 1 Loss:  12601.6408\n",
            "Epoch 167 -kfold 1-n_latent 1 Loss:  12589.3095\n",
            "Epoch 168 -kfold 1-n_latent 1 Loss:  12498.0579\n",
            "Epoch 169 -kfold 1-n_latent 1 Loss:  12534.9902\n",
            "Epoch 170 -kfold 1-n_latent 1 Loss:  12613.2246\n",
            "Epoch 171 -kfold 1-n_latent 1 Loss:  12680.6707\n",
            "Epoch 172 -kfold 1-n_latent 1 Loss:  12761.3369\n",
            "Epoch 173 -kfold 1-n_latent 1 Loss:  12961.8996\n",
            "Epoch 174 -kfold 1-n_latent 1 Loss:  13072.9669\n",
            "Epoch 175 -kfold 1-n_latent 1 Loss:  13095.2285\n",
            "Epoch 176 -kfold 1-n_latent 1 Loss:  13372.9908\n",
            "Epoch 177 -kfold 1-n_latent 1 Loss:  13582.2578\n",
            "Epoch 178 -kfold 1-n_latent 1 Loss:  13499.4109\n",
            "Epoch 179 -kfold 1-n_latent 1 Loss:  13472.2256\n",
            "Epoch 180 -kfold 1-n_latent 1 Loss:  13396.3114\n",
            "Epoch 181 -kfold 1-n_latent 1 Loss:  13257.7040\n",
            "Epoch 182 -kfold 1-n_latent 1 Loss:  13287.8461\n",
            "Epoch 183 -kfold 1-n_latent 1 Loss:  13341.8609\n",
            "Epoch 184 -kfold 1-n_latent 1 Loss:  13321.2876\n",
            "Epoch 185 -kfold 1-n_latent 1 Loss:  13337.3078\n",
            "Epoch 186 -kfold 1-n_latent 1 Loss:  13373.2832\n",
            "Epoch 187 -kfold 1-n_latent 1 Loss:  13338.5707\n",
            "Epoch 188 -kfold 1-n_latent 1 Loss:  13311.2193\n",
            "Epoch 189 -kfold 1-n_latent 1 Loss:  13266.8568\n",
            "Epoch 190 -kfold 1-n_latent 1 Loss:  13175.4291\n",
            "Epoch 191 -kfold 1-n_latent 1 Loss:  13263.8095\n",
            "Epoch 192 -kfold 1-n_latent 1 Loss:  13231.8833\n",
            "Epoch 193 -kfold 1-n_latent 1 Loss:  13145.7888\n",
            "Epoch 194 -kfold 1-n_latent 1 Loss:  13194.1337\n",
            "Epoch 195 -kfold 1-n_latent 1 Loss:  13140.5837\n",
            "Epoch 196 -kfold 1-n_latent 1 Loss:  13038.2280\n",
            "Epoch 197 -kfold 1-n_latent 1 Loss:  13090.4417\n",
            "Epoch 198 -kfold 1-n_latent 1 Loss:  13214.5786\n",
            "Epoch 199 -kfold 1-n_latent 1 Loss:  13173.3310\n",
            "Epoch 200 -kfold 1-n_latent 1 Loss:  13058.2575\n",
            "Epoch 201 -kfold 1-n_latent 1 Loss:  13048.7603\n",
            "Epoch 202 -kfold 1-n_latent 1 Loss:  13024.3292\n",
            "Epoch 203 -kfold 1-n_latent 1 Loss:  12957.0311\n",
            "Epoch 204 -kfold 1-n_latent 1 Loss:  13056.8607\n",
            "Epoch 205 -kfold 1-n_latent 1 Loss:  13101.0547\n",
            "Epoch 206 -kfold 1-n_latent 1 Loss:  13290.5614\n",
            "Epoch 207 -kfold 1-n_latent 1 Loss:  13214.0428\n",
            "Epoch 208 -kfold 1-n_latent 1 Loss:  13398.2778\n",
            "Epoch 209 -kfold 1-n_latent 1 Loss:  13464.2764\n",
            "Epoch 210 -kfold 1-n_latent 1 Loss:  13542.9319\n",
            "Epoch 211 -kfold 1-n_latent 1 Loss:  13556.6417\n",
            "Epoch 212 -kfold 1-n_latent 1 Loss:  13703.9103\n",
            "Epoch 213 -kfold 1-n_latent 1 Loss:  13683.1829\n",
            "Epoch 214 -kfold 1-n_latent 1 Loss:  13597.1344\n",
            "Epoch 215 -kfold 1-n_latent 1 Loss:  13538.7859\n",
            "Epoch 216 -kfold 1-n_latent 1 Loss:  13455.7794\n",
            "Epoch 217 -kfold 1-n_latent 1 Loss:  13406.9325\n",
            "Epoch 218 -kfold 1-n_latent 1 Loss:  13480.4044\n",
            "Epoch 219 -kfold 1-n_latent 1 Loss:  13554.0222\n",
            "Epoch 220 -kfold 1-n_latent 1 Loss:  13330.1997\n",
            "Epoch 221 -kfold 1-n_latent 1 Loss:  13428.8960\n",
            "Epoch 222 -kfold 1-n_latent 1 Loss:  13353.4399\n",
            "Epoch 223 -kfold 1-n_latent 1 Loss:  13439.7983\n",
            "Epoch 224 -kfold 1-n_latent 1 Loss:  13274.0084\n",
            "Epoch 225 -kfold 1-n_latent 1 Loss:  13350.4013\n",
            "Epoch 226 -kfold 1-n_latent 1 Loss:  13464.6513\n",
            "Epoch 227 -kfold 1-n_latent 1 Loss:  13473.1057\n",
            "Epoch 228 -kfold 1-n_latent 1 Loss:  13565.5657\n",
            "Epoch 229 -kfold 1-n_latent 1 Loss:  13488.0189\n",
            "Epoch 230 -kfold 1-n_latent 1 Loss:  13393.2930\n",
            "Epoch 231 -kfold 1-n_latent 1 Loss:  13383.3659\n",
            "Epoch 232 -kfold 1-n_latent 1 Loss:  13281.3028\n",
            "Epoch 233 -kfold 1-n_latent 1 Loss:  13363.2807\n",
            "Epoch 234 -kfold 1-n_latent 1 Loss:  13140.8137\n",
            "Epoch 235 -kfold 1-n_latent 1 Loss:  13174.2742\n",
            "Epoch 236 -kfold 1-n_latent 1 Loss:  13303.0731\n",
            "Epoch 237 -kfold 1-n_latent 1 Loss:  13290.5121\n",
            "Epoch 238 -kfold 1-n_latent 1 Loss:  13160.7564\n",
            "Epoch 239 -kfold 1-n_latent 1 Loss:  13264.5430\n",
            "Epoch 240 -kfold 1-n_latent 1 Loss:  13107.4253\n",
            "Epoch 241 -kfold 1-n_latent 1 Loss:  13096.3161\n",
            "Epoch 242 -kfold 1-n_latent 1 Loss:  13049.2492\n",
            "Epoch 243 -kfold 1-n_latent 1 Loss:  13255.0296\n",
            "Epoch 244 -kfold 1-n_latent 1 Loss:  13145.9459\n",
            "Epoch 245 -kfold 1-n_latent 1 Loss:  13161.3697\n",
            "Epoch 246 -kfold 1-n_latent 1 Loss:  13183.3693\n",
            "Epoch 247 -kfold 1-n_latent 1 Loss:  13314.4366\n",
            "Epoch 248 -kfold 1-n_latent 1 Loss:  13566.9198\n",
            "Epoch 249 -kfold 1-n_latent 1 Loss:  13651.0401\n",
            "Epoch 250 -kfold 1-n_latent 1 Loss:  13902.2949\n",
            "Epoch 251 -kfold 1-n_latent 1 Loss:  13740.1432\n",
            "Epoch 252 -kfold 1-n_latent 1 Loss:  13820.0448\n",
            "Epoch 253 -kfold 1-n_latent 1 Loss:  13812.5637\n",
            "Epoch 254 -kfold 1-n_latent 1 Loss:  13804.0636\n",
            "Epoch 255 -kfold 1-n_latent 1 Loss:  13886.2649\n",
            "Epoch 256 -kfold 1-n_latent 1 Loss:  13703.5752\n",
            "Epoch 257 -kfold 1-n_latent 1 Loss:  13816.9123\n",
            "Epoch 258 -kfold 1-n_latent 1 Loss:  13809.0944\n",
            "Epoch 259 -kfold 1-n_latent 1 Loss:  13939.5303\n",
            "Epoch 260 -kfold 1-n_latent 1 Loss:  13762.5178\n",
            "Epoch 261 -kfold 1-n_latent 1 Loss:  13758.6659\n",
            "Epoch 262 -kfold 1-n_latent 1 Loss:  13714.9658\n",
            "Epoch 263 -kfold 1-n_latent 1 Loss:  13673.1690\n",
            "Epoch 264 -kfold 1-n_latent 1 Loss:  13622.9904\n",
            "Epoch 265 -kfold 1-n_latent 1 Loss:  13774.2596\n",
            "Epoch 266 -kfold 1-n_latent 1 Loss:  13663.9692\n",
            "Epoch 267 -kfold 1-n_latent 1 Loss:  13744.5214\n",
            "Epoch 268 -kfold 1-n_latent 1 Loss:  13629.1137\n",
            "Epoch 269 -kfold 1-n_latent 1 Loss:  13662.7188\n",
            "Epoch 270 -kfold 1-n_latent 1 Loss:  13813.5977\n",
            "Epoch 271 -kfold 1-n_latent 1 Loss:  13725.2814\n",
            "Epoch 272 -kfold 1-n_latent 1 Loss:  13741.1489\n",
            "Epoch 273 -kfold 1-n_latent 1 Loss:  13634.1410\n",
            "Epoch 274 -kfold 1-n_latent 1 Loss:  13673.9841\n",
            "Epoch 275 -kfold 1-n_latent 1 Loss:  13601.6435\n",
            "Epoch 276 -kfold 1-n_latent 1 Loss:  13608.6147\n",
            "Epoch 277 -kfold 1-n_latent 1 Loss:  13471.1844\n",
            "Epoch 278 -kfold 1-n_latent 1 Loss:  13485.4343\n",
            "Epoch 279 -kfold 1-n_latent 1 Loss:  13445.7490\n",
            "Epoch 280 -kfold 1-n_latent 1 Loss:  13397.9639\n",
            "Epoch 281 -kfold 1-n_latent 1 Loss:  13327.5872\n",
            "Epoch 282 -kfold 1-n_latent 1 Loss:  13425.2308\n",
            "Epoch 283 -kfold 1-n_latent 1 Loss:  13571.1186\n",
            "Epoch 284 -kfold 1-n_latent 1 Loss:  13566.4235\n",
            "Epoch 285 -kfold 1-n_latent 1 Loss:  13774.3899\n",
            "Epoch 286 -kfold 1-n_latent 1 Loss:  13990.2656\n",
            "Epoch 287 -kfold 1-n_latent 1 Loss:  14050.1824\n",
            "Epoch 288 -kfold 1-n_latent 1 Loss:  14154.8057\n",
            "Epoch 289 -kfold 1-n_latent 1 Loss:  14263.2930\n",
            "Epoch 290 -kfold 1-n_latent 1 Loss:  14325.1518\n",
            "Epoch 291 -kfold 1-n_latent 1 Loss:  14201.0092\n",
            "Epoch 292 -kfold 1-n_latent 1 Loss:  14193.4949\n",
            "Epoch 293 -kfold 1-n_latent 1 Loss:  14199.4052\n",
            "Epoch 294 -kfold 1-n_latent 1 Loss:  14226.4592\n",
            "Epoch 295 -kfold 1-n_latent 1 Loss:  14145.9514\n",
            "Epoch 296 -kfold 1-n_latent 1 Loss:  14176.5658\n",
            "Epoch 297 -kfold 1-n_latent 1 Loss:  14076.3389\n",
            "Epoch 298 -kfold 1-n_latent 1 Loss:  14108.6219\n",
            "Epoch 299 -kfold 1-n_latent 1 Loss:  14101.8124\n",
            "Epoch 300 -kfold 1-n_latent 1 Loss:  14207.4415\n",
            "Epoch 301 -kfold 1-n_latent 1 Loss:  14105.1988\n",
            "Epoch 302 -kfold 1-n_latent 1 Loss:  14123.5913\n",
            "Epoch 303 -kfold 1-n_latent 1 Loss:  14095.7016\n",
            "Epoch 304 -kfold 1-n_latent 1 Loss:  13865.8233\n",
            "Epoch 305 -kfold 1-n_latent 1 Loss:  13882.0361\n",
            "Epoch 306 -kfold 1-n_latent 1 Loss:  13909.9924\n",
            "Epoch 307 -kfold 1-n_latent 1 Loss:  13841.5510\n",
            "Epoch 308 -kfold 1-n_latent 1 Loss:  13994.8313\n",
            "Epoch 309 -kfold 1-n_latent 1 Loss:  13931.6159\n",
            "Epoch 310 -kfold 1-n_latent 1 Loss:  13890.5228\n",
            "Epoch 311 -kfold 1-n_latent 1 Loss:  13811.7130\n",
            "Epoch 312 -kfold 1-n_latent 1 Loss:  13998.9168\n",
            "Epoch 313 -kfold 1-n_latent 1 Loss:  14017.7624\n",
            "Epoch 314 -kfold 1-n_latent 1 Loss:  13968.5358\n",
            "Epoch 315 -kfold 1-n_latent 1 Loss:  14466.2979\n",
            "Epoch 316 -kfold 1-n_latent 1 Loss:  14026.9970\n",
            "Epoch 317 -kfold 1-n_latent 1 Loss:  13805.7451\n",
            "Epoch 318 -kfold 1-n_latent 1 Loss:  13883.0461\n",
            "Epoch 319 -kfold 1-n_latent 1 Loss:  13853.7823\n",
            "Epoch 320 -kfold 1-n_latent 1 Loss:  13865.7528\n",
            "Epoch 321 -kfold 1-n_latent 1 Loss:  14013.6606\n",
            "Epoch 322 -kfold 1-n_latent 1 Loss:  14110.3146\n",
            "Epoch 323 -kfold 1-n_latent 1 Loss:  13865.8535\n",
            "Epoch 324 -kfold 1-n_latent 1 Loss:  14149.1590\n",
            "Epoch 325 -kfold 1-n_latent 1 Loss:  14058.7289\n",
            "Epoch 326 -kfold 1-n_latent 1 Loss:  14118.2975\n",
            "Epoch 327 -kfold 1-n_latent 1 Loss:  14208.4865\n",
            "Epoch 328 -kfold 1-n_latent 1 Loss:  13945.8116\n",
            "Epoch 329 -kfold 1-n_latent 1 Loss:  14663.9857\n",
            "Epoch 330 -kfold 1-n_latent 1 Loss:  14035.0534\n",
            "Epoch 331 -kfold 1-n_latent 1 Loss:  14131.9077\n",
            "Epoch 332 -kfold 1-n_latent 1 Loss:  13868.8726\n",
            "Epoch 333 -kfold 1-n_latent 1 Loss:  13753.6319\n",
            "Epoch 334 -kfold 1-n_latent 1 Loss:  14012.0003\n",
            "Epoch 335 -kfold 1-n_latent 1 Loss:  13769.2114\n",
            "Epoch 336 -kfold 1-n_latent 1 Loss:  13876.7257\n",
            "Epoch 337 -kfold 1-n_latent 1 Loss:  14032.0536\n",
            "Epoch 338 -kfold 1-n_latent 1 Loss:  14308.8835\n",
            "Epoch 339 -kfold 1-n_latent 1 Loss:  14486.1398\n",
            "Epoch 340 -kfold 1-n_latent 1 Loss:  14718.5228\n",
            "Epoch 341 -kfold 1-n_latent 1 Loss:  14948.2602\n",
            "Epoch 342 -kfold 1-n_latent 1 Loss:  14906.7936\n",
            "Epoch 343 -kfold 1-n_latent 1 Loss:  14932.5408\n",
            "Epoch 344 -kfold 1-n_latent 1 Loss:  14973.4387\n",
            "Epoch 345 -kfold 1-n_latent 1 Loss:  14878.5730\n",
            "Epoch 346 -kfold 1-n_latent 1 Loss:  14844.8880\n",
            "Epoch 347 -kfold 1-n_latent 1 Loss:  14698.4569\n",
            "Epoch 348 -kfold 1-n_latent 1 Loss:  14690.4704\n",
            "Epoch 349 -kfold 1-n_latent 1 Loss:  14885.4869\n",
            "Epoch 350 -kfold 1-n_latent 1 Loss:  14641.7064\n",
            "Epoch 351 -kfold 1-n_latent 1 Loss:  14674.0760\n",
            "Epoch 352 -kfold 1-n_latent 1 Loss:  14583.3919\n",
            "Epoch 353 -kfold 1-n_latent 1 Loss:  14602.8262\n",
            "Epoch 354 -kfold 1-n_latent 1 Loss:  14601.4942\n",
            "Epoch 355 -kfold 1-n_latent 1 Loss:  14614.6334\n",
            "Epoch 356 -kfold 1-n_latent 1 Loss:  14505.8391\n",
            "Epoch 357 -kfold 1-n_latent 1 Loss:  14413.2532\n",
            "Epoch 358 -kfold 1-n_latent 1 Loss:  14547.3811\n",
            "Epoch 359 -kfold 1-n_latent 1 Loss:  14378.7916\n",
            "Epoch 360 -kfold 1-n_latent 1 Loss:  14442.7775\n",
            "Epoch 361 -kfold 1-n_latent 1 Loss:  15049.5824\n",
            "Epoch 362 -kfold 1-n_latent 1 Loss:  14629.9476\n",
            "Epoch 363 -kfold 1-n_latent 1 Loss:  14804.9350\n",
            "Epoch 364 -kfold 1-n_latent 1 Loss:  14793.4004\n",
            "Epoch 365 -kfold 1-n_latent 1 Loss:  14834.8629\n",
            "Epoch 366 -kfold 1-n_latent 1 Loss:  14847.6549\n",
            "Epoch 367 -kfold 1-n_latent 1 Loss:  14668.3599\n",
            "Epoch 368 -kfold 1-n_latent 1 Loss:  14540.1595\n",
            "Epoch 369 -kfold 1-n_latent 1 Loss:  14597.2042\n",
            "Epoch 370 -kfold 1-n_latent 1 Loss:  14890.0443\n",
            "Epoch 371 -kfold 1-n_latent 1 Loss:  14803.8676\n",
            "Epoch 372 -kfold 1-n_latent 1 Loss:  14901.1850\n",
            "Epoch 373 -kfold 1-n_latent 1 Loss:  14690.8787\n",
            "Epoch 374 -kfold 1-n_latent 1 Loss:  15007.6433\n",
            "Epoch 375 -kfold 1-n_latent 1 Loss:  14833.6932\n",
            "Epoch 376 -kfold 1-n_latent 1 Loss:  14830.8009\n",
            "Epoch 377 -kfold 1-n_latent 1 Loss:  14763.9507\n",
            "Epoch 378 -kfold 1-n_latent 1 Loss:  14769.1440\n",
            "Epoch 379 -kfold 1-n_latent 1 Loss:  14808.3005\n",
            "Epoch 380 -kfold 1-n_latent 1 Loss:  15178.3181\n",
            "Epoch 381 -kfold 1-n_latent 1 Loss:  14890.7499\n",
            "Epoch 382 -kfold 1-n_latent 1 Loss:  14736.5926\n",
            "Epoch 383 -kfold 1-n_latent 1 Loss:  14780.5166\n",
            "Epoch 384 -kfold 1-n_latent 1 Loss:  14843.4607\n",
            "Epoch 385 -kfold 1-n_latent 1 Loss:  14962.9962\n",
            "Epoch 386 -kfold 1-n_latent 1 Loss:  14750.6348\n",
            "Epoch 387 -kfold 1-n_latent 1 Loss:  14633.6990\n",
            "Epoch 388 -kfold 1-n_latent 1 Loss:  14635.5620\n",
            "Epoch 389 -kfold 1-n_latent 1 Loss:  14577.4753\n",
            "Epoch 390 -kfold 1-n_latent 1 Loss:  14724.3999\n",
            "Epoch 391 -kfold 1-n_latent 1 Loss:  14873.8997\n",
            "Epoch 392 -kfold 1-n_latent 1 Loss:  15020.8881\n",
            "Epoch 393 -kfold 1-n_latent 1 Loss:  14995.7391\n",
            "Epoch 394 -kfold 1-n_latent 1 Loss:  15226.6693\n",
            "Epoch 395 -kfold 1-n_latent 1 Loss:  14973.1740\n",
            "Epoch 396 -kfold 1-n_latent 1 Loss:  15079.2808\n",
            "Epoch 397 -kfold 1-n_latent 1 Loss:  15181.8490\n",
            "Epoch 398 -kfold 1-n_latent 1 Loss:  14956.3308\n",
            "Epoch 399 -kfold 1-n_latent 1 Loss:  14907.4587\n",
            "Epoch 400 -kfold 1-n_latent 1 Loss:  14839.1024\n",
            "Epoch 401 -kfold 1-n_latent 1 Loss:  14755.0593\n",
            "Epoch 402 -kfold 1-n_latent 1 Loss:  14821.4471\n",
            "Epoch 403 -kfold 1-n_latent 1 Loss:  14863.1898\n",
            "Epoch 404 -kfold 1-n_latent 1 Loss:  14877.8847\n",
            "Epoch 405 -kfold 1-n_latent 1 Loss:  14830.5531\n",
            "Epoch 406 -kfold 1-n_latent 1 Loss:  14793.2505\n",
            "Epoch 407 -kfold 1-n_latent 1 Loss:  14734.1625\n",
            "Epoch 408 -kfold 1-n_latent 1 Loss:  14644.0572\n",
            "Epoch 409 -kfold 1-n_latent 1 Loss:  14577.4603\n",
            "Epoch 410 -kfold 1-n_latent 1 Loss:  14452.0959\n",
            "Epoch 411 -kfold 1-n_latent 1 Loss:  14631.6353\n",
            "Epoch 412 -kfold 1-n_latent 1 Loss:  14766.4046\n",
            "Epoch 413 -kfold 1-n_latent 1 Loss:  14616.9146\n",
            "Epoch 414 -kfold 1-n_latent 1 Loss:  14637.8166\n",
            "Epoch 415 -kfold 1-n_latent 1 Loss:  14812.8750\n",
            "Epoch 416 -kfold 1-n_latent 1 Loss:  14665.5527\n",
            "Epoch 417 -kfold 1-n_latent 1 Loss:  14591.9224\n",
            "Epoch 418 -kfold 1-n_latent 1 Loss:  14571.7477\n",
            "Epoch 419 -kfold 1-n_latent 1 Loss:  14520.0846\n",
            "Epoch 420 -kfold 1-n_latent 1 Loss:  14453.5924\n",
            "Epoch 421 -kfold 1-n_latent 1 Loss:  14427.9075\n",
            "Epoch 422 -kfold 1-n_latent 1 Loss:  14619.4887\n",
            "Epoch 423 -kfold 1-n_latent 1 Loss:  14689.9870\n",
            "Epoch 424 -kfold 1-n_latent 1 Loss:  14721.1671\n",
            "Epoch 425 -kfold 1-n_latent 1 Loss:  14697.8166\n",
            "Epoch 426 -kfold 1-n_latent 1 Loss:  14703.7954\n",
            "Epoch 427 -kfold 1-n_latent 1 Loss:  14666.8478\n",
            "Epoch 428 -kfold 1-n_latent 1 Loss:  14525.9751\n",
            "Epoch 429 -kfold 1-n_latent 1 Loss:  14492.7080\n",
            "Epoch 430 -kfold 1-n_latent 1 Loss:  14559.2035\n",
            "Epoch 431 -kfold 1-n_latent 1 Loss:  14608.1659\n",
            "Epoch 432 -kfold 1-n_latent 1 Loss:  14804.4736\n",
            "Epoch 433 -kfold 1-n_latent 1 Loss:  14829.1388\n",
            "Epoch 434 -kfold 1-n_latent 1 Loss:  14867.8726\n",
            "Epoch 435 -kfold 1-n_latent 1 Loss:  14871.0033\n",
            "Epoch 436 -kfold 1-n_latent 1 Loss:  14913.7320\n",
            "Epoch 437 -kfold 1-n_latent 1 Loss:  14974.4815\n",
            "Epoch 438 -kfold 1-n_latent 1 Loss:  14774.5457\n",
            "Epoch 439 -kfold 1-n_latent 1 Loss:  14713.7179\n",
            "Epoch 440 -kfold 1-n_latent 1 Loss:  14676.0276\n",
            "Epoch 441 -kfold 1-n_latent 1 Loss:  14716.2742\n",
            "Epoch 442 -kfold 1-n_latent 1 Loss:  14672.8044\n",
            "Epoch 443 -kfold 1-n_latent 1 Loss:  14752.6540\n",
            "Epoch 444 -kfold 1-n_latent 1 Loss:  14540.0170\n",
            "Epoch 445 -kfold 1-n_latent 1 Loss:  14524.3591\n",
            "Epoch 446 -kfold 1-n_latent 1 Loss:  14571.1281\n",
            "Epoch 447 -kfold 1-n_latent 1 Loss:  14593.2472\n",
            "Epoch 448 -kfold 1-n_latent 1 Loss:  14426.8796\n",
            "Epoch 449 -kfold 1-n_latent 1 Loss:  14428.0530\n",
            "Epoch 450 -kfold 1-n_latent 1 Loss:  14375.7809\n",
            "Epoch 451 -kfold 1-n_latent 1 Loss:  14367.6304\n",
            "Epoch 452 -kfold 1-n_latent 1 Loss:  14379.4407\n",
            "Epoch 453 -kfold 1-n_latent 1 Loss:  14285.2903\n",
            "Epoch 454 -kfold 1-n_latent 1 Loss:  14255.4677\n",
            "Epoch 455 -kfold 1-n_latent 1 Loss:  14180.0534\n",
            "Epoch 456 -kfold 1-n_latent 1 Loss:  14215.1022\n",
            "Epoch 457 -kfold 1-n_latent 1 Loss:  14337.4736\n",
            "Epoch 458 -kfold 1-n_latent 1 Loss:  14354.9101\n",
            "Epoch 459 -kfold 1-n_latent 1 Loss:  14231.4298\n",
            "Epoch 460 -kfold 1-n_latent 1 Loss:  14186.5358\n",
            "Epoch 461 -kfold 1-n_latent 1 Loss:  14341.6884\n",
            "Epoch 462 -kfold 1-n_latent 1 Loss:  14390.0229\n",
            "Epoch 463 -kfold 1-n_latent 1 Loss:  14342.0243\n",
            "Epoch 464 -kfold 1-n_latent 1 Loss:  14322.4434\n",
            "Epoch 465 -kfold 1-n_latent 1 Loss:  14300.8808\n",
            "Epoch 466 -kfold 1-n_latent 1 Loss:  14432.1245\n",
            "Epoch 467 -kfold 1-n_latent 1 Loss:  14841.6233\n",
            "Epoch 468 -kfold 1-n_latent 1 Loss:  14548.8720\n",
            "Epoch 469 -kfold 1-n_latent 1 Loss:  14755.7423\n",
            "Epoch 470 -kfold 1-n_latent 1 Loss:  14790.3793\n",
            "Epoch 471 -kfold 1-n_latent 1 Loss:  14886.1423\n",
            "Epoch 472 -kfold 1-n_latent 1 Loss:  15018.1040\n",
            "Epoch 473 -kfold 1-n_latent 1 Loss:  14889.6353\n",
            "Epoch 474 -kfold 1-n_latent 1 Loss:  14842.6922\n",
            "Epoch 475 -kfold 1-n_latent 1 Loss:  14920.0730\n",
            "Epoch 476 -kfold 1-n_latent 1 Loss:  14878.4169\n",
            "Epoch 477 -kfold 1-n_latent 1 Loss:  14862.7931\n",
            "Epoch 478 -kfold 1-n_latent 1 Loss:  14928.3378\n",
            "Epoch 479 -kfold 1-n_latent 1 Loss:  14814.9352\n",
            "Epoch 480 -kfold 1-n_latent 1 Loss:  14735.9833\n",
            "Epoch 481 -kfold 1-n_latent 1 Loss:  14712.5908\n",
            "Epoch 482 -kfold 1-n_latent 1 Loss:  14618.1942\n",
            "Epoch 483 -kfold 1-n_latent 1 Loss:  14604.8767\n",
            "Epoch 484 -kfold 1-n_latent 1 Loss:  14588.8695\n",
            "Epoch 485 -kfold 1-n_latent 1 Loss:  14528.7667\n",
            "Epoch 486 -kfold 1-n_latent 1 Loss:  14611.1449\n",
            "Epoch 487 -kfold 1-n_latent 1 Loss:  14636.1940\n",
            "Epoch 488 -kfold 1-n_latent 1 Loss:  14535.7769\n",
            "Epoch 489 -kfold 1-n_latent 1 Loss:  14649.0817\n",
            "Epoch 490 -kfold 1-n_latent 1 Loss:  14586.9757\n",
            "Epoch 491 -kfold 1-n_latent 1 Loss:  14655.6754\n",
            "Epoch 492 -kfold 1-n_latent 1 Loss:  14673.7506\n",
            "Epoch 493 -kfold 1-n_latent 1 Loss:  14612.9784\n",
            "Epoch 494 -kfold 1-n_latent 1 Loss:  14548.2470\n",
            "Epoch 495 -kfold 1-n_latent 1 Loss:  14576.1655\n",
            "Epoch 496 -kfold 1-n_latent 1 Loss:  14639.3214\n",
            "Epoch 497 -kfold 1-n_latent 1 Loss:  14909.9819\n",
            "Epoch 498 -kfold 1-n_latent 1 Loss:  15104.3773\n",
            "Epoch 499 -kfold 1-n_latent 1 Loss:  14998.8992\n",
            "Epoch 500 -kfold 1-n_latent 1 Loss:  15038.3845\n",
            "Epoch 501 -kfold 1-n_latent 1 Loss:  15270.4891\n",
            "Epoch 502 -kfold 1-n_latent 1 Loss:  15176.1068\n",
            "Epoch 503 -kfold 1-n_latent 1 Loss:  15244.0354\n",
            "Epoch 504 -kfold 1-n_latent 1 Loss:  14846.4349\n",
            "Epoch 505 -kfold 1-n_latent 1 Loss:  14937.5463\n",
            "Epoch 506 -kfold 1-n_latent 1 Loss:  14871.1541\n",
            "Epoch 507 -kfold 1-n_latent 1 Loss:  14946.6978\n",
            "Epoch 508 -kfold 1-n_latent 1 Loss:  14928.1058\n",
            "Epoch 509 -kfold 1-n_latent 1 Loss:  14891.5768\n",
            "Epoch 510 -kfold 1-n_latent 1 Loss:  15113.9062\n",
            "Epoch 511 -kfold 1-n_latent 1 Loss:  15055.1458\n",
            "Epoch 512 -kfold 1-n_latent 1 Loss:  15099.3849\n",
            "Epoch 513 -kfold 1-n_latent 1 Loss:  14979.3839\n",
            "Epoch 514 -kfold 1-n_latent 1 Loss:  15176.4120\n",
            "Epoch 515 -kfold 1-n_latent 1 Loss:  15135.3993\n",
            "Epoch 516 -kfold 1-n_latent 1 Loss:  15102.9974\n",
            "Epoch 517 -kfold 1-n_latent 1 Loss:  15161.5862\n",
            "Epoch 518 -kfold 1-n_latent 1 Loss:  15339.3587\n",
            "Epoch 519 -kfold 1-n_latent 1 Loss:  15225.3783\n",
            "Epoch 520 -kfold 1-n_latent 1 Loss:  15364.4997\n",
            "Epoch 521 -kfold 1-n_latent 1 Loss:  15419.2491\n",
            "Epoch 522 -kfold 1-n_latent 1 Loss:  15280.6376\n",
            "Epoch 523 -kfold 1-n_latent 1 Loss:  15351.6122\n",
            "Epoch 524 -kfold 1-n_latent 1 Loss:  15256.6190\n",
            "Epoch 525 -kfold 1-n_latent 1 Loss:  15335.1660\n",
            "Epoch 526 -kfold 1-n_latent 1 Loss:  15129.0850\n",
            "Epoch 527 -kfold 1-n_latent 1 Loss:  15187.7803\n",
            "Epoch 528 -kfold 1-n_latent 1 Loss:  15470.0320\n",
            "Epoch 529 -kfold 1-n_latent 1 Loss:  15544.3203\n",
            "Epoch 530 -kfold 1-n_latent 1 Loss:  15320.8571\n",
            "Epoch 531 -kfold 1-n_latent 1 Loss:  15545.6928\n",
            "Epoch 532 -kfold 1-n_latent 1 Loss:  15659.3072\n",
            "Epoch 533 -kfold 1-n_latent 1 Loss:  15475.9408\n",
            "Epoch 534 -kfold 1-n_latent 1 Loss:  15277.0274\n",
            "Epoch 535 -kfold 1-n_latent 1 Loss:  15342.0872\n",
            "Epoch 536 -kfold 1-n_latent 1 Loss:  15319.8564\n",
            "Epoch 537 -kfold 1-n_latent 1 Loss:  15354.2334\n",
            "Epoch 538 -kfold 1-n_latent 1 Loss:  15273.0140\n",
            "Epoch 539 -kfold 1-n_latent 1 Loss:  15140.4460\n",
            "Epoch 540 -kfold 1-n_latent 1 Loss:  15208.4915\n",
            "Epoch 541 -kfold 1-n_latent 1 Loss:  15081.5744\n",
            "Epoch 542 -kfold 1-n_latent 1 Loss:  15130.5245\n",
            "Epoch 543 -kfold 1-n_latent 1 Loss:  15054.5901\n",
            "Epoch 544 -kfold 1-n_latent 1 Loss:  15043.8304\n",
            "Epoch 545 -kfold 1-n_latent 1 Loss:  15174.4378\n",
            "Epoch 546 -kfold 1-n_latent 1 Loss:  15458.0541\n",
            "Epoch 547 -kfold 1-n_latent 1 Loss:  15150.9217\n",
            "Epoch 548 -kfold 1-n_latent 1 Loss:  15103.9775\n",
            "Epoch 549 -kfold 1-n_latent 1 Loss:  15223.0995\n",
            "Epoch 550 -kfold 1-n_latent 1 Loss:  15095.3620\n",
            "Epoch 551 -kfold 1-n_latent 1 Loss:  15027.4878\n",
            "Epoch 552 -kfold 1-n_latent 1 Loss:  15132.3584\n",
            "Epoch 553 -kfold 1-n_latent 1 Loss:  15147.3046\n",
            "Epoch 554 -kfold 1-n_latent 1 Loss:  15081.3650\n",
            "Epoch 555 -kfold 1-n_latent 1 Loss:  15242.6668\n",
            "Epoch 556 -kfold 1-n_latent 1 Loss:  15338.9596\n",
            "Epoch 557 -kfold 1-n_latent 1 Loss:  15449.3497\n",
            "Epoch 558 -kfold 1-n_latent 1 Loss:  15384.0579\n",
            "Epoch 559 -kfold 1-n_latent 1 Loss:  15240.4104\n",
            "Epoch 560 -kfold 1-n_latent 1 Loss:  15469.7626\n",
            "Epoch 561 -kfold 1-n_latent 1 Loss:  15362.0746\n",
            "Epoch 562 -kfold 1-n_latent 1 Loss:  15219.0455\n",
            "Epoch 563 -kfold 1-n_latent 1 Loss:  15231.9536\n",
            "Epoch 564 -kfold 1-n_latent 1 Loss:  15043.1967\n",
            "Epoch 565 -kfold 1-n_latent 1 Loss:  15566.9277\n",
            "Epoch 566 -kfold 1-n_latent 1 Loss:  15556.1441\n",
            "Epoch 567 -kfold 1-n_latent 1 Loss:  15580.8050\n",
            "Epoch 568 -kfold 1-n_latent 1 Loss:  15526.3685\n",
            "Epoch 569 -kfold 1-n_latent 1 Loss:  15446.2572\n",
            "Epoch 570 -kfold 1-n_latent 1 Loss:  15519.0848\n",
            "Epoch 571 -kfold 1-n_latent 1 Loss:  15652.7338\n",
            "Epoch 572 -kfold 1-n_latent 1 Loss:  15370.7293\n",
            "Epoch 573 -kfold 1-n_latent 1 Loss:  15358.6227\n",
            "Epoch 574 -kfold 1-n_latent 1 Loss:  15384.1790\n",
            "Epoch 575 -kfold 1-n_latent 1 Loss:  15315.4273\n",
            "Epoch 576 -kfold 1-n_latent 1 Loss:  15324.8038\n",
            "Epoch 577 -kfold 1-n_latent 1 Loss:  15221.9642\n",
            "Epoch 578 -kfold 1-n_latent 1 Loss:  15105.1732\n",
            "Epoch 579 -kfold 1-n_latent 1 Loss:  15076.5982\n",
            "Epoch 580 -kfold 1-n_latent 1 Loss:  15015.0978\n",
            "Epoch 581 -kfold 1-n_latent 1 Loss:  15085.5122\n",
            "Epoch 582 -kfold 1-n_latent 1 Loss:  15039.8021\n",
            "Epoch 583 -kfold 1-n_latent 1 Loss:  14937.7895\n",
            "Epoch 584 -kfold 1-n_latent 1 Loss:  14919.3625\n",
            "Epoch 585 -kfold 1-n_latent 1 Loss:  14832.9914\n",
            "Epoch 586 -kfold 1-n_latent 1 Loss:  14973.9278\n",
            "Epoch 587 -kfold 1-n_latent 1 Loss:  14931.0110\n",
            "Epoch 588 -kfold 1-n_latent 1 Loss:  14745.3155\n",
            "Epoch 589 -kfold 1-n_latent 1 Loss:  14686.1143\n",
            "Epoch 590 -kfold 1-n_latent 1 Loss:  14914.3627\n",
            "Epoch 591 -kfold 1-n_latent 1 Loss:  14692.2433\n",
            "Epoch 592 -kfold 1-n_latent 1 Loss:  14979.0361\n",
            "Epoch 593 -kfold 1-n_latent 1 Loss:  14986.8344\n",
            "Epoch 594 -kfold 1-n_latent 1 Loss:  15150.9441\n",
            "Epoch 595 -kfold 1-n_latent 1 Loss:  15271.4019\n",
            "Epoch 596 -kfold 1-n_latent 1 Loss:  15954.5500\n",
            "Epoch 597 -kfold 1-n_latent 1 Loss:  16762.2528\n",
            "Epoch 598 -kfold 1-n_latent 1 Loss:  16261.9564\n",
            "Epoch 599 -kfold 1-n_latent 1 Loss:  16671.9417\n",
            "Epoch 600 -kfold 1-n_latent 1 Loss:  16915.5780\n",
            "Epoch 601 -kfold 1-n_latent 1 Loss:  16725.9337\n",
            "Epoch 602 -kfold 1-n_latent 1 Loss:  16962.1384\n",
            "Epoch 603 -kfold 1-n_latent 1 Loss:  16720.1332\n",
            "Epoch 604 -kfold 1-n_latent 1 Loss:  16691.4935\n",
            "Epoch 605 -kfold 1-n_latent 1 Loss:  16552.1572\n",
            "Epoch 606 -kfold 1-n_latent 1 Loss:  16521.3790\n",
            "Epoch 607 -kfold 1-n_latent 1 Loss:  16556.7051\n",
            "Epoch 608 -kfold 1-n_latent 1 Loss:  16859.7031\n",
            "Epoch 609 -kfold 1-n_latent 1 Loss:  16524.5939\n",
            "Epoch 610 -kfold 1-n_latent 1 Loss:  16483.2333\n",
            "Epoch 611 -kfold 1-n_latent 1 Loss:  16490.6391\n",
            "Epoch 612 -kfold 1-n_latent 1 Loss:  16381.6331\n",
            "Epoch 613 -kfold 1-n_latent 1 Loss:  16510.1397\n",
            "Epoch 614 -kfold 1-n_latent 1 Loss:  16298.8766\n",
            "Epoch 615 -kfold 1-n_latent 1 Loss:  16292.9295\n",
            "Epoch 616 -kfold 1-n_latent 1 Loss:  16471.2247\n",
            "Epoch 617 -kfold 1-n_latent 1 Loss:  16283.9684\n",
            "Epoch 618 -kfold 1-n_latent 1 Loss:  16352.0212\n",
            "Epoch 619 -kfold 1-n_latent 1 Loss:  16483.5951\n",
            "Epoch 620 -kfold 1-n_latent 1 Loss:  16721.3339\n",
            "Epoch 621 -kfold 1-n_latent 1 Loss:  16839.6726\n",
            "Epoch 622 -kfold 1-n_latent 1 Loss:  16835.4054\n",
            "Epoch 623 -kfold 1-n_latent 1 Loss:  16752.5054\n",
            "Epoch 624 -kfold 1-n_latent 1 Loss:  16953.8833\n",
            "Epoch 625 -kfold 1-n_latent 1 Loss:  16845.5397\n",
            "Epoch 626 -kfold 1-n_latent 1 Loss:  16945.5937\n",
            "Epoch 627 -kfold 1-n_latent 1 Loss:  16729.9560\n",
            "Epoch 628 -kfold 1-n_latent 1 Loss:  16466.2037\n",
            "Epoch 629 -kfold 1-n_latent 1 Loss:  16737.3413\n",
            "Epoch 630 -kfold 1-n_latent 1 Loss:  16500.4455\n",
            "Epoch 631 -kfold 1-n_latent 1 Loss:  16687.6508\n",
            "Epoch 632 -kfold 1-n_latent 1 Loss:  16794.4449\n",
            "Epoch 633 -kfold 1-n_latent 1 Loss:  16642.8561\n",
            "Epoch 634 -kfold 1-n_latent 1 Loss:  16783.2727\n",
            "Epoch 635 -kfold 1-n_latent 1 Loss:  16806.0960\n",
            "Epoch 636 -kfold 1-n_latent 1 Loss:  17081.2451\n",
            "Epoch 637 -kfold 1-n_latent 1 Loss:  16951.2994\n",
            "Epoch 638 -kfold 1-n_latent 1 Loss:  17044.6517\n",
            "Epoch 639 -kfold 1-n_latent 1 Loss:  16948.5036\n",
            "Epoch 640 -kfold 1-n_latent 1 Loss:  16992.5552\n",
            "Epoch 641 -kfold 1-n_latent 1 Loss:  16923.4134\n",
            "Epoch 642 -kfold 1-n_latent 1 Loss:  16867.5473\n",
            "Epoch 643 -kfold 1-n_latent 1 Loss:  16757.6390\n",
            "Epoch 644 -kfold 1-n_latent 1 Loss:  16749.2930\n",
            "Epoch 645 -kfold 1-n_latent 1 Loss:  16876.0957\n",
            "Epoch 646 -kfold 1-n_latent 1 Loss:  17033.2843\n",
            "Epoch 647 -kfold 1-n_latent 1 Loss:  17530.6932\n",
            "Epoch 648 -kfold 1-n_latent 1 Loss:  17921.0058\n",
            "Epoch 649 -kfold 1-n_latent 1 Loss:  17682.1276\n",
            "Epoch 650 -kfold 1-n_latent 1 Loss:  17979.4257\n",
            "Epoch 651 -kfold 1-n_latent 1 Loss:  18197.9494\n",
            "Epoch 652 -kfold 1-n_latent 1 Loss:  18100.7977\n",
            "Epoch 653 -kfold 1-n_latent 1 Loss:  17807.4924\n",
            "Epoch 654 -kfold 1-n_latent 1 Loss:  17856.9285\n",
            "Epoch 655 -kfold 1-n_latent 1 Loss:  17808.3179\n",
            "Epoch 656 -kfold 1-n_latent 1 Loss:  17704.0432\n",
            "Epoch 657 -kfold 1-n_latent 1 Loss:  17478.8500\n",
            "Epoch 658 -kfold 1-n_latent 1 Loss:  17307.5607\n",
            "Epoch 659 -kfold 1-n_latent 1 Loss:  17147.0771\n",
            "Epoch 660 -kfold 1-n_latent 1 Loss:  17183.2626\n",
            "Epoch 661 -kfold 1-n_latent 1 Loss:  17071.7514\n",
            "Epoch 662 -kfold 1-n_latent 1 Loss:  16777.4392\n",
            "Epoch 663 -kfold 1-n_latent 1 Loss:  16807.2670\n",
            "Epoch 664 -kfold 1-n_latent 1 Loss:  16825.4310\n",
            "Epoch 665 -kfold 1-n_latent 1 Loss:  16658.9242\n",
            "Epoch 666 -kfold 1-n_latent 1 Loss:  16620.4565\n",
            "Epoch 667 -kfold 1-n_latent 1 Loss:  16807.3806\n",
            "Epoch 668 -kfold 1-n_latent 1 Loss:  16511.9053\n",
            "Epoch 669 -kfold 1-n_latent 1 Loss:  16502.5227\n",
            "Epoch 670 -kfold 1-n_latent 1 Loss:  16266.8053\n",
            "Epoch 671 -kfold 1-n_latent 1 Loss:  16317.0251\n",
            "Epoch 672 -kfold 1-n_latent 1 Loss:  16333.1410\n",
            "Epoch 673 -kfold 1-n_latent 1 Loss:  16352.3312\n",
            "Epoch 674 -kfold 1-n_latent 1 Loss:  16773.3913\n",
            "Epoch 675 -kfold 1-n_latent 1 Loss:  16572.2162\n",
            "Epoch 676 -kfold 1-n_latent 1 Loss:  16660.0980\n",
            "Epoch 677 -kfold 1-n_latent 1 Loss:  16892.0757\n",
            "Epoch 678 -kfold 1-n_latent 1 Loss:  16811.6317\n",
            "Epoch 679 -kfold 1-n_latent 1 Loss:  17023.6196\n",
            "Epoch 680 -kfold 1-n_latent 1 Loss:  17142.4711\n",
            "Epoch 681 -kfold 1-n_latent 1 Loss:  17175.4552\n",
            "Epoch 682 -kfold 1-n_latent 1 Loss:  17467.4998\n",
            "Epoch 683 -kfold 1-n_latent 1 Loss:  17189.1643\n",
            "Epoch 684 -kfold 1-n_latent 1 Loss:  17226.4409\n",
            "Epoch 685 -kfold 1-n_latent 1 Loss:  17227.2575\n",
            "Epoch 686 -kfold 1-n_latent 1 Loss:  17070.2653\n",
            "Epoch 687 -kfold 1-n_latent 1 Loss:  17339.2131\n",
            "Epoch 688 -kfold 1-n_latent 1 Loss:  16852.5554\n",
            "Epoch 689 -kfold 1-n_latent 1 Loss:  17111.2358\n",
            "Epoch 690 -kfold 1-n_latent 1 Loss:  16746.6838\n",
            "Epoch 691 -kfold 1-n_latent 1 Loss:  16807.5967\n",
            "Epoch 692 -kfold 1-n_latent 1 Loss:  16990.6428\n",
            "Epoch 693 -kfold 1-n_latent 1 Loss:  17118.4541\n",
            "Epoch 694 -kfold 1-n_latent 1 Loss:  17178.3524\n",
            "Epoch 695 -kfold 1-n_latent 1 Loss:  17186.6527\n",
            "Epoch 696 -kfold 1-n_latent 1 Loss:  17342.5758\n",
            "Epoch 697 -kfold 1-n_latent 1 Loss:  17471.8002\n",
            "Epoch 698 -kfold 1-n_latent 1 Loss:  17504.4997\n",
            "Epoch 699 -kfold 1-n_latent 1 Loss:  17619.2953\n",
            "2\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 1-n_latent 2 Loss:  19885.5099\n",
            "Epoch 2 -kfold 1-n_latent 2 Loss:  18418.7356\n",
            "Epoch 3 -kfold 1-n_latent 2 Loss:  17352.3592\n",
            "Epoch 4 -kfold 1-n_latent 2 Loss:  16408.7231\n",
            "Epoch 5 -kfold 1-n_latent 2 Loss:  15567.4714\n",
            "Epoch 6 -kfold 1-n_latent 2 Loss:  14889.6975\n",
            "Epoch 7 -kfold 1-n_latent 2 Loss:  14390.6600\n",
            "Epoch 8 -kfold 1-n_latent 2 Loss:  13879.3804\n",
            "Epoch 9 -kfold 1-n_latent 2 Loss:  13490.9944\n",
            "Epoch 10 -kfold 1-n_latent 2 Loss:  13278.3254\n",
            "Epoch 11 -kfold 1-n_latent 2 Loss:  12831.1019\n",
            "Epoch 12 -kfold 1-n_latent 2 Loss:  12635.9078\n",
            "Epoch 13 -kfold 1-n_latent 2 Loss:  12432.0765\n",
            "Epoch 14 -kfold 1-n_latent 2 Loss:  12246.1565\n",
            "Epoch 15 -kfold 1-n_latent 2 Loss:  12156.1934\n",
            "Epoch 16 -kfold 1-n_latent 2 Loss:  12046.8597\n",
            "Epoch 17 -kfold 1-n_latent 2 Loss:  11954.5484\n",
            "Epoch 18 -kfold 1-n_latent 2 Loss:  11905.2741\n",
            "Epoch 19 -kfold 1-n_latent 2 Loss:  11867.2241\n",
            "Epoch 20 -kfold 1-n_latent 2 Loss:  11853.5064\n",
            "Epoch 21 -kfold 1-n_latent 2 Loss:  11800.3727\n",
            "Epoch 22 -kfold 1-n_latent 2 Loss:  11711.3450\n",
            "Epoch 23 -kfold 1-n_latent 2 Loss:  11681.4611\n",
            "Epoch 24 -kfold 1-n_latent 2 Loss:  11693.5887\n",
            "Epoch 25 -kfold 1-n_latent 2 Loss:  11582.2637\n",
            "Epoch 26 -kfold 1-n_latent 2 Loss:  11580.1050\n",
            "Epoch 27 -kfold 1-n_latent 2 Loss:  11561.8760\n",
            "Epoch 28 -kfold 1-n_latent 2 Loss:  11493.9577\n",
            "Epoch 29 -kfold 1-n_latent 2 Loss:  11510.0696\n",
            "Epoch 30 -kfold 1-n_latent 2 Loss:  11445.6679\n",
            "Epoch 31 -kfold 1-n_latent 2 Loss:  11448.3449\n",
            "Epoch 32 -kfold 1-n_latent 2 Loss:  11400.0352\n",
            "Epoch 33 -kfold 1-n_latent 2 Loss:  11297.3722\n",
            "Epoch 34 -kfold 1-n_latent 2 Loss:  11346.1634\n",
            "Epoch 35 -kfold 1-n_latent 2 Loss:  11299.2648\n",
            "Epoch 36 -kfold 1-n_latent 2 Loss:  11290.0928\n",
            "Epoch 37 -kfold 1-n_latent 2 Loss:  11247.5001\n",
            "Epoch 38 -kfold 1-n_latent 2 Loss:  11251.2853\n",
            "Epoch 39 -kfold 1-n_latent 2 Loss:  11239.1781\n",
            "Epoch 40 -kfold 1-n_latent 2 Loss:  11178.4765\n",
            "Epoch 41 -kfold 1-n_latent 2 Loss:  11110.4102\n",
            "Epoch 42 -kfold 1-n_latent 2 Loss:  11056.3102\n",
            "Epoch 43 -kfold 1-n_latent 2 Loss:  10965.2049\n",
            "Epoch 44 -kfold 1-n_latent 2 Loss:  10863.7694\n",
            "Epoch 45 -kfold 1-n_latent 2 Loss:  10808.0689\n",
            "Epoch 46 -kfold 1-n_latent 2 Loss:  10740.6114\n",
            "Epoch 47 -kfold 1-n_latent 2 Loss:  10737.3992\n",
            "Epoch 48 -kfold 1-n_latent 2 Loss:  10654.9099\n",
            "Epoch 49 -kfold 1-n_latent 2 Loss:  10647.1901\n",
            "Epoch 50 -kfold 1-n_latent 2 Loss:  10568.7512\n",
            "Epoch 51 -kfold 1-n_latent 2 Loss:  10532.9617\n",
            "Epoch 52 -kfold 1-n_latent 2 Loss:  10534.4265\n",
            "Epoch 53 -kfold 1-n_latent 2 Loss:  10500.3375\n",
            "Epoch 54 -kfold 1-n_latent 2 Loss:  10423.7465\n",
            "Epoch 55 -kfold 1-n_latent 2 Loss:  10516.9970\n",
            "Epoch 56 -kfold 1-n_latent 2 Loss:  10533.6426\n",
            "Epoch 57 -kfold 1-n_latent 2 Loss:  10540.5168\n",
            "Epoch 58 -kfold 1-n_latent 2 Loss:  10653.5884\n",
            "Epoch 59 -kfold 1-n_latent 2 Loss:  10708.8356\n",
            "Epoch 60 -kfold 1-n_latent 2 Loss:  10703.6489\n",
            "Epoch 61 -kfold 1-n_latent 2 Loss:  10815.9908\n",
            "Epoch 62 -kfold 1-n_latent 2 Loss:  10847.6046\n",
            "Epoch 63 -kfold 1-n_latent 2 Loss:  10964.9404\n",
            "Epoch 64 -kfold 1-n_latent 2 Loss:  11115.1168\n",
            "Epoch 65 -kfold 1-n_latent 2 Loss:  11282.4777\n",
            "Epoch 66 -kfold 1-n_latent 2 Loss:  11420.4314\n",
            "Epoch 67 -kfold 1-n_latent 2 Loss:  11499.3656\n",
            "Epoch 68 -kfold 1-n_latent 2 Loss:  11644.0171\n",
            "Epoch 69 -kfold 1-n_latent 2 Loss:  11664.3560\n",
            "Epoch 70 -kfold 1-n_latent 2 Loss:  11729.7164\n",
            "Epoch 71 -kfold 1-n_latent 2 Loss:  11770.0972\n",
            "Epoch 72 -kfold 1-n_latent 2 Loss:  11926.0191\n",
            "Epoch 73 -kfold 1-n_latent 2 Loss:  12137.5974\n",
            "Epoch 74 -kfold 1-n_latent 2 Loss:  12267.1497\n",
            "Epoch 75 -kfold 1-n_latent 2 Loss:  12155.3042\n",
            "Epoch 76 -kfold 1-n_latent 2 Loss:  12308.7492\n",
            "Epoch 77 -kfold 1-n_latent 2 Loss:  12449.9041\n",
            "Epoch 78 -kfold 1-n_latent 2 Loss:  12468.6374\n",
            "Epoch 79 -kfold 1-n_latent 2 Loss:  12613.7280\n",
            "Epoch 80 -kfold 1-n_latent 2 Loss:  12587.0734\n",
            "Epoch 81 -kfold 1-n_latent 2 Loss:  12716.5459\n",
            "Epoch 82 -kfold 1-n_latent 2 Loss:  12733.0114\n",
            "Epoch 83 -kfold 1-n_latent 2 Loss:  12876.6848\n",
            "Epoch 84 -kfold 1-n_latent 2 Loss:  12836.9753\n",
            "Epoch 85 -kfold 1-n_latent 2 Loss:  12848.3754\n",
            "Epoch 86 -kfold 1-n_latent 2 Loss:  12899.1047\n",
            "Epoch 87 -kfold 1-n_latent 2 Loss:  12929.7383\n",
            "Epoch 88 -kfold 1-n_latent 2 Loss:  13065.3016\n",
            "Epoch 89 -kfold 1-n_latent 2 Loss:  12921.0942\n",
            "Epoch 90 -kfold 1-n_latent 2 Loss:  12963.2478\n",
            "Epoch 91 -kfold 1-n_latent 2 Loss:  12972.8066\n",
            "Epoch 92 -kfold 1-n_latent 2 Loss:  12965.7435\n",
            "Epoch 93 -kfold 1-n_latent 2 Loss:  12859.9759\n",
            "Epoch 94 -kfold 1-n_latent 2 Loss:  12696.1983\n",
            "Epoch 95 -kfold 1-n_latent 2 Loss:  12659.3162\n",
            "Epoch 96 -kfold 1-n_latent 2 Loss:  12692.5870\n",
            "Epoch 97 -kfold 1-n_latent 2 Loss:  12551.5610\n",
            "Epoch 98 -kfold 1-n_latent 2 Loss:  12489.6159\n",
            "Epoch 99 -kfold 1-n_latent 2 Loss:  12450.0934\n",
            "Epoch 100 -kfold 1-n_latent 2 Loss:  12468.2547\n",
            "Epoch 101 -kfold 1-n_latent 2 Loss:  12393.4768\n",
            "Epoch 102 -kfold 1-n_latent 2 Loss:  12327.5460\n",
            "Epoch 103 -kfold 1-n_latent 2 Loss:  12291.5997\n",
            "Epoch 104 -kfold 1-n_latent 2 Loss:  12262.9385\n",
            "Epoch 105 -kfold 1-n_latent 2 Loss:  12210.1320\n",
            "Epoch 106 -kfold 1-n_latent 2 Loss:  12320.8077\n",
            "Epoch 107 -kfold 1-n_latent 2 Loss:  12136.7659\n",
            "Epoch 108 -kfold 1-n_latent 2 Loss:  12146.3042\n",
            "Epoch 109 -kfold 1-n_latent 2 Loss:  12106.5317\n",
            "Epoch 110 -kfold 1-n_latent 2 Loss:  12013.4962\n",
            "Epoch 111 -kfold 1-n_latent 2 Loss:  11996.3807\n",
            "Epoch 112 -kfold 1-n_latent 2 Loss:  12013.7419\n",
            "Epoch 113 -kfold 1-n_latent 2 Loss:  11940.0374\n",
            "Epoch 114 -kfold 1-n_latent 2 Loss:  11865.7775\n",
            "Epoch 115 -kfold 1-n_latent 2 Loss:  11918.0936\n",
            "Epoch 116 -kfold 1-n_latent 2 Loss:  11943.3920\n",
            "Epoch 117 -kfold 1-n_latent 2 Loss:  11901.0565\n",
            "Epoch 118 -kfold 1-n_latent 2 Loss:  11830.9600\n",
            "Epoch 119 -kfold 1-n_latent 2 Loss:  11832.9055\n",
            "Epoch 120 -kfold 1-n_latent 2 Loss:  11873.6796\n",
            "Epoch 121 -kfold 1-n_latent 2 Loss:  11941.5163\n",
            "Epoch 122 -kfold 1-n_latent 2 Loss:  12038.5690\n",
            "Epoch 123 -kfold 1-n_latent 2 Loss:  12133.0173\n",
            "Epoch 124 -kfold 1-n_latent 2 Loss:  12097.7819\n",
            "Epoch 125 -kfold 1-n_latent 2 Loss:  12104.3162\n",
            "Epoch 126 -kfold 1-n_latent 2 Loss:  12167.1616\n",
            "Epoch 127 -kfold 1-n_latent 2 Loss:  12303.2670\n",
            "Epoch 128 -kfold 1-n_latent 2 Loss:  12330.6482\n",
            "Epoch 129 -kfold 1-n_latent 2 Loss:  12404.4484\n",
            "Epoch 130 -kfold 1-n_latent 2 Loss:  12394.1473\n",
            "Epoch 131 -kfold 1-n_latent 2 Loss:  12401.2664\n",
            "Epoch 132 -kfold 1-n_latent 2 Loss:  12436.9977\n",
            "Epoch 133 -kfold 1-n_latent 2 Loss:  12517.8833\n",
            "Epoch 134 -kfold 1-n_latent 2 Loss:  12516.2113\n",
            "Epoch 135 -kfold 1-n_latent 2 Loss:  12501.3651\n",
            "Epoch 136 -kfold 1-n_latent 2 Loss:  12504.9085\n",
            "Epoch 137 -kfold 1-n_latent 2 Loss:  12640.7914\n",
            "Epoch 138 -kfold 1-n_latent 2 Loss:  12693.3102\n",
            "Epoch 139 -kfold 1-n_latent 2 Loss:  12643.2719\n",
            "Epoch 140 -kfold 1-n_latent 2 Loss:  12762.8012\n",
            "Epoch 141 -kfold 1-n_latent 2 Loss:  12773.3520\n",
            "Epoch 142 -kfold 1-n_latent 2 Loss:  12927.8662\n",
            "Epoch 143 -kfold 1-n_latent 2 Loss:  12902.9808\n",
            "Epoch 144 -kfold 1-n_latent 2 Loss:  12879.4933\n",
            "Epoch 145 -kfold 1-n_latent 2 Loss:  12968.0437\n",
            "Epoch 146 -kfold 1-n_latent 2 Loss:  12947.4810\n",
            "Epoch 147 -kfold 1-n_latent 2 Loss:  12945.2594\n",
            "Epoch 148 -kfold 1-n_latent 2 Loss:  13061.7387\n",
            "Epoch 149 -kfold 1-n_latent 2 Loss:  13141.1510\n",
            "Epoch 150 -kfold 1-n_latent 2 Loss:  13107.8829\n",
            "Epoch 151 -kfold 1-n_latent 2 Loss:  13159.5373\n",
            "Epoch 152 -kfold 1-n_latent 2 Loss:  13190.7247\n",
            "Epoch 153 -kfold 1-n_latent 2 Loss:  13271.6223\n",
            "Epoch 154 -kfold 1-n_latent 2 Loss:  13300.5653\n",
            "Epoch 155 -kfold 1-n_latent 2 Loss:  13246.9080\n",
            "Epoch 156 -kfold 1-n_latent 2 Loss:  13216.2462\n",
            "Epoch 157 -kfold 1-n_latent 2 Loss:  13186.0796\n",
            "Epoch 158 -kfold 1-n_latent 2 Loss:  13071.3297\n",
            "Epoch 159 -kfold 1-n_latent 2 Loss:  13157.9109\n",
            "Epoch 160 -kfold 1-n_latent 2 Loss:  13168.8744\n",
            "Epoch 161 -kfold 1-n_latent 2 Loss:  13234.0641\n",
            "Epoch 162 -kfold 1-n_latent 2 Loss:  13203.7557\n",
            "Epoch 163 -kfold 1-n_latent 2 Loss:  13243.9237\n",
            "Epoch 164 -kfold 1-n_latent 2 Loss:  13211.9808\n",
            "Epoch 165 -kfold 1-n_latent 2 Loss:  13184.2133\n",
            "Epoch 166 -kfold 1-n_latent 2 Loss:  13191.0594\n",
            "Epoch 167 -kfold 1-n_latent 2 Loss:  13304.7746\n",
            "Epoch 168 -kfold 1-n_latent 2 Loss:  13310.4325\n",
            "Epoch 169 -kfold 1-n_latent 2 Loss:  13315.2318\n",
            "Epoch 170 -kfold 1-n_latent 2 Loss:  13182.9848\n",
            "Epoch 171 -kfold 1-n_latent 2 Loss:  13088.1618\n",
            "Epoch 172 -kfold 1-n_latent 2 Loss:  13131.4766\n",
            "Epoch 173 -kfold 1-n_latent 2 Loss:  13158.6646\n",
            "Epoch 174 -kfold 1-n_latent 2 Loss:  13244.5602\n",
            "Epoch 175 -kfold 1-n_latent 2 Loss:  13227.9528\n",
            "Epoch 176 -kfold 1-n_latent 2 Loss:  13230.5479\n",
            "Epoch 177 -kfold 1-n_latent 2 Loss:  13236.2888\n",
            "Epoch 178 -kfold 1-n_latent 2 Loss:  13118.9092\n",
            "Epoch 179 -kfold 1-n_latent 2 Loss:  13170.9302\n",
            "Epoch 180 -kfold 1-n_latent 2 Loss:  13120.1986\n",
            "Epoch 181 -kfold 1-n_latent 2 Loss:  13121.7081\n",
            "Epoch 182 -kfold 1-n_latent 2 Loss:  13191.1768\n",
            "Epoch 183 -kfold 1-n_latent 2 Loss:  13524.5307\n",
            "Epoch 184 -kfold 1-n_latent 2 Loss:  13528.5240\n",
            "Epoch 185 -kfold 1-n_latent 2 Loss:  13461.9044\n",
            "Epoch 186 -kfold 1-n_latent 2 Loss:  13506.9580\n",
            "Epoch 187 -kfold 1-n_latent 2 Loss:  13623.3551\n",
            "Epoch 188 -kfold 1-n_latent 2 Loss:  13888.7529\n",
            "Epoch 189 -kfold 1-n_latent 2 Loss:  13871.8684\n",
            "Epoch 190 -kfold 1-n_latent 2 Loss:  13686.0845\n",
            "Epoch 191 -kfold 1-n_latent 2 Loss:  13740.2833\n",
            "Epoch 192 -kfold 1-n_latent 2 Loss:  13734.2191\n",
            "Epoch 193 -kfold 1-n_latent 2 Loss:  13759.2634\n",
            "Epoch 194 -kfold 1-n_latent 2 Loss:  13609.2785\n",
            "Epoch 195 -kfold 1-n_latent 2 Loss:  13608.1399\n",
            "Epoch 196 -kfold 1-n_latent 2 Loss:  13627.1444\n",
            "Epoch 197 -kfold 1-n_latent 2 Loss:  13576.9136\n",
            "Epoch 198 -kfold 1-n_latent 2 Loss:  13788.1860\n",
            "Epoch 199 -kfold 1-n_latent 2 Loss:  13698.0925\n",
            "Epoch 200 -kfold 1-n_latent 2 Loss:  13876.6543\n",
            "Epoch 201 -kfold 1-n_latent 2 Loss:  13726.8116\n",
            "Epoch 202 -kfold 1-n_latent 2 Loss:  13925.1738\n",
            "Epoch 203 -kfold 1-n_latent 2 Loss:  13786.0595\n",
            "Epoch 204 -kfold 1-n_latent 2 Loss:  13925.3693\n",
            "Epoch 205 -kfold 1-n_latent 2 Loss:  14027.7664\n",
            "Epoch 206 -kfold 1-n_latent 2 Loss:  13720.6438\n",
            "Epoch 207 -kfold 1-n_latent 2 Loss:  13795.8542\n",
            "Epoch 208 -kfold 1-n_latent 2 Loss:  13782.1550\n",
            "Epoch 209 -kfold 1-n_latent 2 Loss:  13915.6394\n",
            "Epoch 210 -kfold 1-n_latent 2 Loss:  13770.6102\n",
            "Epoch 211 -kfold 1-n_latent 2 Loss:  13970.9617\n",
            "Epoch 212 -kfold 1-n_latent 2 Loss:  13982.6850\n",
            "Epoch 213 -kfold 1-n_latent 2 Loss:  13915.1370\n",
            "Epoch 214 -kfold 1-n_latent 2 Loss:  13974.2146\n",
            "Epoch 215 -kfold 1-n_latent 2 Loss:  14049.5840\n",
            "Epoch 216 -kfold 1-n_latent 2 Loss:  14120.2242\n",
            "Epoch 217 -kfold 1-n_latent 2 Loss:  14033.9765\n",
            "Epoch 218 -kfold 1-n_latent 2 Loss:  13987.4761\n",
            "Epoch 219 -kfold 1-n_latent 2 Loss:  14008.3266\n",
            "Epoch 220 -kfold 1-n_latent 2 Loss:  13889.5042\n",
            "Epoch 221 -kfold 1-n_latent 2 Loss:  13994.4167\n",
            "Epoch 222 -kfold 1-n_latent 2 Loss:  14058.1909\n",
            "Epoch 223 -kfold 1-n_latent 2 Loss:  14003.4547\n",
            "Epoch 224 -kfold 1-n_latent 2 Loss:  14026.7771\n",
            "Epoch 225 -kfold 1-n_latent 2 Loss:  13973.0176\n",
            "Epoch 226 -kfold 1-n_latent 2 Loss:  13982.5553\n",
            "Epoch 227 -kfold 1-n_latent 2 Loss:  13984.5173\n",
            "Epoch 228 -kfold 1-n_latent 2 Loss:  14141.9417\n",
            "Epoch 229 -kfold 1-n_latent 2 Loss:  14316.4075\n",
            "Epoch 230 -kfold 1-n_latent 2 Loss:  14254.9234\n",
            "Epoch 231 -kfold 1-n_latent 2 Loss:  14041.4074\n",
            "Epoch 232 -kfold 1-n_latent 2 Loss:  14000.8739\n",
            "Epoch 233 -kfold 1-n_latent 2 Loss:  14010.4475\n",
            "Epoch 234 -kfold 1-n_latent 2 Loss:  13899.5516\n",
            "Epoch 235 -kfold 1-n_latent 2 Loss:  13802.2174\n",
            "Epoch 236 -kfold 1-n_latent 2 Loss:  13820.0469\n",
            "Epoch 237 -kfold 1-n_latent 2 Loss:  13752.8768\n",
            "Epoch 238 -kfold 1-n_latent 2 Loss:  13767.6841\n",
            "Epoch 239 -kfold 1-n_latent 2 Loss:  13683.4069\n",
            "Epoch 240 -kfold 1-n_latent 2 Loss:  13691.2746\n",
            "Epoch 241 -kfold 1-n_latent 2 Loss:  13746.0812\n",
            "Epoch 242 -kfold 1-n_latent 2 Loss:  13733.5137\n",
            "Epoch 243 -kfold 1-n_latent 2 Loss:  13733.6506\n",
            "Epoch 244 -kfold 1-n_latent 2 Loss:  13672.1652\n",
            "Epoch 245 -kfold 1-n_latent 2 Loss:  13734.0916\n",
            "Epoch 246 -kfold 1-n_latent 2 Loss:  13667.9968\n",
            "Epoch 247 -kfold 1-n_latent 2 Loss:  13577.1256\n",
            "Epoch 248 -kfold 1-n_latent 2 Loss:  13574.6783\n",
            "Epoch 249 -kfold 1-n_latent 2 Loss:  13559.6429\n",
            "Epoch 250 -kfold 1-n_latent 2 Loss:  13540.3327\n",
            "Epoch 251 -kfold 1-n_latent 2 Loss:  13575.1581\n",
            "Epoch 252 -kfold 1-n_latent 2 Loss:  13553.2454\n",
            "Epoch 253 -kfold 1-n_latent 2 Loss:  13653.3263\n",
            "Epoch 254 -kfold 1-n_latent 2 Loss:  13742.0656\n",
            "Epoch 255 -kfold 1-n_latent 2 Loss:  13618.5158\n",
            "Epoch 256 -kfold 1-n_latent 2 Loss:  13705.3585\n",
            "Epoch 257 -kfold 1-n_latent 2 Loss:  13738.0964\n",
            "Epoch 258 -kfold 1-n_latent 2 Loss:  14285.0677\n",
            "Epoch 259 -kfold 1-n_latent 2 Loss:  14108.3768\n",
            "Epoch 260 -kfold 1-n_latent 2 Loss:  14375.7352\n",
            "Epoch 261 -kfold 1-n_latent 2 Loss:  14453.8653\n",
            "Epoch 262 -kfold 1-n_latent 2 Loss:  14702.9250\n",
            "Epoch 263 -kfold 1-n_latent 2 Loss:  14613.4795\n",
            "Epoch 264 -kfold 1-n_latent 2 Loss:  14592.0177\n",
            "Epoch 265 -kfold 1-n_latent 2 Loss:  14769.4630\n",
            "Epoch 266 -kfold 1-n_latent 2 Loss:  14718.1659\n",
            "Epoch 267 -kfold 1-n_latent 2 Loss:  14622.4111\n",
            "Epoch 268 -kfold 1-n_latent 2 Loss:  14620.2984\n",
            "Epoch 269 -kfold 1-n_latent 2 Loss:  14746.7148\n",
            "Epoch 270 -kfold 1-n_latent 2 Loss:  14961.0019\n",
            "Epoch 271 -kfold 1-n_latent 2 Loss:  14783.3644\n",
            "Epoch 272 -kfold 1-n_latent 2 Loss:  14833.8372\n",
            "Epoch 273 -kfold 1-n_latent 2 Loss:  14737.5690\n",
            "Epoch 274 -kfold 1-n_latent 2 Loss:  14847.1686\n",
            "Epoch 275 -kfold 1-n_latent 2 Loss:  14920.0369\n",
            "Epoch 276 -kfold 1-n_latent 2 Loss:  15090.3297\n",
            "Epoch 277 -kfold 1-n_latent 2 Loss:  15156.6508\n",
            "Epoch 278 -kfold 1-n_latent 2 Loss:  15123.2277\n",
            "Epoch 279 -kfold 1-n_latent 2 Loss:  14926.0520\n",
            "Epoch 280 -kfold 1-n_latent 2 Loss:  14840.3325\n",
            "Epoch 281 -kfold 1-n_latent 2 Loss:  14847.9615\n",
            "Epoch 282 -kfold 1-n_latent 2 Loss:  14782.6327\n",
            "Epoch 283 -kfold 1-n_latent 2 Loss:  14717.3382\n",
            "Epoch 284 -kfold 1-n_latent 2 Loss:  14703.6959\n",
            "Epoch 285 -kfold 1-n_latent 2 Loss:  14769.6036\n",
            "Epoch 286 -kfold 1-n_latent 2 Loss:  14902.9874\n",
            "Epoch 287 -kfold 1-n_latent 2 Loss:  14844.0427\n",
            "Epoch 288 -kfold 1-n_latent 2 Loss:  14631.2133\n",
            "Epoch 289 -kfold 1-n_latent 2 Loss:  14580.0759\n",
            "Epoch 290 -kfold 1-n_latent 2 Loss:  14610.3446\n",
            "Epoch 291 -kfold 1-n_latent 2 Loss:  14583.7610\n",
            "Epoch 292 -kfold 1-n_latent 2 Loss:  14571.6538\n",
            "Epoch 293 -kfold 1-n_latent 2 Loss:  14606.9333\n",
            "Epoch 294 -kfold 1-n_latent 2 Loss:  14647.5650\n",
            "Epoch 295 -kfold 1-n_latent 2 Loss:  14593.5453\n",
            "Epoch 296 -kfold 1-n_latent 2 Loss:  14554.2767\n",
            "Epoch 297 -kfold 1-n_latent 2 Loss:  14494.1633\n",
            "Epoch 298 -kfold 1-n_latent 2 Loss:  14489.3329\n",
            "Epoch 299 -kfold 1-n_latent 2 Loss:  14456.7366\n",
            "Epoch 300 -kfold 1-n_latent 2 Loss:  14467.0578\n",
            "Epoch 301 -kfold 1-n_latent 2 Loss:  14456.8591\n",
            "Epoch 302 -kfold 1-n_latent 2 Loss:  14383.4063\n",
            "Epoch 303 -kfold 1-n_latent 2 Loss:  14318.8884\n",
            "Epoch 304 -kfold 1-n_latent 2 Loss:  14434.7949\n",
            "Epoch 305 -kfold 1-n_latent 2 Loss:  14385.7348\n",
            "Epoch 306 -kfold 1-n_latent 2 Loss:  14488.1579\n",
            "Epoch 307 -kfold 1-n_latent 2 Loss:  14467.6846\n",
            "Epoch 308 -kfold 1-n_latent 2 Loss:  14523.5555\n",
            "Epoch 309 -kfold 1-n_latent 2 Loss:  14485.1486\n",
            "Epoch 310 -kfold 1-n_latent 2 Loss:  14460.2145\n",
            "Epoch 311 -kfold 1-n_latent 2 Loss:  14472.2300\n",
            "Epoch 312 -kfold 1-n_latent 2 Loss:  14462.6643\n",
            "Epoch 313 -kfold 1-n_latent 2 Loss:  14473.3101\n",
            "Epoch 314 -kfold 1-n_latent 2 Loss:  14429.4795\n",
            "Epoch 315 -kfold 1-n_latent 2 Loss:  14466.1648\n",
            "Epoch 316 -kfold 1-n_latent 2 Loss:  14478.0062\n",
            "Epoch 317 -kfold 1-n_latent 2 Loss:  14486.2615\n",
            "Epoch 318 -kfold 1-n_latent 2 Loss:  14733.9846\n",
            "Epoch 319 -kfold 1-n_latent 2 Loss:  14634.6393\n",
            "Epoch 320 -kfold 1-n_latent 2 Loss:  14778.6175\n",
            "Epoch 321 -kfold 1-n_latent 2 Loss:  14622.4110\n",
            "Epoch 322 -kfold 1-n_latent 2 Loss:  14643.6937\n",
            "Epoch 323 -kfold 1-n_latent 2 Loss:  14673.0806\n",
            "Epoch 324 -kfold 1-n_latent 2 Loss:  14773.7487\n",
            "Epoch 325 -kfold 1-n_latent 2 Loss:  14689.2931\n",
            "Epoch 326 -kfold 1-n_latent 2 Loss:  14726.7615\n",
            "Epoch 327 -kfold 1-n_latent 2 Loss:  14772.0315\n",
            "Epoch 328 -kfold 1-n_latent 2 Loss:  14662.1558\n",
            "Epoch 329 -kfold 1-n_latent 2 Loss:  14828.3928\n",
            "Epoch 330 -kfold 1-n_latent 2 Loss:  14789.3623\n",
            "Epoch 331 -kfold 1-n_latent 2 Loss:  14710.0649\n",
            "Epoch 332 -kfold 1-n_latent 2 Loss:  14746.8551\n",
            "Epoch 333 -kfold 1-n_latent 2 Loss:  14803.6008\n",
            "Epoch 334 -kfold 1-n_latent 2 Loss:  14777.9929\n",
            "Epoch 335 -kfold 1-n_latent 2 Loss:  14879.1790\n",
            "Epoch 336 -kfold 1-n_latent 2 Loss:  14915.7240\n",
            "Epoch 337 -kfold 1-n_latent 2 Loss:  14918.3412\n",
            "Epoch 338 -kfold 1-n_latent 2 Loss:  14887.3596\n",
            "Epoch 339 -kfold 1-n_latent 2 Loss:  14829.6615\n",
            "Epoch 340 -kfold 1-n_latent 2 Loss:  14810.8676\n",
            "Epoch 341 -kfold 1-n_latent 2 Loss:  14741.2944\n",
            "Epoch 342 -kfold 1-n_latent 2 Loss:  14765.2311\n",
            "Epoch 343 -kfold 1-n_latent 2 Loss:  14698.0854\n",
            "Epoch 344 -kfold 1-n_latent 2 Loss:  14757.2780\n",
            "Epoch 345 -kfold 1-n_latent 2 Loss:  14722.6389\n",
            "Epoch 346 -kfold 1-n_latent 2 Loss:  14868.6615\n",
            "Epoch 347 -kfold 1-n_latent 2 Loss:  14868.9971\n",
            "Epoch 348 -kfold 1-n_latent 2 Loss:  14852.9447\n",
            "Epoch 349 -kfold 1-n_latent 2 Loss:  14872.5991\n",
            "Epoch 350 -kfold 1-n_latent 2 Loss:  14987.9131\n",
            "Epoch 351 -kfold 1-n_latent 2 Loss:  14994.3244\n",
            "Epoch 352 -kfold 1-n_latent 2 Loss:  15073.1399\n",
            "Epoch 353 -kfold 1-n_latent 2 Loss:  15230.7408\n",
            "Epoch 354 -kfold 1-n_latent 2 Loss:  15125.5529\n",
            "Epoch 355 -kfold 1-n_latent 2 Loss:  15240.8224\n",
            "Epoch 356 -kfold 1-n_latent 2 Loss:  15338.3080\n",
            "Epoch 357 -kfold 1-n_latent 2 Loss:  15060.4043\n",
            "Epoch 358 -kfold 1-n_latent 2 Loss:  15083.3957\n",
            "Epoch 359 -kfold 1-n_latent 2 Loss:  15086.8245\n",
            "Epoch 360 -kfold 1-n_latent 2 Loss:  15088.4681\n",
            "Epoch 361 -kfold 1-n_latent 2 Loss:  15198.5997\n",
            "Epoch 362 -kfold 1-n_latent 2 Loss:  15361.3626\n",
            "Epoch 363 -kfold 1-n_latent 2 Loss:  15340.0114\n",
            "Epoch 364 -kfold 1-n_latent 2 Loss:  15335.9561\n",
            "Epoch 365 -kfold 1-n_latent 2 Loss:  15210.2370\n",
            "Epoch 366 -kfold 1-n_latent 2 Loss:  15203.0691\n",
            "Epoch 367 -kfold 1-n_latent 2 Loss:  15301.3339\n",
            "Epoch 368 -kfold 1-n_latent 2 Loss:  15167.8608\n",
            "Epoch 369 -kfold 1-n_latent 2 Loss:  15176.5375\n",
            "Epoch 370 -kfold 1-n_latent 2 Loss:  15242.0576\n",
            "Epoch 371 -kfold 1-n_latent 2 Loss:  15133.4385\n",
            "Epoch 372 -kfold 1-n_latent 2 Loss:  15203.8790\n",
            "Epoch 373 -kfold 1-n_latent 2 Loss:  15125.5185\n",
            "Epoch 374 -kfold 1-n_latent 2 Loss:  15251.8620\n",
            "Epoch 375 -kfold 1-n_latent 2 Loss:  15221.2294\n",
            "Epoch 376 -kfold 1-n_latent 2 Loss:  15505.3926\n",
            "Epoch 377 -kfold 1-n_latent 2 Loss:  15523.5137\n",
            "Epoch 378 -kfold 1-n_latent 2 Loss:  15381.2576\n",
            "Epoch 379 -kfold 1-n_latent 2 Loss:  15386.0978\n",
            "Epoch 380 -kfold 1-n_latent 2 Loss:  15358.3006\n",
            "Epoch 381 -kfold 1-n_latent 2 Loss:  15375.3452\n",
            "Epoch 382 -kfold 1-n_latent 2 Loss:  15308.5023\n",
            "Epoch 383 -kfold 1-n_latent 2 Loss:  15395.5930\n",
            "Epoch 384 -kfold 1-n_latent 2 Loss:  15364.3958\n",
            "Epoch 385 -kfold 1-n_latent 2 Loss:  15336.8340\n",
            "Epoch 386 -kfold 1-n_latent 2 Loss:  15414.4510\n",
            "Epoch 387 -kfold 1-n_latent 2 Loss:  15324.7934\n",
            "Epoch 388 -kfold 1-n_latent 2 Loss:  15402.3851\n",
            "Epoch 389 -kfold 1-n_latent 2 Loss:  15359.8802\n",
            "Epoch 390 -kfold 1-n_latent 2 Loss:  15369.0170\n",
            "Epoch 391 -kfold 1-n_latent 2 Loss:  15288.4930\n",
            "Epoch 392 -kfold 1-n_latent 2 Loss:  15377.4381\n",
            "Epoch 393 -kfold 1-n_latent 2 Loss:  15441.2609\n",
            "Epoch 394 -kfold 1-n_latent 2 Loss:  15341.3846\n",
            "Epoch 395 -kfold 1-n_latent 2 Loss:  15250.5989\n",
            "Epoch 396 -kfold 1-n_latent 2 Loss:  15191.6262\n",
            "Epoch 397 -kfold 1-n_latent 2 Loss:  15153.3475\n",
            "Epoch 398 -kfold 1-n_latent 2 Loss:  15156.9621\n",
            "Epoch 399 -kfold 1-n_latent 2 Loss:  15102.6340\n",
            "Epoch 400 -kfold 1-n_latent 2 Loss:  15104.1766\n",
            "Epoch 401 -kfold 1-n_latent 2 Loss:  15079.2935\n",
            "Epoch 402 -kfold 1-n_latent 2 Loss:  14997.5962\n",
            "Epoch 403 -kfold 1-n_latent 2 Loss:  14988.3042\n",
            "Epoch 404 -kfold 1-n_latent 2 Loss:  14878.8392\n",
            "Epoch 405 -kfold 1-n_latent 2 Loss:  14838.2751\n",
            "Epoch 406 -kfold 1-n_latent 2 Loss:  14870.4948\n",
            "Epoch 407 -kfold 1-n_latent 2 Loss:  14876.1455\n",
            "Epoch 408 -kfold 1-n_latent 2 Loss:  14867.6264\n",
            "Epoch 409 -kfold 1-n_latent 2 Loss:  14812.4329\n",
            "Epoch 410 -kfold 1-n_latent 2 Loss:  14825.7329\n",
            "Epoch 411 -kfold 1-n_latent 2 Loss:  14781.7589\n",
            "Epoch 412 -kfold 1-n_latent 2 Loss:  14747.4804\n",
            "Epoch 413 -kfold 1-n_latent 2 Loss:  14873.1088\n",
            "Epoch 414 -kfold 1-n_latent 2 Loss:  14814.1336\n",
            "Epoch 415 -kfold 1-n_latent 2 Loss:  14769.2079\n",
            "Epoch 416 -kfold 1-n_latent 2 Loss:  14741.4298\n",
            "Epoch 417 -kfold 1-n_latent 2 Loss:  14697.7053\n",
            "Epoch 418 -kfold 1-n_latent 2 Loss:  14718.8120\n",
            "Epoch 419 -kfold 1-n_latent 2 Loss:  14748.8734\n",
            "Epoch 420 -kfold 1-n_latent 2 Loss:  14888.0370\n",
            "Epoch 421 -kfold 1-n_latent 2 Loss:  14835.2685\n",
            "Epoch 422 -kfold 1-n_latent 2 Loss:  14934.2537\n",
            "Epoch 423 -kfold 1-n_latent 2 Loss:  14787.9247\n",
            "Epoch 424 -kfold 1-n_latent 2 Loss:  14831.2157\n",
            "Epoch 425 -kfold 1-n_latent 2 Loss:  14802.7501\n",
            "Epoch 426 -kfold 1-n_latent 2 Loss:  14772.8206\n",
            "Epoch 427 -kfold 1-n_latent 2 Loss:  14751.6192\n",
            "Epoch 428 -kfold 1-n_latent 2 Loss:  14820.0898\n",
            "Epoch 429 -kfold 1-n_latent 2 Loss:  14785.3030\n",
            "Epoch 430 -kfold 1-n_latent 2 Loss:  14882.9116\n",
            "Epoch 431 -kfold 1-n_latent 2 Loss:  14968.6783\n",
            "Epoch 432 -kfold 1-n_latent 2 Loss:  14963.8609\n",
            "Epoch 433 -kfold 1-n_latent 2 Loss:  15062.0582\n",
            "Epoch 434 -kfold 1-n_latent 2 Loss:  15187.1525\n",
            "Epoch 435 -kfold 1-n_latent 2 Loss:  15043.6233\n",
            "Epoch 436 -kfold 1-n_latent 2 Loss:  15158.4583\n",
            "Epoch 437 -kfold 1-n_latent 2 Loss:  15094.2276\n",
            "Epoch 438 -kfold 1-n_latent 2 Loss:  15041.1172\n",
            "Epoch 439 -kfold 1-n_latent 2 Loss:  15111.3231\n",
            "Epoch 440 -kfold 1-n_latent 2 Loss:  15092.8828\n",
            "Epoch 441 -kfold 1-n_latent 2 Loss:  15277.3180\n",
            "Epoch 442 -kfold 1-n_latent 2 Loss:  15280.2126\n",
            "Epoch 443 -kfold 1-n_latent 2 Loss:  15318.0543\n",
            "Epoch 444 -kfold 1-n_latent 2 Loss:  15480.2655\n",
            "Epoch 445 -kfold 1-n_latent 2 Loss:  15508.7489\n",
            "Epoch 446 -kfold 1-n_latent 2 Loss:  15466.9544\n",
            "Epoch 447 -kfold 1-n_latent 2 Loss:  15758.9690\n",
            "Epoch 448 -kfold 1-n_latent 2 Loss:  15556.4361\n",
            "Epoch 449 -kfold 1-n_latent 2 Loss:  15748.2420\n",
            "Epoch 450 -kfold 1-n_latent 2 Loss:  15792.4423\n",
            "Epoch 451 -kfold 1-n_latent 2 Loss:  15613.7170\n",
            "Epoch 452 -kfold 1-n_latent 2 Loss:  15612.4607\n",
            "Epoch 453 -kfold 1-n_latent 2 Loss:  15700.7136\n",
            "Epoch 454 -kfold 1-n_latent 2 Loss:  15652.3801\n",
            "Epoch 455 -kfold 1-n_latent 2 Loss:  15994.6878\n",
            "Epoch 456 -kfold 1-n_latent 2 Loss:  16204.5929\n",
            "Epoch 457 -kfold 1-n_latent 2 Loss:  15804.1906\n",
            "Epoch 458 -kfold 1-n_latent 2 Loss:  15968.0588\n",
            "Epoch 459 -kfold 1-n_latent 2 Loss:  16079.4499\n",
            "Epoch 460 -kfold 1-n_latent 2 Loss:  16320.4695\n",
            "Epoch 461 -kfold 1-n_latent 2 Loss:  16323.3422\n",
            "Epoch 462 -kfold 1-n_latent 2 Loss:  16497.2934\n",
            "Epoch 463 -kfold 1-n_latent 2 Loss:  16284.5217\n",
            "Epoch 464 -kfold 1-n_latent 2 Loss:  16644.1939\n",
            "Epoch 465 -kfold 1-n_latent 2 Loss:  16529.4778\n",
            "Epoch 466 -kfold 1-n_latent 2 Loss:  16419.1025\n",
            "Epoch 467 -kfold 1-n_latent 2 Loss:  16570.2719\n",
            "Epoch 468 -kfold 1-n_latent 2 Loss:  16586.0176\n",
            "Epoch 469 -kfold 1-n_latent 2 Loss:  16418.0330\n",
            "Epoch 470 -kfold 1-n_latent 2 Loss:  16557.9523\n",
            "Epoch 471 -kfold 1-n_latent 2 Loss:  16452.9861\n",
            "Epoch 472 -kfold 1-n_latent 2 Loss:  16589.3757\n",
            "Epoch 473 -kfold 1-n_latent 2 Loss:  16488.2323\n",
            "Epoch 474 -kfold 1-n_latent 2 Loss:  16526.4496\n",
            "Epoch 475 -kfold 1-n_latent 2 Loss:  16587.4673\n",
            "Epoch 476 -kfold 1-n_latent 2 Loss:  16551.5930\n",
            "Epoch 477 -kfold 1-n_latent 2 Loss:  16558.4474\n",
            "Epoch 478 -kfold 1-n_latent 2 Loss:  16683.6360\n",
            "Epoch 479 -kfold 1-n_latent 2 Loss:  16582.3473\n",
            "Epoch 480 -kfold 1-n_latent 2 Loss:  16457.9446\n",
            "Epoch 481 -kfold 1-n_latent 2 Loss:  16672.6033\n",
            "Epoch 482 -kfold 1-n_latent 2 Loss:  16482.8713\n",
            "Epoch 483 -kfold 1-n_latent 2 Loss:  16536.2090\n",
            "Epoch 484 -kfold 1-n_latent 2 Loss:  16506.5234\n",
            "Epoch 485 -kfold 1-n_latent 2 Loss:  16535.0712\n",
            "Epoch 486 -kfold 1-n_latent 2 Loss:  16401.8133\n",
            "Epoch 487 -kfold 1-n_latent 2 Loss:  16392.8133\n",
            "Epoch 488 -kfold 1-n_latent 2 Loss:  16442.0654\n",
            "Epoch 489 -kfold 1-n_latent 2 Loss:  16363.0557\n",
            "Epoch 490 -kfold 1-n_latent 2 Loss:  16595.7119\n",
            "Epoch 491 -kfold 1-n_latent 2 Loss:  16573.5041\n",
            "Epoch 492 -kfold 1-n_latent 2 Loss:  16498.9417\n",
            "Epoch 493 -kfold 1-n_latent 2 Loss:  16752.6685\n",
            "Epoch 494 -kfold 1-n_latent 2 Loss:  16630.8721\n",
            "Epoch 495 -kfold 1-n_latent 2 Loss:  16541.6655\n",
            "Epoch 496 -kfold 1-n_latent 2 Loss:  16581.8638\n",
            "Epoch 497 -kfold 1-n_latent 2 Loss:  16440.0222\n",
            "Epoch 498 -kfold 1-n_latent 2 Loss:  16430.9003\n",
            "Epoch 499 -kfold 1-n_latent 2 Loss:  16669.5807\n",
            "Epoch 500 -kfold 1-n_latent 2 Loss:  16494.1220\n",
            "Epoch 501 -kfold 1-n_latent 2 Loss:  16381.3041\n",
            "Epoch 502 -kfold 1-n_latent 2 Loss:  16411.6737\n",
            "Epoch 503 -kfold 1-n_latent 2 Loss:  16258.3979\n",
            "Epoch 504 -kfold 1-n_latent 2 Loss:  16267.6447\n",
            "Epoch 505 -kfold 1-n_latent 2 Loss:  16275.5030\n",
            "Epoch 506 -kfold 1-n_latent 2 Loss:  16240.0186\n",
            "Epoch 507 -kfold 1-n_latent 2 Loss:  16240.6474\n",
            "Epoch 508 -kfold 1-n_latent 2 Loss:  16155.7280\n",
            "Epoch 509 -kfold 1-n_latent 2 Loss:  16102.0619\n",
            "Epoch 510 -kfold 1-n_latent 2 Loss:  16099.2201\n",
            "Epoch 511 -kfold 1-n_latent 2 Loss:  16108.1184\n",
            "Epoch 512 -kfold 1-n_latent 2 Loss:  16132.3456\n",
            "Epoch 513 -kfold 1-n_latent 2 Loss:  16055.0226\n",
            "Epoch 514 -kfold 1-n_latent 2 Loss:  15969.7469\n",
            "Epoch 515 -kfold 1-n_latent 2 Loss:  16118.5643\n",
            "Epoch 516 -kfold 1-n_latent 2 Loss:  16141.3821\n",
            "Epoch 517 -kfold 1-n_latent 2 Loss:  16259.7662\n",
            "Epoch 518 -kfold 1-n_latent 2 Loss:  16163.5205\n",
            "Epoch 519 -kfold 1-n_latent 2 Loss:  16142.5603\n",
            "Epoch 520 -kfold 1-n_latent 2 Loss:  16154.3057\n",
            "Epoch 521 -kfold 1-n_latent 2 Loss:  16125.5506\n",
            "Epoch 522 -kfold 1-n_latent 2 Loss:  16054.5586\n",
            "Epoch 523 -kfold 1-n_latent 2 Loss:  16020.4007\n",
            "Epoch 524 -kfold 1-n_latent 2 Loss:  16058.8985\n",
            "Epoch 525 -kfold 1-n_latent 2 Loss:  16154.0700\n",
            "Epoch 526 -kfold 1-n_latent 2 Loss:  16189.1061\n",
            "Epoch 527 -kfold 1-n_latent 2 Loss:  16285.2541\n",
            "Epoch 528 -kfold 1-n_latent 2 Loss:  16071.6788\n",
            "Epoch 529 -kfold 1-n_latent 2 Loss:  16261.6329\n",
            "Epoch 530 -kfold 1-n_latent 2 Loss:  16221.1698\n",
            "Epoch 531 -kfold 1-n_latent 2 Loss:  16126.9595\n",
            "Epoch 532 -kfold 1-n_latent 2 Loss:  16162.8211\n",
            "Epoch 533 -kfold 1-n_latent 2 Loss:  16285.4089\n",
            "Epoch 534 -kfold 1-n_latent 2 Loss:  16174.9043\n",
            "Epoch 535 -kfold 1-n_latent 2 Loss:  16267.5065\n",
            "Epoch 536 -kfold 1-n_latent 2 Loss:  16134.6223\n",
            "Epoch 537 -kfold 1-n_latent 2 Loss:  16230.4524\n",
            "Epoch 538 -kfold 1-n_latent 2 Loss:  16257.7333\n",
            "Epoch 539 -kfold 1-n_latent 2 Loss:  16237.6417\n",
            "Epoch 540 -kfold 1-n_latent 2 Loss:  16163.0097\n",
            "Epoch 541 -kfold 1-n_latent 2 Loss:  16195.2391\n",
            "Epoch 542 -kfold 1-n_latent 2 Loss:  16595.9896\n",
            "Epoch 543 -kfold 1-n_latent 2 Loss:  16614.5149\n",
            "Epoch 544 -kfold 1-n_latent 2 Loss:  16364.6540\n",
            "Epoch 545 -kfold 1-n_latent 2 Loss:  16546.2665\n",
            "Epoch 546 -kfold 1-n_latent 2 Loss:  16383.0849\n",
            "Epoch 547 -kfold 1-n_latent 2 Loss:  16510.6519\n",
            "Epoch 548 -kfold 1-n_latent 2 Loss:  16599.0020\n",
            "Epoch 549 -kfold 1-n_latent 2 Loss:  16387.1445\n",
            "Epoch 550 -kfold 1-n_latent 2 Loss:  16389.5125\n",
            "Epoch 551 -kfold 1-n_latent 2 Loss:  16431.0973\n",
            "Epoch 552 -kfold 1-n_latent 2 Loss:  16297.4878\n",
            "Epoch 553 -kfold 1-n_latent 2 Loss:  16342.5874\n",
            "Epoch 554 -kfold 1-n_latent 2 Loss:  16344.1986\n",
            "Epoch 555 -kfold 1-n_latent 2 Loss:  16559.9720\n",
            "Epoch 556 -kfold 1-n_latent 2 Loss:  16734.2602\n",
            "Epoch 557 -kfold 1-n_latent 2 Loss:  16734.0399\n",
            "Epoch 558 -kfold 1-n_latent 2 Loss:  16666.1707\n",
            "Epoch 559 -kfold 1-n_latent 2 Loss:  16528.9510\n",
            "Epoch 560 -kfold 1-n_latent 2 Loss:  16446.0774\n",
            "Epoch 561 -kfold 1-n_latent 2 Loss:  16559.6050\n",
            "Epoch 562 -kfold 1-n_latent 2 Loss:  16448.3373\n",
            "Epoch 563 -kfold 1-n_latent 2 Loss:  16460.0997\n",
            "Epoch 564 -kfold 1-n_latent 2 Loss:  16337.0005\n",
            "Epoch 565 -kfold 1-n_latent 2 Loss:  16368.2799\n",
            "Epoch 566 -kfold 1-n_latent 2 Loss:  16399.4426\n",
            "Epoch 567 -kfold 1-n_latent 2 Loss:  16452.5827\n",
            "Epoch 568 -kfold 1-n_latent 2 Loss:  16352.3254\n",
            "Epoch 569 -kfold 1-n_latent 2 Loss:  16317.4610\n",
            "Epoch 570 -kfold 1-n_latent 2 Loss:  16405.6867\n",
            "Epoch 571 -kfold 1-n_latent 2 Loss:  16404.8774\n",
            "Epoch 572 -kfold 1-n_latent 2 Loss:  16297.5130\n",
            "Epoch 573 -kfold 1-n_latent 2 Loss:  16267.8741\n",
            "Epoch 574 -kfold 1-n_latent 2 Loss:  16282.7612\n",
            "Epoch 575 -kfold 1-n_latent 2 Loss:  16262.9122\n",
            "Epoch 576 -kfold 1-n_latent 2 Loss:  16242.9652\n",
            "Epoch 577 -kfold 1-n_latent 2 Loss:  16189.0434\n",
            "Epoch 578 -kfold 1-n_latent 2 Loss:  16201.9769\n",
            "Epoch 579 -kfold 1-n_latent 2 Loss:  16245.2660\n",
            "Epoch 580 -kfold 1-n_latent 2 Loss:  16208.5449\n",
            "Epoch 581 -kfold 1-n_latent 2 Loss:  16272.6233\n",
            "Epoch 582 -kfold 1-n_latent 2 Loss:  16236.7655\n",
            "Epoch 583 -kfold 1-n_latent 2 Loss:  16411.9821\n",
            "Epoch 584 -kfold 1-n_latent 2 Loss:  17016.1939\n",
            "Epoch 585 -kfold 1-n_latent 2 Loss:  16374.6901\n",
            "Epoch 586 -kfold 1-n_latent 2 Loss:  16478.3161\n",
            "Epoch 587 -kfold 1-n_latent 2 Loss:  16428.0190\n",
            "Epoch 588 -kfold 1-n_latent 2 Loss:  16544.2358\n",
            "Epoch 589 -kfold 1-n_latent 2 Loss:  16616.9957\n",
            "Epoch 590 -kfold 1-n_latent 2 Loss:  16572.4656\n",
            "Epoch 591 -kfold 1-n_latent 2 Loss:  16599.9716\n",
            "Epoch 592 -kfold 1-n_latent 2 Loss:  16496.6697\n",
            "Epoch 593 -kfold 1-n_latent 2 Loss:  16368.3334\n",
            "Epoch 594 -kfold 1-n_latent 2 Loss:  16412.4393\n",
            "Epoch 595 -kfold 1-n_latent 2 Loss:  16420.2268\n",
            "Epoch 596 -kfold 1-n_latent 2 Loss:  16353.9530\n",
            "Epoch 597 -kfold 1-n_latent 2 Loss:  16324.6552\n",
            "Epoch 598 -kfold 1-n_latent 2 Loss:  16317.6408\n",
            "Epoch 599 -kfold 1-n_latent 2 Loss:  16211.5536\n",
            "Epoch 600 -kfold 1-n_latent 2 Loss:  16206.5198\n",
            "Epoch 601 -kfold 1-n_latent 2 Loss:  16090.6666\n",
            "Epoch 602 -kfold 1-n_latent 2 Loss:  16148.9890\n",
            "Epoch 603 -kfold 1-n_latent 2 Loss:  16138.9692\n",
            "Epoch 604 -kfold 1-n_latent 2 Loss:  16165.7419\n",
            "Epoch 605 -kfold 1-n_latent 2 Loss:  16423.8267\n",
            "Epoch 606 -kfold 1-n_latent 2 Loss:  16473.5780\n",
            "Epoch 607 -kfold 1-n_latent 2 Loss:  16503.5330\n",
            "Epoch 608 -kfold 1-n_latent 2 Loss:  16644.3497\n",
            "Epoch 609 -kfold 1-n_latent 2 Loss:  16753.3965\n",
            "Epoch 610 -kfold 1-n_latent 2 Loss:  16607.4079\n",
            "Epoch 611 -kfold 1-n_latent 2 Loss:  16621.0886\n",
            "Epoch 612 -kfold 1-n_latent 2 Loss:  16720.7086\n",
            "Epoch 613 -kfold 1-n_latent 2 Loss:  16670.3073\n",
            "Epoch 614 -kfold 1-n_latent 2 Loss:  16761.3068\n",
            "Epoch 615 -kfold 1-n_latent 2 Loss:  16707.6958\n",
            "Epoch 616 -kfold 1-n_latent 2 Loss:  16812.5346\n",
            "Epoch 617 -kfold 1-n_latent 2 Loss:  16574.3976\n",
            "Epoch 618 -kfold 1-n_latent 2 Loss:  16646.8615\n",
            "Epoch 619 -kfold 1-n_latent 2 Loss:  16576.4734\n",
            "Epoch 620 -kfold 1-n_latent 2 Loss:  16726.4043\n",
            "Epoch 621 -kfold 1-n_latent 2 Loss:  16808.7717\n",
            "Epoch 622 -kfold 1-n_latent 2 Loss:  16713.8605\n",
            "Epoch 623 -kfold 1-n_latent 2 Loss:  16703.8682\n",
            "Epoch 624 -kfold 1-n_latent 2 Loss:  16723.9351\n",
            "Epoch 625 -kfold 1-n_latent 2 Loss:  16759.6619\n",
            "Epoch 626 -kfold 1-n_latent 2 Loss:  16659.6230\n",
            "Epoch 627 -kfold 1-n_latent 2 Loss:  16721.0412\n",
            "Epoch 628 -kfold 1-n_latent 2 Loss:  16648.7047\n",
            "Epoch 629 -kfold 1-n_latent 2 Loss:  16601.8143\n",
            "Epoch 630 -kfold 1-n_latent 2 Loss:  16730.6625\n",
            "Epoch 631 -kfold 1-n_latent 2 Loss:  16762.5995\n",
            "Epoch 632 -kfold 1-n_latent 2 Loss:  16848.5604\n",
            "Epoch 633 -kfold 1-n_latent 2 Loss:  16698.2573\n",
            "Epoch 634 -kfold 1-n_latent 2 Loss:  16707.8931\n",
            "Epoch 635 -kfold 1-n_latent 2 Loss:  16617.2411\n",
            "Epoch 636 -kfold 1-n_latent 2 Loss:  16748.4773\n",
            "Epoch 637 -kfold 1-n_latent 2 Loss:  16742.8404\n",
            "Epoch 638 -kfold 1-n_latent 2 Loss:  16649.5347\n",
            "Epoch 639 -kfold 1-n_latent 2 Loss:  16605.2374\n",
            "Epoch 640 -kfold 1-n_latent 2 Loss:  16730.5166\n",
            "Epoch 641 -kfold 1-n_latent 2 Loss:  16694.1065\n",
            "Epoch 642 -kfold 1-n_latent 2 Loss:  16546.3676\n",
            "Epoch 643 -kfold 1-n_latent 2 Loss:  16604.0535\n",
            "Epoch 644 -kfold 1-n_latent 2 Loss:  16598.2001\n",
            "Epoch 645 -kfold 1-n_latent 2 Loss:  16477.7166\n",
            "Epoch 646 -kfold 1-n_latent 2 Loss:  16514.1970\n",
            "Epoch 647 -kfold 1-n_latent 2 Loss:  16599.1051\n",
            "Epoch 648 -kfold 1-n_latent 2 Loss:  16580.9881\n",
            "Epoch 649 -kfold 1-n_latent 2 Loss:  16422.9102\n",
            "Epoch 650 -kfold 1-n_latent 2 Loss:  16412.7327\n",
            "Epoch 651 -kfold 1-n_latent 2 Loss:  16450.9718\n",
            "Epoch 652 -kfold 1-n_latent 2 Loss:  16388.0776\n",
            "Epoch 653 -kfold 1-n_latent 2 Loss:  16498.8233\n",
            "Epoch 654 -kfold 1-n_latent 2 Loss:  16352.0605\n",
            "Epoch 655 -kfold 1-n_latent 2 Loss:  16354.8652\n",
            "Epoch 656 -kfold 1-n_latent 2 Loss:  16371.0277\n",
            "Epoch 657 -kfold 1-n_latent 2 Loss:  16478.2737\n",
            "Epoch 658 -kfold 1-n_latent 2 Loss:  16495.9178\n",
            "Epoch 659 -kfold 1-n_latent 2 Loss:  16420.0018\n",
            "Epoch 660 -kfold 1-n_latent 2 Loss:  16434.2979\n",
            "Epoch 661 -kfold 1-n_latent 2 Loss:  16430.9114\n",
            "Epoch 662 -kfold 1-n_latent 2 Loss:  16389.6759\n",
            "Epoch 663 -kfold 1-n_latent 2 Loss:  16420.2997\n",
            "Epoch 664 -kfold 1-n_latent 2 Loss:  16447.7518\n",
            "Epoch 665 -kfold 1-n_latent 2 Loss:  16405.2781\n",
            "Epoch 666 -kfold 1-n_latent 2 Loss:  16326.8240\n",
            "Epoch 667 -kfold 1-n_latent 2 Loss:  16362.1901\n",
            "Epoch 668 -kfold 1-n_latent 2 Loss:  16276.2989\n",
            "Epoch 669 -kfold 1-n_latent 2 Loss:  16309.0621\n",
            "Epoch 670 -kfold 1-n_latent 2 Loss:  16353.0051\n",
            "Epoch 671 -kfold 1-n_latent 2 Loss:  16370.7277\n",
            "Epoch 672 -kfold 1-n_latent 2 Loss:  16507.8412\n",
            "Epoch 673 -kfold 1-n_latent 2 Loss:  16517.4504\n",
            "Epoch 674 -kfold 1-n_latent 2 Loss:  16430.1318\n",
            "Epoch 675 -kfold 1-n_latent 2 Loss:  16401.1323\n",
            "Epoch 676 -kfold 1-n_latent 2 Loss:  16373.5595\n",
            "Epoch 677 -kfold 1-n_latent 2 Loss:  16452.1575\n",
            "Epoch 678 -kfold 1-n_latent 2 Loss:  17088.0067\n",
            "Epoch 679 -kfold 1-n_latent 2 Loss:  16521.8857\n",
            "Epoch 680 -kfold 1-n_latent 2 Loss:  16789.6125\n",
            "Epoch 681 -kfold 1-n_latent 2 Loss:  16452.3693\n",
            "Epoch 682 -kfold 1-n_latent 2 Loss:  16502.5379\n",
            "Epoch 683 -kfold 1-n_latent 2 Loss:  16666.7205\n",
            "Epoch 684 -kfold 1-n_latent 2 Loss:  16424.8743\n",
            "Epoch 685 -kfold 1-n_latent 2 Loss:  16391.4967\n",
            "Epoch 686 -kfold 1-n_latent 2 Loss:  16489.7611\n",
            "Epoch 687 -kfold 1-n_latent 2 Loss:  16320.1847\n",
            "Epoch 688 -kfold 1-n_latent 2 Loss:  16362.0591\n",
            "Epoch 689 -kfold 1-n_latent 2 Loss:  16473.9124\n",
            "Epoch 690 -kfold 1-n_latent 2 Loss:  16512.9971\n",
            "Epoch 691 -kfold 1-n_latent 2 Loss:  16510.6201\n",
            "Epoch 692 -kfold 1-n_latent 2 Loss:  16475.6220\n",
            "Epoch 693 -kfold 1-n_latent 2 Loss:  16423.2651\n",
            "Epoch 694 -kfold 1-n_latent 2 Loss:  16542.0840\n",
            "Epoch 695 -kfold 1-n_latent 2 Loss:  16448.3691\n",
            "Epoch 696 -kfold 1-n_latent 2 Loss:  16394.8788\n",
            "Epoch 697 -kfold 1-n_latent 2 Loss:  16338.4895\n",
            "Epoch 698 -kfold 1-n_latent 2 Loss:  16351.7675\n",
            "Epoch 699 -kfold 1-n_latent 2 Loss:  16391.8796\n",
            "3\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 1-n_latent 3 Loss:  21290.3407\n",
            "Epoch 2 -kfold 1-n_latent 3 Loss:  19760.7910\n",
            "Epoch 3 -kfold 1-n_latent 3 Loss:  18530.8976\n",
            "Epoch 4 -kfold 1-n_latent 3 Loss:  17462.8552\n",
            "Epoch 5 -kfold 1-n_latent 3 Loss:  16427.0985\n",
            "Epoch 6 -kfold 1-n_latent 3 Loss:  15308.7124\n",
            "Epoch 7 -kfold 1-n_latent 3 Loss:  14216.4871\n",
            "Epoch 8 -kfold 1-n_latent 3 Loss:  13262.8264\n",
            "Epoch 9 -kfold 1-n_latent 3 Loss:  12472.3526\n",
            "Epoch 10 -kfold 1-n_latent 3 Loss:  11706.6460\n",
            "Epoch 11 -kfold 1-n_latent 3 Loss:  11113.6611\n",
            "Epoch 12 -kfold 1-n_latent 3 Loss:  10543.9499\n",
            "Epoch 13 -kfold 1-n_latent 3 Loss:  10137.4283\n",
            "Epoch 14 -kfold 1-n_latent 3 Loss:  9792.4895\n",
            "Epoch 15 -kfold 1-n_latent 3 Loss:  9518.7694\n",
            "Epoch 16 -kfold 1-n_latent 3 Loss:  9341.1677\n",
            "Epoch 17 -kfold 1-n_latent 3 Loss:  8997.5359\n",
            "Epoch 18 -kfold 1-n_latent 3 Loss:  8816.2050\n",
            "Epoch 19 -kfold 1-n_latent 3 Loss:  8602.4094\n",
            "Epoch 20 -kfold 1-n_latent 3 Loss:  8408.7443\n",
            "Epoch 21 -kfold 1-n_latent 3 Loss:  8164.4224\n",
            "Epoch 22 -kfold 1-n_latent 3 Loss:  7974.7659\n",
            "Epoch 23 -kfold 1-n_latent 3 Loss:  7822.8722\n",
            "Epoch 24 -kfold 1-n_latent 3 Loss:  7672.6617\n",
            "Epoch 25 -kfold 1-n_latent 3 Loss:  7546.4089\n",
            "Epoch 26 -kfold 1-n_latent 3 Loss:  7391.6238\n",
            "Epoch 27 -kfold 1-n_latent 3 Loss:  7255.4773\n",
            "Epoch 28 -kfold 1-n_latent 3 Loss:  7155.8222\n",
            "Epoch 29 -kfold 1-n_latent 3 Loss:  7045.6814\n",
            "Epoch 30 -kfold 1-n_latent 3 Loss:  6893.2902\n",
            "Epoch 31 -kfold 1-n_latent 3 Loss:  6799.8524\n",
            "Epoch 32 -kfold 1-n_latent 3 Loss:  6683.4617\n",
            "Epoch 33 -kfold 1-n_latent 3 Loss:  6622.6418\n",
            "Epoch 34 -kfold 1-n_latent 3 Loss:  6534.6992\n",
            "Epoch 35 -kfold 1-n_latent 3 Loss:  6498.9252\n",
            "Epoch 36 -kfold 1-n_latent 3 Loss:  6417.0058\n",
            "Epoch 37 -kfold 1-n_latent 3 Loss:  6358.3412\n",
            "Epoch 38 -kfold 1-n_latent 3 Loss:  6376.3675\n",
            "Epoch 39 -kfold 1-n_latent 3 Loss:  6314.4990\n",
            "Epoch 40 -kfold 1-n_latent 3 Loss:  6219.5119\n",
            "Epoch 41 -kfold 1-n_latent 3 Loss:  6157.4230\n",
            "Epoch 42 -kfold 1-n_latent 3 Loss:  6104.4833\n",
            "Epoch 43 -kfold 1-n_latent 3 Loss:  5992.7842\n",
            "Epoch 44 -kfold 1-n_latent 3 Loss:  5955.9272\n",
            "Epoch 45 -kfold 1-n_latent 3 Loss:  5842.6936\n",
            "Epoch 46 -kfold 1-n_latent 3 Loss:  5767.7470\n",
            "Epoch 47 -kfold 1-n_latent 3 Loss:  5723.8115\n",
            "Epoch 48 -kfold 1-n_latent 3 Loss:  5665.4075\n",
            "Epoch 49 -kfold 1-n_latent 3 Loss:  5616.8762\n",
            "Epoch 50 -kfold 1-n_latent 3 Loss:  5569.0662\n",
            "Epoch 51 -kfold 1-n_latent 3 Loss:  5541.9019\n",
            "Epoch 52 -kfold 1-n_latent 3 Loss:  5454.1285\n",
            "Epoch 53 -kfold 1-n_latent 3 Loss:  5368.0452\n",
            "Epoch 54 -kfold 1-n_latent 3 Loss:  5346.7220\n",
            "Epoch 55 -kfold 1-n_latent 3 Loss:  5349.1125\n",
            "Epoch 56 -kfold 1-n_latent 3 Loss:  5369.7287\n",
            "Epoch 57 -kfold 1-n_latent 3 Loss:  5298.3648\n",
            "Epoch 58 -kfold 1-n_latent 3 Loss:  5293.0685\n",
            "Epoch 59 -kfold 1-n_latent 3 Loss:  5208.3662\n",
            "Epoch 60 -kfold 1-n_latent 3 Loss:  5222.8657\n",
            "Epoch 61 -kfold 1-n_latent 3 Loss:  5193.7831\n",
            "Epoch 62 -kfold 1-n_latent 3 Loss:  5114.0544\n",
            "Epoch 63 -kfold 1-n_latent 3 Loss:  5142.7120\n",
            "Epoch 64 -kfold 1-n_latent 3 Loss:  5087.9698\n",
            "Epoch 65 -kfold 1-n_latent 3 Loss:  5055.5844\n",
            "Epoch 66 -kfold 1-n_latent 3 Loss:  5009.5960\n",
            "Epoch 67 -kfold 1-n_latent 3 Loss:  4974.7199\n",
            "Epoch 68 -kfold 1-n_latent 3 Loss:  4960.5383\n",
            "Epoch 69 -kfold 1-n_latent 3 Loss:  5008.5681\n",
            "Epoch 70 -kfold 1-n_latent 3 Loss:  5021.0420\n",
            "Epoch 71 -kfold 1-n_latent 3 Loss:  5087.4675\n",
            "Epoch 72 -kfold 1-n_latent 3 Loss:  5098.2259\n",
            "Epoch 73 -kfold 1-n_latent 3 Loss:  5105.9376\n",
            "Epoch 74 -kfold 1-n_latent 3 Loss:  5137.1120\n",
            "Epoch 75 -kfold 1-n_latent 3 Loss:  5061.4258\n",
            "Epoch 76 -kfold 1-n_latent 3 Loss:  4959.5257\n",
            "Epoch 77 -kfold 1-n_latent 3 Loss:  4919.4454\n",
            "Epoch 78 -kfold 1-n_latent 3 Loss:  4885.3963\n",
            "Epoch 79 -kfold 1-n_latent 3 Loss:  4868.3832\n",
            "Epoch 80 -kfold 1-n_latent 3 Loss:  4827.4250\n",
            "Epoch 81 -kfold 1-n_latent 3 Loss:  4812.6070\n",
            "Epoch 82 -kfold 1-n_latent 3 Loss:  4912.2111\n",
            "Epoch 83 -kfold 1-n_latent 3 Loss:  4827.8859\n",
            "Epoch 84 -kfold 1-n_latent 3 Loss:  4871.0995\n",
            "Epoch 85 -kfold 1-n_latent 3 Loss:  4926.1155\n",
            "Epoch 86 -kfold 1-n_latent 3 Loss:  4897.0717\n",
            "Epoch 87 -kfold 1-n_latent 3 Loss:  4920.8442\n",
            "Epoch 88 -kfold 1-n_latent 3 Loss:  4819.5723\n",
            "Epoch 89 -kfold 1-n_latent 3 Loss:  4779.8561\n",
            "Epoch 90 -kfold 1-n_latent 3 Loss:  4730.0585\n",
            "Epoch 91 -kfold 1-n_latent 3 Loss:  4658.0447\n",
            "Epoch 92 -kfold 1-n_latent 3 Loss:  4629.3060\n",
            "Epoch 93 -kfold 1-n_latent 3 Loss:  4517.6616\n",
            "Epoch 94 -kfold 1-n_latent 3 Loss:  4461.4742\n",
            "Epoch 95 -kfold 1-n_latent 3 Loss:  4443.8771\n",
            "Epoch 96 -kfold 1-n_latent 3 Loss:  4326.2662\n",
            "Epoch 97 -kfold 1-n_latent 3 Loss:  4406.2014\n",
            "Epoch 98 -kfold 1-n_latent 3 Loss:  4412.2089\n",
            "Epoch 99 -kfold 1-n_latent 3 Loss:  4320.4611\n",
            "Epoch 100 -kfold 1-n_latent 3 Loss:  4512.9552\n",
            "Epoch 101 -kfold 1-n_latent 3 Loss:  4484.2946\n",
            "Epoch 102 -kfold 1-n_latent 3 Loss:  4491.5005\n",
            "Epoch 103 -kfold 1-n_latent 3 Loss:  4585.3645\n",
            "Epoch 104 -kfold 1-n_latent 3 Loss:  4480.9213\n",
            "Epoch 105 -kfold 1-n_latent 3 Loss:  4568.7779\n",
            "Epoch 106 -kfold 1-n_latent 3 Loss:  4514.5734\n",
            "Epoch 107 -kfold 1-n_latent 3 Loss:  4444.0824\n",
            "Epoch 108 -kfold 1-n_latent 3 Loss:  4700.9241\n",
            "Epoch 109 -kfold 1-n_latent 3 Loss:  4670.4298\n",
            "Epoch 110 -kfold 1-n_latent 3 Loss:  4645.1355\n",
            "Epoch 111 -kfold 1-n_latent 3 Loss:  4689.5339\n",
            "Epoch 112 -kfold 1-n_latent 3 Loss:  4584.3070\n",
            "Epoch 113 -kfold 1-n_latent 3 Loss:  4516.6345\n",
            "Epoch 114 -kfold 1-n_latent 3 Loss:  4650.2615\n",
            "Epoch 115 -kfold 1-n_latent 3 Loss:  4605.9780\n",
            "Epoch 116 -kfold 1-n_latent 3 Loss:  4652.5754\n",
            "Epoch 117 -kfold 1-n_latent 3 Loss:  4644.7342\n",
            "Epoch 118 -kfold 1-n_latent 3 Loss:  4583.5999\n",
            "Epoch 119 -kfold 1-n_latent 3 Loss:  4583.2527\n",
            "Epoch 120 -kfold 1-n_latent 3 Loss:  4529.4064\n",
            "Epoch 121 -kfold 1-n_latent 3 Loss:  4552.7526\n",
            "Epoch 122 -kfold 1-n_latent 3 Loss:  4631.0344\n",
            "Epoch 123 -kfold 1-n_latent 3 Loss:  4558.0263\n",
            "Epoch 124 -kfold 1-n_latent 3 Loss:  4612.2846\n",
            "Epoch 125 -kfold 1-n_latent 3 Loss:  4558.1443\n",
            "Epoch 126 -kfold 1-n_latent 3 Loss:  4516.6151\n",
            "Epoch 127 -kfold 1-n_latent 3 Loss:  4536.2313\n",
            "Epoch 128 -kfold 1-n_latent 3 Loss:  4553.3744\n",
            "Epoch 129 -kfold 1-n_latent 3 Loss:  4692.4182\n",
            "Epoch 130 -kfold 1-n_latent 3 Loss:  4720.5403\n",
            "Epoch 131 -kfold 1-n_latent 3 Loss:  4712.2009\n",
            "Epoch 132 -kfold 1-n_latent 3 Loss:  4731.7717\n",
            "Epoch 133 -kfold 1-n_latent 3 Loss:  4684.7663\n",
            "Epoch 134 -kfold 1-n_latent 3 Loss:  4615.3788\n",
            "Epoch 135 -kfold 1-n_latent 3 Loss:  4545.4416\n",
            "Epoch 136 -kfold 1-n_latent 3 Loss:  4515.0635\n",
            "Epoch 137 -kfold 1-n_latent 3 Loss:  4392.3271\n",
            "Epoch 138 -kfold 1-n_latent 3 Loss:  4359.9594\n",
            "Epoch 139 -kfold 1-n_latent 3 Loss:  4286.3436\n",
            "Epoch 140 -kfold 1-n_latent 3 Loss:  4394.6653\n",
            "Epoch 141 -kfold 1-n_latent 3 Loss:  4527.3674\n",
            "Epoch 142 -kfold 1-n_latent 3 Loss:  4428.3057\n",
            "Epoch 143 -kfold 1-n_latent 3 Loss:  4489.1163\n",
            "Epoch 144 -kfold 1-n_latent 3 Loss:  4379.1927\n",
            "Epoch 145 -kfold 1-n_latent 3 Loss:  4390.8646\n",
            "Epoch 146 -kfold 1-n_latent 3 Loss:  4448.5573\n",
            "Epoch 147 -kfold 1-n_latent 3 Loss:  4384.3564\n",
            "Epoch 148 -kfold 1-n_latent 3 Loss:  4304.9141\n",
            "Epoch 149 -kfold 1-n_latent 3 Loss:  4313.7051\n",
            "Epoch 150 -kfold 1-n_latent 3 Loss:  4245.7543\n",
            "Epoch 151 -kfold 1-n_latent 3 Loss:  4182.2278\n",
            "Epoch 152 -kfold 1-n_latent 3 Loss:  4122.6322\n",
            "Epoch 153 -kfold 1-n_latent 3 Loss:  4059.8483\n",
            "Epoch 154 -kfold 1-n_latent 3 Loss:  4048.1879\n",
            "Epoch 155 -kfold 1-n_latent 3 Loss:  4015.0908\n",
            "Epoch 156 -kfold 1-n_latent 3 Loss:  3961.3130\n",
            "Epoch 157 -kfold 1-n_latent 3 Loss:  3923.1233\n",
            "Epoch 158 -kfold 1-n_latent 3 Loss:  3892.0001\n",
            "Epoch 159 -kfold 1-n_latent 3 Loss:  3811.7656\n",
            "Epoch 160 -kfold 1-n_latent 3 Loss:  3749.8850\n",
            "Epoch 161 -kfold 1-n_latent 3 Loss:  3639.4433\n",
            "Epoch 162 -kfold 1-n_latent 3 Loss:  3652.6074\n",
            "Epoch 163 -kfold 1-n_latent 3 Loss:  3748.5514\n",
            "Epoch 164 -kfold 1-n_latent 3 Loss:  3818.0977\n",
            "Epoch 165 -kfold 1-n_latent 3 Loss:  3888.8887\n",
            "Epoch 166 -kfold 1-n_latent 3 Loss:  4153.2304\n",
            "Epoch 167 -kfold 1-n_latent 3 Loss:  4131.8039\n",
            "Epoch 168 -kfold 1-n_latent 3 Loss:  4250.3899\n",
            "Epoch 169 -kfold 1-n_latent 3 Loss:  4408.1409\n",
            "Epoch 170 -kfold 1-n_latent 3 Loss:  4468.1460\n",
            "Epoch 171 -kfold 1-n_latent 3 Loss:  4454.1024\n",
            "Epoch 172 -kfold 1-n_latent 3 Loss:  4427.1438\n",
            "Epoch 173 -kfold 1-n_latent 3 Loss:  4416.8892\n",
            "Epoch 174 -kfold 1-n_latent 3 Loss:  4349.2879\n",
            "Epoch 175 -kfold 1-n_latent 3 Loss:  4375.4959\n",
            "Epoch 176 -kfold 1-n_latent 3 Loss:  4352.1104\n",
            "Epoch 177 -kfold 1-n_latent 3 Loss:  4253.6589\n",
            "Epoch 178 -kfold 1-n_latent 3 Loss:  4150.7636\n",
            "Epoch 179 -kfold 1-n_latent 3 Loss:  4083.4947\n",
            "Epoch 180 -kfold 1-n_latent 3 Loss:  4097.1526\n",
            "Epoch 181 -kfold 1-n_latent 3 Loss:  4092.8690\n",
            "Epoch 182 -kfold 1-n_latent 3 Loss:  4031.2873\n",
            "Epoch 183 -kfold 1-n_latent 3 Loss:  3999.1217\n",
            "Epoch 184 -kfold 1-n_latent 3 Loss:  3990.5619\n",
            "Epoch 185 -kfold 1-n_latent 3 Loss:  3811.6464\n",
            "Epoch 186 -kfold 1-n_latent 3 Loss:  3849.7560\n",
            "Epoch 187 -kfold 1-n_latent 3 Loss:  3723.0325\n",
            "Epoch 188 -kfold 1-n_latent 3 Loss:  3661.0197\n",
            "Epoch 189 -kfold 1-n_latent 3 Loss:  3711.7460\n",
            "Epoch 190 -kfold 1-n_latent 3 Loss:  3650.8446\n",
            "Epoch 191 -kfold 1-n_latent 3 Loss:  3577.9491\n",
            "Epoch 192 -kfold 1-n_latent 3 Loss:  3564.6180\n",
            "Epoch 193 -kfold 1-n_latent 3 Loss:  3617.5399\n",
            "Epoch 194 -kfold 1-n_latent 3 Loss:  3622.0231\n",
            "Epoch 195 -kfold 1-n_latent 3 Loss:  3501.0605\n",
            "Epoch 196 -kfold 1-n_latent 3 Loss:  3380.1863\n",
            "Epoch 197 -kfold 1-n_latent 3 Loss:  3251.4606\n",
            "Epoch 198 -kfold 1-n_latent 3 Loss:  3237.6505\n",
            "Epoch 199 -kfold 1-n_latent 3 Loss:  3170.6254\n",
            "Epoch 200 -kfold 1-n_latent 3 Loss:  3060.7915\n",
            "Epoch 201 -kfold 1-n_latent 3 Loss:  3013.0567\n",
            "Epoch 202 -kfold 1-n_latent 3 Loss:  2967.3545\n",
            "Epoch 203 -kfold 1-n_latent 3 Loss:  2835.7064\n",
            "Epoch 204 -kfold 1-n_latent 3 Loss:  2840.3076\n",
            "Epoch 205 -kfold 1-n_latent 3 Loss:  2807.3679\n",
            "Epoch 206 -kfold 1-n_latent 3 Loss:  2754.2048\n",
            "Epoch 207 -kfold 1-n_latent 3 Loss:  2991.2088\n",
            "Epoch 208 -kfold 1-n_latent 3 Loss:  3036.6372\n",
            "Epoch 209 -kfold 1-n_latent 3 Loss:  3204.3370\n",
            "Epoch 210 -kfold 1-n_latent 3 Loss:  3158.8618\n",
            "Epoch 211 -kfold 1-n_latent 3 Loss:  3116.6856\n",
            "Epoch 212 -kfold 1-n_latent 3 Loss:  3030.6704\n",
            "Epoch 213 -kfold 1-n_latent 3 Loss:  3000.6634\n",
            "Epoch 214 -kfold 1-n_latent 3 Loss:  2932.3523\n",
            "Epoch 215 -kfold 1-n_latent 3 Loss:  2819.6883\n",
            "Epoch 216 -kfold 1-n_latent 3 Loss:  2754.6204\n",
            "Epoch 217 -kfold 1-n_latent 3 Loss:  2952.3750\n",
            "Epoch 218 -kfold 1-n_latent 3 Loss:  2971.0501\n",
            "Epoch 219 -kfold 1-n_latent 3 Loss:  2977.8496\n",
            "Epoch 220 -kfold 1-n_latent 3 Loss:  3131.0154\n",
            "Epoch 221 -kfold 1-n_latent 3 Loss:  3040.9730\n",
            "Epoch 222 -kfold 1-n_latent 3 Loss:  3336.6977\n",
            "Epoch 223 -kfold 1-n_latent 3 Loss:  3533.6191\n",
            "Epoch 224 -kfold 1-n_latent 3 Loss:  3491.9661\n",
            "Epoch 225 -kfold 1-n_latent 3 Loss:  3510.5362\n",
            "Epoch 226 -kfold 1-n_latent 3 Loss:  3506.2072\n",
            "Epoch 227 -kfold 1-n_latent 3 Loss:  3418.1072\n",
            "Epoch 228 -kfold 1-n_latent 3 Loss:  3311.5451\n",
            "Epoch 229 -kfold 1-n_latent 3 Loss:  3227.4881\n",
            "Epoch 230 -kfold 1-n_latent 3 Loss:  3198.2520\n",
            "Epoch 231 -kfold 1-n_latent 3 Loss:  3233.5723\n",
            "Epoch 232 -kfold 1-n_latent 3 Loss:  3209.6623\n",
            "Epoch 233 -kfold 1-n_latent 3 Loss:  3362.4765\n",
            "Epoch 234 -kfold 1-n_latent 3 Loss:  3425.4003\n",
            "Epoch 235 -kfold 1-n_latent 3 Loss:  3475.4971\n",
            "Epoch 236 -kfold 1-n_latent 3 Loss:  3674.3759\n",
            "Epoch 237 -kfold 1-n_latent 3 Loss:  3773.2868\n",
            "Epoch 238 -kfold 1-n_latent 3 Loss:  3775.0181\n",
            "Epoch 239 -kfold 1-n_latent 3 Loss:  3738.8482\n",
            "Epoch 240 -kfold 1-n_latent 3 Loss:  3629.1739\n",
            "Epoch 241 -kfold 1-n_latent 3 Loss:  3618.0261\n",
            "Epoch 242 -kfold 1-n_latent 3 Loss:  3567.4532\n",
            "Epoch 243 -kfold 1-n_latent 3 Loss:  3471.9177\n",
            "Epoch 244 -kfold 1-n_latent 3 Loss:  3415.7226\n",
            "Epoch 245 -kfold 1-n_latent 3 Loss:  3405.8275\n",
            "Epoch 246 -kfold 1-n_latent 3 Loss:  3340.3237\n",
            "Epoch 247 -kfold 1-n_latent 3 Loss:  3341.1591\n",
            "Epoch 248 -kfold 1-n_latent 3 Loss:  3548.9633\n",
            "Epoch 249 -kfold 1-n_latent 3 Loss:  3535.8943\n",
            "Epoch 250 -kfold 1-n_latent 3 Loss:  3397.9103\n",
            "Epoch 251 -kfold 1-n_latent 3 Loss:  3340.3749\n",
            "Epoch 252 -kfold 1-n_latent 3 Loss:  3169.1652\n",
            "Epoch 253 -kfold 1-n_latent 3 Loss:  3078.4990\n",
            "Epoch 254 -kfold 1-n_latent 3 Loss:  3007.4323\n",
            "Epoch 255 -kfold 1-n_latent 3 Loss:  2953.4941\n",
            "Epoch 256 -kfold 1-n_latent 3 Loss:  2794.9246\n",
            "Epoch 257 -kfold 1-n_latent 3 Loss:  2697.5433\n",
            "Epoch 258 -kfold 1-n_latent 3 Loss:  2752.0720\n",
            "Epoch 259 -kfold 1-n_latent 3 Loss:  2660.5907\n",
            "Epoch 260 -kfold 1-n_latent 3 Loss:  2777.7384\n",
            "Epoch 261 -kfold 1-n_latent 3 Loss:  2670.9913\n",
            "Epoch 262 -kfold 1-n_latent 3 Loss:  2524.1006\n",
            "Epoch 263 -kfold 1-n_latent 3 Loss:  2445.3639\n",
            "Epoch 264 -kfold 1-n_latent 3 Loss:  2354.7850\n",
            "Epoch 265 -kfold 1-n_latent 3 Loss:  2316.3745\n",
            "Epoch 266 -kfold 1-n_latent 3 Loss:  2285.9969\n",
            "Epoch 267 -kfold 1-n_latent 3 Loss:  2207.8108\n",
            "Epoch 268 -kfold 1-n_latent 3 Loss:  2260.8432\n",
            "Epoch 269 -kfold 1-n_latent 3 Loss:  2289.7982\n",
            "Epoch 270 -kfold 1-n_latent 3 Loss:  2311.9210\n",
            "Epoch 271 -kfold 1-n_latent 3 Loss:  2305.2314\n",
            "Epoch 272 -kfold 1-n_latent 3 Loss:  2368.9522\n",
            "Epoch 273 -kfold 1-n_latent 3 Loss:  2333.0032\n",
            "Epoch 274 -kfold 1-n_latent 3 Loss:  2245.1026\n",
            "Epoch 275 -kfold 1-n_latent 3 Loss:  2242.8130\n",
            "Epoch 276 -kfold 1-n_latent 3 Loss:  2171.6567\n",
            "Epoch 277 -kfold 1-n_latent 3 Loss:  2175.5026\n",
            "Epoch 278 -kfold 1-n_latent 3 Loss:  2103.5140\n",
            "Epoch 279 -kfold 1-n_latent 3 Loss:  2123.5348\n",
            "Epoch 280 -kfold 1-n_latent 3 Loss:  2031.3084\n",
            "Epoch 281 -kfold 1-n_latent 3 Loss:  1937.3537\n",
            "Epoch 282 -kfold 1-n_latent 3 Loss:  1898.6869\n",
            "Epoch 283 -kfold 1-n_latent 3 Loss:  1759.2194\n",
            "Epoch 284 -kfold 1-n_latent 3 Loss:  1715.7080\n",
            "Epoch 285 -kfold 1-n_latent 3 Loss:  1705.5274\n",
            "Epoch 286 -kfold 1-n_latent 3 Loss:  1728.8188\n",
            "Epoch 287 -kfold 1-n_latent 3 Loss:  1674.3599\n",
            "Epoch 288 -kfold 1-n_latent 3 Loss:  1469.0874\n",
            "Epoch 289 -kfold 1-n_latent 3 Loss:  1442.0512\n",
            "Epoch 290 -kfold 1-n_latent 3 Loss:  1386.6659\n",
            "Epoch 291 -kfold 1-n_latent 3 Loss:  1425.1966\n",
            "Epoch 292 -kfold 1-n_latent 3 Loss:  1536.4155\n",
            "Epoch 293 -kfold 1-n_latent 3 Loss:  1697.6327\n",
            "Epoch 294 -kfold 1-n_latent 3 Loss:  1834.0838\n",
            "Epoch 295 -kfold 1-n_latent 3 Loss:  1901.9447\n",
            "Epoch 296 -kfold 1-n_latent 3 Loss:  2125.3886\n",
            "Epoch 297 -kfold 1-n_latent 3 Loss:  2078.9078\n",
            "Epoch 298 -kfold 1-n_latent 3 Loss:  2174.8701\n",
            "Epoch 299 -kfold 1-n_latent 3 Loss:  2099.7673\n",
            "Epoch 300 -kfold 1-n_latent 3 Loss:  2112.7026\n",
            "Epoch 301 -kfold 1-n_latent 3 Loss:  2063.9824\n",
            "Epoch 302 -kfold 1-n_latent 3 Loss:  2111.9474\n",
            "Epoch 303 -kfold 1-n_latent 3 Loss:  2185.3062\n",
            "Epoch 304 -kfold 1-n_latent 3 Loss:  2090.1975\n",
            "Epoch 305 -kfold 1-n_latent 3 Loss:  2155.7213\n",
            "Epoch 306 -kfold 1-n_latent 3 Loss:  2111.9965\n",
            "Epoch 307 -kfold 1-n_latent 3 Loss:  2274.2482\n",
            "Epoch 308 -kfold 1-n_latent 3 Loss:  2180.7161\n",
            "Epoch 309 -kfold 1-n_latent 3 Loss:  2205.3060\n",
            "Epoch 310 -kfold 1-n_latent 3 Loss:  2212.6753\n",
            "Epoch 311 -kfold 1-n_latent 3 Loss:  2199.5417\n",
            "Epoch 312 -kfold 1-n_latent 3 Loss:  2285.0763\n",
            "Epoch 313 -kfold 1-n_latent 3 Loss:  2181.9739\n",
            "Epoch 314 -kfold 1-n_latent 3 Loss:  2142.6292\n",
            "Epoch 315 -kfold 1-n_latent 3 Loss:  2065.5407\n",
            "Epoch 316 -kfold 1-n_latent 3 Loss:  1948.9129\n",
            "Epoch 317 -kfold 1-n_latent 3 Loss:  1912.9417\n",
            "Epoch 318 -kfold 1-n_latent 3 Loss:  1825.0848\n",
            "Epoch 319 -kfold 1-n_latent 3 Loss:  1669.5296\n",
            "Epoch 320 -kfold 1-n_latent 3 Loss:  1490.8347\n",
            "Epoch 321 -kfold 1-n_latent 3 Loss:  1387.0870\n",
            "Epoch 322 -kfold 1-n_latent 3 Loss:  1378.1553\n",
            "Epoch 323 -kfold 1-n_latent 3 Loss:  1438.6640\n",
            "Epoch 324 -kfold 1-n_latent 3 Loss:  1519.5391\n",
            "Epoch 325 -kfold 1-n_latent 3 Loss:  1582.8343\n",
            "Epoch 326 -kfold 1-n_latent 3 Loss:  1708.3263\n",
            "Epoch 327 -kfold 1-n_latent 3 Loss:  1696.4584\n",
            "Epoch 328 -kfold 1-n_latent 3 Loss:  1869.3840\n",
            "Epoch 329 -kfold 1-n_latent 3 Loss:  1893.3325\n",
            "Epoch 330 -kfold 1-n_latent 3 Loss:  1927.2080\n",
            "Epoch 331 -kfold 1-n_latent 3 Loss:  2012.7224\n",
            "Epoch 332 -kfold 1-n_latent 3 Loss:  1836.1085\n",
            "Epoch 333 -kfold 1-n_latent 3 Loss:  1907.9477\n",
            "Epoch 334 -kfold 1-n_latent 3 Loss:  1907.4735\n",
            "Epoch 335 -kfold 1-n_latent 3 Loss:  1948.3263\n",
            "Epoch 336 -kfold 1-n_latent 3 Loss:  2079.5791\n",
            "Epoch 337 -kfold 1-n_latent 3 Loss:  2019.3799\n",
            "Epoch 338 -kfold 1-n_latent 3 Loss:  1920.5523\n",
            "Epoch 339 -kfold 1-n_latent 3 Loss:  1950.8574\n",
            "Epoch 340 -kfold 1-n_latent 3 Loss:  1915.7908\n",
            "Epoch 341 -kfold 1-n_latent 3 Loss:  1808.4967\n",
            "Epoch 342 -kfold 1-n_latent 3 Loss:  1765.7240\n",
            "Epoch 343 -kfold 1-n_latent 3 Loss:  1787.9366\n",
            "Epoch 344 -kfold 1-n_latent 3 Loss:  2241.0375\n",
            "Epoch 345 -kfold 1-n_latent 3 Loss:  2458.0813\n",
            "Epoch 346 -kfold 1-n_latent 3 Loss:  2584.3818\n",
            "Epoch 347 -kfold 1-n_latent 3 Loss:  2798.2274\n",
            "Epoch 348 -kfold 1-n_latent 3 Loss:  2900.3100\n",
            "Epoch 349 -kfold 1-n_latent 3 Loss:  2977.2724\n",
            "Epoch 350 -kfold 1-n_latent 3 Loss:  3048.1517\n",
            "Epoch 351 -kfold 1-n_latent 3 Loss:  2923.1820\n",
            "Epoch 352 -kfold 1-n_latent 3 Loss:  2925.0474\n",
            "Epoch 353 -kfold 1-n_latent 3 Loss:  2827.6630\n",
            "Epoch 354 -kfold 1-n_latent 3 Loss:  2776.0959\n",
            "Epoch 355 -kfold 1-n_latent 3 Loss:  2791.8325\n",
            "Epoch 356 -kfold 1-n_latent 3 Loss:  2681.5434\n",
            "Epoch 357 -kfold 1-n_latent 3 Loss:  2772.5046\n",
            "Epoch 358 -kfold 1-n_latent 3 Loss:  2733.4503\n",
            "Epoch 359 -kfold 1-n_latent 3 Loss:  2745.3388\n",
            "Epoch 360 -kfold 1-n_latent 3 Loss:  2702.1067\n",
            "Epoch 361 -kfold 1-n_latent 3 Loss:  2612.1711\n",
            "Epoch 362 -kfold 1-n_latent 3 Loss:  2598.8110\n",
            "Epoch 363 -kfold 1-n_latent 3 Loss:  2523.4904\n",
            "Epoch 364 -kfold 1-n_latent 3 Loss:  2542.7673\n",
            "Epoch 365 -kfold 1-n_latent 3 Loss:  2570.6260\n",
            "Epoch 366 -kfold 1-n_latent 3 Loss:  2828.7739\n",
            "Epoch 367 -kfold 1-n_latent 3 Loss:  2642.5090\n",
            "Epoch 368 -kfold 1-n_latent 3 Loss:  2714.6712\n",
            "Epoch 369 -kfold 1-n_latent 3 Loss:  2613.0876\n",
            "Epoch 370 -kfold 1-n_latent 3 Loss:  2455.1392\n",
            "Epoch 371 -kfold 1-n_latent 3 Loss:  2362.7855\n",
            "Epoch 372 -kfold 1-n_latent 3 Loss:  2373.1876\n",
            "Epoch 373 -kfold 1-n_latent 3 Loss:  2385.1424\n",
            "Epoch 374 -kfold 1-n_latent 3 Loss:  2382.8364\n",
            "Epoch 375 -kfold 1-n_latent 3 Loss:  2392.2590\n",
            "Epoch 376 -kfold 1-n_latent 3 Loss:  2363.4458\n",
            "Epoch 377 -kfold 1-n_latent 3 Loss:  2320.1243\n",
            "Epoch 378 -kfold 1-n_latent 3 Loss:  2314.7289\n",
            "Epoch 379 -kfold 1-n_latent 3 Loss:  2229.2774\n",
            "Epoch 380 -kfold 1-n_latent 3 Loss:  2422.9056\n",
            "Epoch 381 -kfold 1-n_latent 3 Loss:  2256.5573\n",
            "Epoch 382 -kfold 1-n_latent 3 Loss:  2106.7537\n",
            "Epoch 383 -kfold 1-n_latent 3 Loss:  2012.9994\n",
            "Epoch 384 -kfold 1-n_latent 3 Loss:  1969.5942\n",
            "Epoch 385 -kfold 1-n_latent 3 Loss:  1862.5631\n",
            "Epoch 386 -kfold 1-n_latent 3 Loss:  1776.5536\n",
            "Epoch 387 -kfold 1-n_latent 3 Loss:  1743.2324\n",
            "Epoch 388 -kfold 1-n_latent 3 Loss:  1767.2016\n",
            "Epoch 389 -kfold 1-n_latent 3 Loss:  1733.6170\n",
            "Epoch 390 -kfold 1-n_latent 3 Loss:  1721.5639\n",
            "Epoch 391 -kfold 1-n_latent 3 Loss:  1594.2171\n",
            "Epoch 392 -kfold 1-n_latent 3 Loss:  1502.3264\n",
            "Epoch 393 -kfold 1-n_latent 3 Loss:  1343.5946\n",
            "Epoch 394 -kfold 1-n_latent 3 Loss:  1231.2219\n",
            "Epoch 395 -kfold 1-n_latent 3 Loss:  1240.0776\n",
            "Epoch 396 -kfold 1-n_latent 3 Loss:  1178.9515\n",
            "Epoch 397 -kfold 1-n_latent 3 Loss:  1108.0247\n",
            "Epoch 398 -kfold 1-n_latent 3 Loss:  1161.7546\n",
            "Epoch 399 -kfold 1-n_latent 3 Loss:  1198.2003\n",
            "Epoch 400 -kfold 1-n_latent 3 Loss:  1172.3878\n",
            "Epoch 401 -kfold 1-n_latent 3 Loss:  1154.4754\n",
            "Epoch 402 -kfold 1-n_latent 3 Loss:  1069.1055\n",
            "Epoch 403 -kfold 1-n_latent 3 Loss:  1021.6779\n",
            "Epoch 404 -kfold 1-n_latent 3 Loss:  934.8171\n",
            "Epoch 405 -kfold 1-n_latent 3 Loss:  758.3487\n",
            "Epoch 406 -kfold 1-n_latent 3 Loss:  656.8308\n",
            "Epoch 407 -kfold 1-n_latent 3 Loss:  723.4490\n",
            "Epoch 408 -kfold 1-n_latent 3 Loss:  888.3951\n",
            "Epoch 409 -kfold 1-n_latent 3 Loss:  749.5918\n",
            "Epoch 410 -kfold 1-n_latent 3 Loss:  714.7498\n",
            "Epoch 411 -kfold 1-n_latent 3 Loss:  764.3427\n",
            "Epoch 412 -kfold 1-n_latent 3 Loss:  697.2798\n",
            "Epoch 413 -kfold 1-n_latent 3 Loss:  610.0323\n",
            "Epoch 414 -kfold 1-n_latent 3 Loss:  524.4724\n",
            "Epoch 415 -kfold 1-n_latent 3 Loss:  468.2209\n",
            "Epoch 416 -kfold 1-n_latent 3 Loss:  514.6213\n",
            "Epoch 417 -kfold 1-n_latent 3 Loss:  482.2871\n",
            "Epoch 418 -kfold 1-n_latent 3 Loss:  732.7681\n",
            "Epoch 419 -kfold 1-n_latent 3 Loss:  1003.2610\n",
            "Epoch 420 -kfold 1-n_latent 3 Loss:  1280.1994\n",
            "Epoch 421 -kfold 1-n_latent 3 Loss:  1416.5386\n",
            "Epoch 422 -kfold 1-n_latent 3 Loss:  1622.0883\n",
            "Epoch 423 -kfold 1-n_latent 3 Loss:  1572.0043\n",
            "Epoch 424 -kfold 1-n_latent 3 Loss:  1702.8830\n",
            "Epoch 425 -kfold 1-n_latent 3 Loss:  1639.6019\n",
            "Epoch 426 -kfold 1-n_latent 3 Loss:  1581.1608\n",
            "Epoch 427 -kfold 1-n_latent 3 Loss:  1390.9347\n",
            "Epoch 428 -kfold 1-n_latent 3 Loss:  1314.8791\n",
            "Epoch 429 -kfold 1-n_latent 3 Loss:  1379.6479\n",
            "Epoch 430 -kfold 1-n_latent 3 Loss:  1300.3395\n",
            "Epoch 431 -kfold 1-n_latent 3 Loss:  1269.4575\n",
            "Epoch 432 -kfold 1-n_latent 3 Loss:  1262.7060\n",
            "Epoch 433 -kfold 1-n_latent 3 Loss:  1401.0334\n",
            "Epoch 434 -kfold 1-n_latent 3 Loss:  1476.5330\n",
            "Epoch 435 -kfold 1-n_latent 3 Loss:  1321.7199\n",
            "Epoch 436 -kfold 1-n_latent 3 Loss:  1268.0876\n",
            "Epoch 437 -kfold 1-n_latent 3 Loss:  1119.2353\n",
            "Epoch 438 -kfold 1-n_latent 3 Loss:  1037.2834\n",
            "Epoch 439 -kfold 1-n_latent 3 Loss:  966.4743\n",
            "Epoch 440 -kfold 1-n_latent 3 Loss:  831.9104\n",
            "Epoch 441 -kfold 1-n_latent 3 Loss:  638.6494\n",
            "Epoch 442 -kfold 1-n_latent 3 Loss:  728.8317\n",
            "Epoch 443 -kfold 1-n_latent 3 Loss:  778.5703\n",
            "Epoch 444 -kfold 1-n_latent 3 Loss:  760.2914\n",
            "Epoch 445 -kfold 1-n_latent 3 Loss:  814.7613\n",
            "Epoch 446 -kfold 1-n_latent 3 Loss:  738.4392\n",
            "Epoch 447 -kfold 1-n_latent 3 Loss:  846.0956\n",
            "Epoch 448 -kfold 1-n_latent 3 Loss:  1054.0314\n",
            "Epoch 449 -kfold 1-n_latent 3 Loss:  1716.5951\n",
            "Epoch 450 -kfold 1-n_latent 3 Loss:  1789.1241\n",
            "Epoch 451 -kfold 1-n_latent 3 Loss:  1715.0106\n",
            "Epoch 452 -kfold 1-n_latent 3 Loss:  1803.9354\n",
            "Epoch 453 -kfold 1-n_latent 3 Loss:  1827.2634\n",
            "Epoch 454 -kfold 1-n_latent 3 Loss:  1886.6473\n",
            "Epoch 455 -kfold 1-n_latent 3 Loss:  1769.2755\n",
            "Epoch 456 -kfold 1-n_latent 3 Loss:  1641.1766\n",
            "Epoch 457 -kfold 1-n_latent 3 Loss:  1490.7914\n",
            "Epoch 458 -kfold 1-n_latent 3 Loss:  1375.2515\n",
            "Epoch 459 -kfold 1-n_latent 3 Loss:  1280.8048\n",
            "Epoch 460 -kfold 1-n_latent 3 Loss:  1350.7650\n",
            "Epoch 461 -kfold 1-n_latent 3 Loss:  1462.1345\n",
            "Epoch 462 -kfold 1-n_latent 3 Loss:  1384.7426\n",
            "Epoch 463 -kfold 1-n_latent 3 Loss:  1304.6642\n",
            "Epoch 464 -kfold 1-n_latent 3 Loss:  1344.8268\n",
            "Epoch 465 -kfold 1-n_latent 3 Loss:  1208.4147\n",
            "Epoch 466 -kfold 1-n_latent 3 Loss:  1223.0045\n",
            "Epoch 467 -kfold 1-n_latent 3 Loss:  1089.3182\n",
            "Epoch 468 -kfold 1-n_latent 3 Loss:  1070.3404\n",
            "Epoch 469 -kfold 1-n_latent 3 Loss:  1169.3187\n",
            "Epoch 470 -kfold 1-n_latent 3 Loss:  1196.1757\n",
            "Epoch 471 -kfold 1-n_latent 3 Loss:  1093.6563\n",
            "Epoch 472 -kfold 1-n_latent 3 Loss:  1200.8568\n",
            "Epoch 473 -kfold 1-n_latent 3 Loss:  1348.8419\n",
            "Epoch 474 -kfold 1-n_latent 3 Loss:  1434.2403\n",
            "Epoch 475 -kfold 1-n_latent 3 Loss:  1485.8085\n",
            "Epoch 476 -kfold 1-n_latent 3 Loss:  1469.2365\n",
            "Epoch 477 -kfold 1-n_latent 3 Loss:  1593.8582\n",
            "Epoch 478 -kfold 1-n_latent 3 Loss:  1627.1736\n",
            "Epoch 479 -kfold 1-n_latent 3 Loss:  1689.7099\n",
            "Epoch 480 -kfold 1-n_latent 3 Loss:  1878.1110\n",
            "Epoch 481 -kfold 1-n_latent 3 Loss:  1912.2578\n",
            "Epoch 482 -kfold 1-n_latent 3 Loss:  2043.7466\n",
            "Epoch 483 -kfold 1-n_latent 3 Loss:  2156.2558\n",
            "Epoch 484 -kfold 1-n_latent 3 Loss:  2366.5199\n",
            "Epoch 485 -kfold 1-n_latent 3 Loss:  2376.0913\n",
            "Epoch 486 -kfold 1-n_latent 3 Loss:  2359.9631\n",
            "Epoch 487 -kfold 1-n_latent 3 Loss:  2378.8168\n",
            "Epoch 488 -kfold 1-n_latent 3 Loss:  2480.5382\n",
            "Epoch 489 -kfold 1-n_latent 3 Loss:  2329.7024\n",
            "Epoch 490 -kfold 1-n_latent 3 Loss:  2325.3155\n",
            "Epoch 491 -kfold 1-n_latent 3 Loss:  2292.7914\n",
            "Epoch 492 -kfold 1-n_latent 3 Loss:  2257.0675\n",
            "Epoch 493 -kfold 1-n_latent 3 Loss:  2263.5977\n",
            "Epoch 494 -kfold 1-n_latent 3 Loss:  2277.5767\n",
            "Epoch 495 -kfold 1-n_latent 3 Loss:  2285.5496\n",
            "Epoch 496 -kfold 1-n_latent 3 Loss:  2237.4257\n",
            "Epoch 497 -kfold 1-n_latent 3 Loss:  2147.6885\n",
            "Epoch 498 -kfold 1-n_latent 3 Loss:  2096.3124\n",
            "Epoch 499 -kfold 1-n_latent 3 Loss:  2117.9082\n",
            "Epoch 500 -kfold 1-n_latent 3 Loss:  2046.1184\n",
            "Epoch 501 -kfold 1-n_latent 3 Loss:  2046.5647\n",
            "Epoch 502 -kfold 1-n_latent 3 Loss:  1952.9148\n",
            "Epoch 503 -kfold 1-n_latent 3 Loss:  2015.1855\n",
            "Epoch 504 -kfold 1-n_latent 3 Loss:  2013.2072\n",
            "Epoch 505 -kfold 1-n_latent 3 Loss:  1966.0941\n",
            "Epoch 506 -kfold 1-n_latent 3 Loss:  1963.4509\n",
            "Epoch 507 -kfold 1-n_latent 3 Loss:  1954.1919\n",
            "Epoch 508 -kfold 1-n_latent 3 Loss:  1979.2138\n",
            "Epoch 509 -kfold 1-n_latent 3 Loss:  2003.8199\n",
            "Epoch 510 -kfold 1-n_latent 3 Loss:  2082.2276\n",
            "Epoch 511 -kfold 1-n_latent 3 Loss:  2156.0498\n",
            "Epoch 512 -kfold 1-n_latent 3 Loss:  2142.2141\n",
            "Epoch 513 -kfold 1-n_latent 3 Loss:  2023.4310\n",
            "Epoch 514 -kfold 1-n_latent 3 Loss:  1999.2154\n",
            "Epoch 515 -kfold 1-n_latent 3 Loss:  1902.0847\n",
            "Epoch 516 -kfold 1-n_latent 3 Loss:  1785.9265\n",
            "Epoch 517 -kfold 1-n_latent 3 Loss:  1715.4327\n",
            "Epoch 518 -kfold 1-n_latent 3 Loss:  1617.4561\n",
            "Epoch 519 -kfold 1-n_latent 3 Loss:  1576.7276\n",
            "Epoch 520 -kfold 1-n_latent 3 Loss:  1577.4057\n",
            "Epoch 521 -kfold 1-n_latent 3 Loss:  1470.4088\n",
            "Epoch 522 -kfold 1-n_latent 3 Loss:  1527.0453\n",
            "Epoch 523 -kfold 1-n_latent 3 Loss:  1579.4836\n",
            "Epoch 524 -kfold 1-n_latent 3 Loss:  1631.3088\n",
            "Epoch 525 -kfold 1-n_latent 3 Loss:  1726.9474\n",
            "Epoch 526 -kfold 1-n_latent 3 Loss:  1794.0920\n",
            "Epoch 527 -kfold 1-n_latent 3 Loss:  1800.8272\n",
            "Epoch 528 -kfold 1-n_latent 3 Loss:  1955.7561\n",
            "Epoch 529 -kfold 1-n_latent 3 Loss:  2050.8819\n",
            "Epoch 530 -kfold 1-n_latent 3 Loss:  2149.3948\n",
            "Epoch 531 -kfold 1-n_latent 3 Loss:  2126.1782\n",
            "Epoch 532 -kfold 1-n_latent 3 Loss:  1979.9685\n",
            "Epoch 533 -kfold 1-n_latent 3 Loss:  1988.8471\n",
            "Epoch 534 -kfold 1-n_latent 3 Loss:  1813.4043\n",
            "Epoch 535 -kfold 1-n_latent 3 Loss:  1701.7336\n",
            "Epoch 536 -kfold 1-n_latent 3 Loss:  1636.6666\n",
            "Epoch 537 -kfold 1-n_latent 3 Loss:  1542.7205\n",
            "Epoch 538 -kfold 1-n_latent 3 Loss:  1574.7391\n",
            "Epoch 539 -kfold 1-n_latent 3 Loss:  1543.1420\n",
            "Epoch 540 -kfold 1-n_latent 3 Loss:  1467.0404\n",
            "Epoch 541 -kfold 1-n_latent 3 Loss:  1408.5734\n",
            "Epoch 542 -kfold 1-n_latent 3 Loss:  1624.8525\n",
            "Epoch 543 -kfold 1-n_latent 3 Loss:  1548.0855\n",
            "Epoch 544 -kfold 1-n_latent 3 Loss:  1548.2534\n",
            "Epoch 545 -kfold 1-n_latent 3 Loss:  1624.7311\n",
            "Epoch 546 -kfold 1-n_latent 3 Loss:  1568.9054\n",
            "Epoch 547 -kfold 1-n_latent 3 Loss:  1561.8415\n",
            "Epoch 548 -kfold 1-n_latent 3 Loss:  1445.0409\n",
            "Epoch 549 -kfold 1-n_latent 3 Loss:  1323.7059\n",
            "Epoch 550 -kfold 1-n_latent 3 Loss:  1194.5269\n",
            "Epoch 551 -kfold 1-n_latent 3 Loss:  1097.1164\n",
            "Epoch 552 -kfold 1-n_latent 3 Loss:  1028.8502\n",
            "Epoch 553 -kfold 1-n_latent 3 Loss:  892.9467\n",
            "Epoch 554 -kfold 1-n_latent 3 Loss:  836.0495\n",
            "Epoch 555 -kfold 1-n_latent 3 Loss:  822.2521\n",
            "Epoch 556 -kfold 1-n_latent 3 Loss:  803.9862\n",
            "Epoch 557 -kfold 1-n_latent 3 Loss:  778.2193\n",
            "Epoch 558 -kfold 1-n_latent 3 Loss:  731.4904\n",
            "Epoch 559 -kfold 1-n_latent 3 Loss:  724.0609\n",
            "Epoch 560 -kfold 1-n_latent 3 Loss:  766.6649\n",
            "Epoch 561 -kfold 1-n_latent 3 Loss:  1159.6568\n",
            "Epoch 562 -kfold 1-n_latent 3 Loss:  1170.8197\n",
            "Epoch 563 -kfold 1-n_latent 3 Loss:  1114.0134\n",
            "Epoch 564 -kfold 1-n_latent 3 Loss:  1155.3936\n",
            "Epoch 565 -kfold 1-n_latent 3 Loss:  1067.2110\n",
            "Epoch 566 -kfold 1-n_latent 3 Loss:  1043.4852\n",
            "Epoch 567 -kfold 1-n_latent 3 Loss:  931.0508\n",
            "Epoch 568 -kfold 1-n_latent 3 Loss:  870.8883\n",
            "Epoch 569 -kfold 1-n_latent 3 Loss:  796.2742\n",
            "Epoch 570 -kfold 1-n_latent 3 Loss:  740.7804\n",
            "Epoch 571 -kfold 1-n_latent 3 Loss:  761.8868\n",
            "Epoch 572 -kfold 1-n_latent 3 Loss:  921.1115\n",
            "Epoch 573 -kfold 1-n_latent 3 Loss:  1189.2476\n",
            "Epoch 574 -kfold 1-n_latent 3 Loss:  1370.7160\n",
            "Epoch 575 -kfold 1-n_latent 3 Loss:  1488.2053\n",
            "Epoch 576 -kfold 1-n_latent 3 Loss:  1588.0250\n",
            "Epoch 577 -kfold 1-n_latent 3 Loss:  1676.9342\n",
            "Epoch 578 -kfold 1-n_latent 3 Loss:  1620.7391\n",
            "Epoch 579 -kfold 1-n_latent 3 Loss:  1544.9251\n",
            "Epoch 580 -kfold 1-n_latent 3 Loss:  1479.4272\n",
            "Epoch 581 -kfold 1-n_latent 3 Loss:  1298.3437\n",
            "Epoch 582 -kfold 1-n_latent 3 Loss:  1248.4617\n",
            "Epoch 583 -kfold 1-n_latent 3 Loss:  1094.5631\n",
            "Epoch 584 -kfold 1-n_latent 3 Loss:  857.6115\n",
            "Epoch 585 -kfold 1-n_latent 3 Loss:  740.1619\n",
            "Epoch 586 -kfold 1-n_latent 3 Loss:  744.3792\n",
            "Epoch 587 -kfold 1-n_latent 3 Loss:  715.2176\n",
            "Epoch 588 -kfold 1-n_latent 3 Loss:  811.8483\n",
            "Epoch 589 -kfold 1-n_latent 3 Loss:  740.8451\n",
            "Epoch 590 -kfold 1-n_latent 3 Loss:  824.7184\n",
            "Epoch 591 -kfold 1-n_latent 3 Loss:  832.4230\n",
            "Epoch 592 -kfold 1-n_latent 3 Loss:  992.8283\n",
            "Epoch 593 -kfold 1-n_latent 3 Loss:  1023.0646\n",
            "Epoch 594 -kfold 1-n_latent 3 Loss:  910.9325\n",
            "Epoch 595 -kfold 1-n_latent 3 Loss:  942.9782\n",
            "Epoch 596 -kfold 1-n_latent 3 Loss:  926.9693\n",
            "Epoch 597 -kfold 1-n_latent 3 Loss:  987.8953\n",
            "Epoch 598 -kfold 1-n_latent 3 Loss:  1178.9494\n",
            "Epoch 599 -kfold 1-n_latent 3 Loss:  1196.0098\n",
            "Epoch 600 -kfold 1-n_latent 3 Loss:  1337.0203\n",
            "Epoch 601 -kfold 1-n_latent 3 Loss:  1346.1801\n",
            "Epoch 602 -kfold 1-n_latent 3 Loss:  1353.0547\n",
            "Epoch 603 -kfold 1-n_latent 3 Loss:  1538.9891\n",
            "Epoch 604 -kfold 1-n_latent 3 Loss:  2221.9153\n",
            "Epoch 605 -kfold 1-n_latent 3 Loss:  2115.0881\n",
            "Epoch 606 -kfold 1-n_latent 3 Loss:  2344.5139\n",
            "Epoch 607 -kfold 1-n_latent 3 Loss:  2712.6808\n",
            "Epoch 608 -kfold 1-n_latent 3 Loss:  2946.2765\n",
            "Epoch 609 -kfold 1-n_latent 3 Loss:  2946.4502\n",
            "Epoch 610 -kfold 1-n_latent 3 Loss:  3010.2854\n",
            "Epoch 611 -kfold 1-n_latent 3 Loss:  3161.5109\n",
            "Epoch 612 -kfold 1-n_latent 3 Loss:  3234.7405\n",
            "Epoch 613 -kfold 1-n_latent 3 Loss:  3213.2035\n",
            "Epoch 614 -kfold 1-n_latent 3 Loss:  3147.1102\n",
            "Epoch 615 -kfold 1-n_latent 3 Loss:  3093.8639\n",
            "Epoch 616 -kfold 1-n_latent 3 Loss:  3007.9957\n",
            "Epoch 617 -kfold 1-n_latent 3 Loss:  2817.0088\n",
            "Epoch 618 -kfold 1-n_latent 3 Loss:  2733.2265\n",
            "Epoch 619 -kfold 1-n_latent 3 Loss:  2749.5958\n",
            "Epoch 620 -kfold 1-n_latent 3 Loss:  2613.6280\n",
            "Epoch 621 -kfold 1-n_latent 3 Loss:  2599.0617\n",
            "Epoch 622 -kfold 1-n_latent 3 Loss:  2587.3691\n",
            "Epoch 623 -kfold 1-n_latent 3 Loss:  2596.8537\n",
            "Epoch 624 -kfold 1-n_latent 3 Loss:  2470.5936\n",
            "Epoch 625 -kfold 1-n_latent 3 Loss:  2365.8596\n",
            "Epoch 626 -kfold 1-n_latent 3 Loss:  2283.1894\n",
            "Epoch 627 -kfold 1-n_latent 3 Loss:  2136.1716\n",
            "Epoch 628 -kfold 1-n_latent 3 Loss:  1987.8380\n",
            "Epoch 629 -kfold 1-n_latent 3 Loss:  1856.8085\n",
            "Epoch 630 -kfold 1-n_latent 3 Loss:  1773.1436\n",
            "Epoch 631 -kfold 1-n_latent 3 Loss:  1667.0880\n",
            "Epoch 632 -kfold 1-n_latent 3 Loss:  1568.3754\n",
            "Epoch 633 -kfold 1-n_latent 3 Loss:  1405.1315\n",
            "Epoch 634 -kfold 1-n_latent 3 Loss:  1292.8620\n",
            "Epoch 635 -kfold 1-n_latent 3 Loss:  1216.0689\n",
            "Epoch 636 -kfold 1-n_latent 3 Loss:  1221.7755\n",
            "Epoch 637 -kfold 1-n_latent 3 Loss:  1165.4423\n",
            "Epoch 638 -kfold 1-n_latent 3 Loss:  1167.7033\n",
            "Epoch 639 -kfold 1-n_latent 3 Loss:  1076.1833\n",
            "Epoch 640 -kfold 1-n_latent 3 Loss:  1091.3925\n",
            "Epoch 641 -kfold 1-n_latent 3 Loss:  1031.5535\n",
            "Epoch 642 -kfold 1-n_latent 3 Loss:  926.9025\n",
            "Epoch 643 -kfold 1-n_latent 3 Loss:  955.7409\n",
            "Epoch 644 -kfold 1-n_latent 3 Loss:  879.6284\n",
            "Epoch 645 -kfold 1-n_latent 3 Loss:  790.5835\n",
            "Epoch 646 -kfold 1-n_latent 3 Loss:  688.5420\n",
            "Epoch 647 -kfold 1-n_latent 3 Loss:  655.3698\n",
            "Epoch 648 -kfold 1-n_latent 3 Loss:  703.5502\n",
            "Epoch 649 -kfold 1-n_latent 3 Loss:  661.4242\n",
            "Epoch 650 -kfold 1-n_latent 3 Loss:  681.8163\n",
            "Epoch 651 -kfold 1-n_latent 3 Loss:  803.5539\n",
            "Epoch 652 -kfold 1-n_latent 3 Loss:  978.3136\n",
            "Epoch 653 -kfold 1-n_latent 3 Loss:  1201.7358\n",
            "Epoch 654 -kfold 1-n_latent 3 Loss:  1390.4838\n",
            "Epoch 655 -kfold 1-n_latent 3 Loss:  1575.8827\n",
            "Epoch 656 -kfold 1-n_latent 3 Loss:  1714.7514\n",
            "Epoch 657 -kfold 1-n_latent 3 Loss:  1744.4980\n",
            "Epoch 658 -kfold 1-n_latent 3 Loss:  1866.2416\n",
            "Epoch 659 -kfold 1-n_latent 3 Loss:  1900.0776\n",
            "Epoch 660 -kfold 1-n_latent 3 Loss:  1915.2491\n",
            "Epoch 661 -kfold 1-n_latent 3 Loss:  2030.9965\n",
            "Epoch 662 -kfold 1-n_latent 3 Loss:  2006.3640\n",
            "Epoch 663 -kfold 1-n_latent 3 Loss:  2090.6240\n",
            "Epoch 664 -kfold 1-n_latent 3 Loss:  2059.4049\n",
            "Epoch 665 -kfold 1-n_latent 3 Loss:  2005.7168\n",
            "Epoch 666 -kfold 1-n_latent 3 Loss:  1915.2471\n",
            "Epoch 667 -kfold 1-n_latent 3 Loss:  1880.4176\n",
            "Epoch 668 -kfold 1-n_latent 3 Loss:  1748.7618\n",
            "Epoch 669 -kfold 1-n_latent 3 Loss:  1604.2380\n",
            "Epoch 670 -kfold 1-n_latent 3 Loss:  1447.7936\n",
            "Epoch 671 -kfold 1-n_latent 3 Loss:  1355.8540\n",
            "Epoch 672 -kfold 1-n_latent 3 Loss:  1382.9301\n",
            "Epoch 673 -kfold 1-n_latent 3 Loss:  1396.6211\n",
            "Epoch 674 -kfold 1-n_latent 3 Loss:  1356.1573\n",
            "Epoch 675 -kfold 1-n_latent 3 Loss:  1292.7238\n",
            "Epoch 676 -kfold 1-n_latent 3 Loss:  1245.8036\n",
            "Epoch 677 -kfold 1-n_latent 3 Loss:  1244.0538\n",
            "Epoch 678 -kfold 1-n_latent 3 Loss:  1201.7013\n",
            "Epoch 679 -kfold 1-n_latent 3 Loss:  1299.6009\n",
            "Epoch 680 -kfold 1-n_latent 3 Loss:  1090.0337\n",
            "Epoch 681 -kfold 1-n_latent 3 Loss:  1131.2694\n",
            "Epoch 682 -kfold 1-n_latent 3 Loss:  1189.9594\n",
            "Epoch 683 -kfold 1-n_latent 3 Loss:  1123.0826\n",
            "Epoch 684 -kfold 1-n_latent 3 Loss:  1193.4142\n",
            "Epoch 685 -kfold 1-n_latent 3 Loss:  1297.7462\n",
            "Epoch 686 -kfold 1-n_latent 3 Loss:  1472.6585\n",
            "Epoch 687 -kfold 1-n_latent 3 Loss:  1766.0084\n",
            "Epoch 688 -kfold 1-n_latent 3 Loss:  2055.9918\n",
            "Epoch 689 -kfold 1-n_latent 3 Loss:  2022.0534\n",
            "Epoch 690 -kfold 1-n_latent 3 Loss:  2151.3369\n",
            "Epoch 691 -kfold 1-n_latent 3 Loss:  2224.1305\n",
            "Epoch 692 -kfold 1-n_latent 3 Loss:  2212.5652\n",
            "Epoch 693 -kfold 1-n_latent 3 Loss:  2077.4161\n",
            "Epoch 694 -kfold 1-n_latent 3 Loss:  2006.0881\n",
            "Epoch 695 -kfold 1-n_latent 3 Loss:  1878.2254\n",
            "Epoch 696 -kfold 1-n_latent 3 Loss:  1709.1015\n",
            "Epoch 697 -kfold 1-n_latent 3 Loss:  1432.3828\n",
            "Epoch 698 -kfold 1-n_latent 3 Loss:  1259.8002\n",
            "Epoch 699 -kfold 1-n_latent 3 Loss:  1042.0882\n",
            "4\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 1-n_latent 4 Loss:  22949.1693\n",
            "Epoch 2 -kfold 1-n_latent 4 Loss:  21360.1858\n",
            "Epoch 3 -kfold 1-n_latent 4 Loss:  20039.3688\n",
            "Epoch 4 -kfold 1-n_latent 4 Loss:  18742.3903\n",
            "Epoch 5 -kfold 1-n_latent 4 Loss:  17446.1405\n",
            "Epoch 6 -kfold 1-n_latent 4 Loss:  16212.5651\n",
            "Epoch 7 -kfold 1-n_latent 4 Loss:  15032.9380\n",
            "Epoch 8 -kfold 1-n_latent 4 Loss:  13873.6870\n",
            "Epoch 9 -kfold 1-n_latent 4 Loss:  12752.5081\n",
            "Epoch 10 -kfold 1-n_latent 4 Loss:  11730.8782\n",
            "Epoch 11 -kfold 1-n_latent 4 Loss:  10863.7997\n",
            "Epoch 12 -kfold 1-n_latent 4 Loss:  10161.6780\n",
            "Epoch 13 -kfold 1-n_latent 4 Loss:  9594.4711\n",
            "Epoch 14 -kfold 1-n_latent 4 Loss:  9181.0674\n",
            "Epoch 15 -kfold 1-n_latent 4 Loss:  8833.2141\n",
            "Epoch 16 -kfold 1-n_latent 4 Loss:  8444.3329\n",
            "Epoch 17 -kfold 1-n_latent 4 Loss:  8075.6656\n",
            "Epoch 18 -kfold 1-n_latent 4 Loss:  7671.8842\n",
            "Epoch 19 -kfold 1-n_latent 4 Loss:  7318.7297\n",
            "Epoch 20 -kfold 1-n_latent 4 Loss:  6959.0811\n",
            "Epoch 21 -kfold 1-n_latent 4 Loss:  6564.3662\n",
            "Epoch 22 -kfold 1-n_latent 4 Loss:  6282.4474\n",
            "Epoch 23 -kfold 1-n_latent 4 Loss:  5959.7948\n",
            "Epoch 24 -kfold 1-n_latent 4 Loss:  5686.3500\n",
            "Epoch 25 -kfold 1-n_latent 4 Loss:  5340.9423\n",
            "Epoch 26 -kfold 1-n_latent 4 Loss:  5061.8418\n",
            "Epoch 27 -kfold 1-n_latent 4 Loss:  4795.8403\n",
            "Epoch 28 -kfold 1-n_latent 4 Loss:  4586.7165\n",
            "Epoch 29 -kfold 1-n_latent 4 Loss:  4336.0337\n",
            "Epoch 30 -kfold 1-n_latent 4 Loss:  4073.9019\n",
            "Epoch 31 -kfold 1-n_latent 4 Loss:  3847.2947\n",
            "Epoch 32 -kfold 1-n_latent 4 Loss:  3627.3860\n",
            "Epoch 33 -kfold 1-n_latent 4 Loss:  3431.6643\n",
            "Epoch 34 -kfold 1-n_latent 4 Loss:  3211.3465\n",
            "Epoch 35 -kfold 1-n_latent 4 Loss:  2996.7852\n",
            "Epoch 36 -kfold 1-n_latent 4 Loss:  2782.5988\n",
            "Epoch 37 -kfold 1-n_latent 4 Loss:  2536.4180\n",
            "Epoch 38 -kfold 1-n_latent 4 Loss:  2318.9104\n",
            "Epoch 39 -kfold 1-n_latent 4 Loss:  2198.9152\n",
            "Epoch 40 -kfold 1-n_latent 4 Loss:  2018.3088\n",
            "Epoch 41 -kfold 1-n_latent 4 Loss:  1812.8605\n",
            "Epoch 42 -kfold 1-n_latent 4 Loss:  1688.2121\n",
            "Epoch 43 -kfold 1-n_latent 4 Loss:  1359.7483\n",
            "Epoch 44 -kfold 1-n_latent 4 Loss:  1214.7311\n",
            "Epoch 45 -kfold 1-n_latent 4 Loss:  922.5446\n",
            "Epoch 46 -kfold 1-n_latent 4 Loss:  807.7282\n",
            "Epoch 47 -kfold 1-n_latent 4 Loss:  668.8996\n",
            "Epoch 48 -kfold 1-n_latent 4 Loss:  409.6551\n",
            "Epoch 49 -kfold 1-n_latent 4 Loss:  236.7943\n",
            "Epoch 50 -kfold 1-n_latent 4 Loss:  31.9040\n",
            "Epoch 51 -kfold 1-n_latent 4 Loss: -217.9757\n",
            "Epoch 52 -kfold 1-n_latent 4 Loss: -422.3610\n",
            "Epoch 53 -kfold 1-n_latent 4 Loss: -502.8303\n",
            "Epoch 54 -kfold 1-n_latent 4 Loss: -661.2698\n",
            "Epoch 55 -kfold 1-n_latent 4 Loss: -855.7884\n",
            "Epoch 56 -kfold 1-n_latent 4 Loss: -1119.3149\n",
            "Epoch 57 -kfold 1-n_latent 4 Loss: -1247.0017\n",
            "Epoch 58 -kfold 1-n_latent 4 Loss: -1444.1407\n",
            "Epoch 59 -kfold 1-n_latent 4 Loss: -1562.3858\n",
            "Epoch 60 -kfold 1-n_latent 4 Loss: -1808.2658\n",
            "Epoch 61 -kfold 1-n_latent 4 Loss: -1895.0145\n",
            "Epoch 62 -kfold 1-n_latent 4 Loss: -1900.6234\n",
            "Epoch 63 -kfold 1-n_latent 4 Loss: -2019.8011\n",
            "Epoch 64 -kfold 1-n_latent 4 Loss: -2220.9789\n",
            "Epoch 65 -kfold 1-n_latent 4 Loss: -2402.9425\n",
            "Epoch 66 -kfold 1-n_latent 4 Loss: -2505.7807\n",
            "Epoch 67 -kfold 1-n_latent 4 Loss: -2687.3754\n",
            "Epoch 68 -kfold 1-n_latent 4 Loss: -2724.3609\n",
            "Epoch 69 -kfold 1-n_latent 4 Loss: -2842.2198\n",
            "Epoch 70 -kfold 1-n_latent 4 Loss: -2768.7049\n",
            "Epoch 71 -kfold 1-n_latent 4 Loss: -2865.5473\n",
            "Epoch 72 -kfold 1-n_latent 4 Loss: -2850.8165\n",
            "Epoch 73 -kfold 1-n_latent 4 Loss: -2695.3539\n",
            "Epoch 74 -kfold 1-n_latent 4 Loss: -2839.5339\n",
            "Epoch 75 -kfold 1-n_latent 4 Loss: -3053.3654\n",
            "Epoch 76 -kfold 1-n_latent 4 Loss: -3206.0238\n",
            "Epoch 77 -kfold 1-n_latent 4 Loss: -3282.1110\n",
            "Epoch 78 -kfold 1-n_latent 4 Loss: -3420.1363\n",
            "Epoch 79 -kfold 1-n_latent 4 Loss: -3708.5770\n",
            "Epoch 80 -kfold 1-n_latent 4 Loss: -3763.2142\n",
            "Epoch 81 -kfold 1-n_latent 4 Loss: -3866.6423\n",
            "Epoch 82 -kfold 1-n_latent 4 Loss: -4029.1220\n",
            "Epoch 83 -kfold 1-n_latent 4 Loss: -4169.8344\n",
            "Epoch 84 -kfold 1-n_latent 4 Loss: -4217.1746\n",
            "Epoch 85 -kfold 1-n_latent 4 Loss: -4232.8449\n",
            "Epoch 86 -kfold 1-n_latent 4 Loss: -4169.3085\n",
            "Epoch 87 -kfold 1-n_latent 4 Loss: -4013.6128\n",
            "Epoch 88 -kfold 1-n_latent 4 Loss: -4005.2247\n",
            "Epoch 89 -kfold 1-n_latent 4 Loss: -3939.5756\n",
            "Epoch 90 -kfold 1-n_latent 4 Loss: -3958.3089\n",
            "Epoch 91 -kfold 1-n_latent 4 Loss: -4209.5476\n",
            "Epoch 92 -kfold 1-n_latent 4 Loss: -4182.2039\n",
            "Epoch 93 -kfold 1-n_latent 4 Loss: -4190.9038\n",
            "Epoch 94 -kfold 1-n_latent 4 Loss: -4106.1711\n",
            "Epoch 95 -kfold 1-n_latent 4 Loss: -4208.2403\n",
            "Epoch 96 -kfold 1-n_latent 4 Loss: -4159.2856\n",
            "Epoch 97 -kfold 1-n_latent 4 Loss: -4089.2238\n",
            "Epoch 98 -kfold 1-n_latent 4 Loss: -4050.3018\n",
            "Epoch 99 -kfold 1-n_latent 4 Loss: -4069.8810\n",
            "Epoch 100 -kfold 1-n_latent 4 Loss: -4275.7342\n",
            "Epoch 101 -kfold 1-n_latent 4 Loss: -4466.9394\n",
            "Epoch 102 -kfold 1-n_latent 4 Loss: -4622.6435\n",
            "Epoch 103 -kfold 1-n_latent 4 Loss: -4693.5862\n",
            "Epoch 104 -kfold 1-n_latent 4 Loss: -4574.8354\n",
            "Epoch 105 -kfold 1-n_latent 4 Loss: -4559.1339\n",
            "Epoch 106 -kfold 1-n_latent 4 Loss: -4414.3135\n",
            "Epoch 107 -kfold 1-n_latent 4 Loss: -4261.6398\n",
            "Epoch 108 -kfold 1-n_latent 4 Loss: -4252.5772\n",
            "Epoch 109 -kfold 1-n_latent 4 Loss: -4316.6714\n",
            "Epoch 110 -kfold 1-n_latent 4 Loss: -4558.5108\n",
            "Epoch 111 -kfold 1-n_latent 4 Loss: -4802.3214\n",
            "Epoch 112 -kfold 1-n_latent 4 Loss: -4879.8311\n",
            "Epoch 113 -kfold 1-n_latent 4 Loss: -4821.8560\n",
            "Epoch 114 -kfold 1-n_latent 4 Loss: -4763.0605\n",
            "Epoch 115 -kfold 1-n_latent 4 Loss: -4715.7832\n",
            "Epoch 116 -kfold 1-n_latent 4 Loss: -4590.0400\n",
            "Epoch 117 -kfold 1-n_latent 4 Loss: -4519.7434\n",
            "Epoch 118 -kfold 1-n_latent 4 Loss: -4488.4035\n",
            "Epoch 119 -kfold 1-n_latent 4 Loss: -4673.2105\n",
            "Epoch 120 -kfold 1-n_latent 4 Loss: -4695.4750\n",
            "Epoch 121 -kfold 1-n_latent 4 Loss: -4890.9853\n",
            "Epoch 122 -kfold 1-n_latent 4 Loss: -4909.8376\n",
            "Epoch 123 -kfold 1-n_latent 4 Loss: -5232.6390\n",
            "Epoch 124 -kfold 1-n_latent 4 Loss: -5301.0314\n",
            "Epoch 125 -kfold 1-n_latent 4 Loss: -5289.8991\n",
            "Epoch 126 -kfold 1-n_latent 4 Loss: -5365.5338\n",
            "Epoch 127 -kfold 1-n_latent 4 Loss: -5345.4545\n",
            "Epoch 128 -kfold 1-n_latent 4 Loss: -5367.2523\n",
            "Epoch 129 -kfold 1-n_latent 4 Loss: -5507.4924\n",
            "Epoch 130 -kfold 1-n_latent 4 Loss: -5491.7523\n",
            "Epoch 131 -kfold 1-n_latent 4 Loss: -5720.8775\n",
            "Epoch 132 -kfold 1-n_latent 4 Loss: -5696.0961\n",
            "Epoch 133 -kfold 1-n_latent 4 Loss: -5736.7622\n",
            "Epoch 134 -kfold 1-n_latent 4 Loss: -5740.3047\n",
            "Epoch 135 -kfold 1-n_latent 4 Loss: -5718.5498\n",
            "Epoch 136 -kfold 1-n_latent 4 Loss: -5739.4095\n",
            "Epoch 137 -kfold 1-n_latent 4 Loss: -5811.0231\n",
            "Epoch 138 -kfold 1-n_latent 4 Loss: -5897.5450\n",
            "Epoch 139 -kfold 1-n_latent 4 Loss: -5872.1661\n",
            "Epoch 140 -kfold 1-n_latent 4 Loss: -5835.7161\n",
            "Epoch 141 -kfold 1-n_latent 4 Loss: -5876.1042\n",
            "Epoch 142 -kfold 1-n_latent 4 Loss: -5691.4123\n",
            "Epoch 143 -kfold 1-n_latent 4 Loss: -5550.0983\n",
            "Epoch 144 -kfold 1-n_latent 4 Loss: -5247.3690\n",
            "Epoch 145 -kfold 1-n_latent 4 Loss: -5046.8642\n",
            "Epoch 146 -kfold 1-n_latent 4 Loss: -5138.0893\n",
            "Epoch 147 -kfold 1-n_latent 4 Loss: -5062.5543\n",
            "Epoch 148 -kfold 1-n_latent 4 Loss: -5142.6843\n",
            "Epoch 149 -kfold 1-n_latent 4 Loss: -5289.5589\n",
            "Epoch 150 -kfold 1-n_latent 4 Loss: -5512.9855\n",
            "Epoch 151 -kfold 1-n_latent 4 Loss: -5743.8285\n",
            "Epoch 152 -kfold 1-n_latent 4 Loss: -5901.1385\n",
            "Epoch 153 -kfold 1-n_latent 4 Loss: -5852.9389\n",
            "Epoch 154 -kfold 1-n_latent 4 Loss: -5884.3343\n",
            "Epoch 155 -kfold 1-n_latent 4 Loss: -5898.2789\n",
            "Epoch 156 -kfold 1-n_latent 4 Loss: -5882.8765\n",
            "Epoch 157 -kfold 1-n_latent 4 Loss: -5820.2712\n",
            "Epoch 158 -kfold 1-n_latent 4 Loss: -5936.3446\n",
            "Epoch 159 -kfold 1-n_latent 4 Loss: -6027.2000\n",
            "Epoch 160 -kfold 1-n_latent 4 Loss: -6068.4830\n",
            "Epoch 161 -kfold 1-n_latent 4 Loss: -6349.6245\n",
            "Epoch 162 -kfold 1-n_latent 4 Loss: -6388.2370\n",
            "Epoch 163 -kfold 1-n_latent 4 Loss: -6398.2710\n",
            "Epoch 164 -kfold 1-n_latent 4 Loss: -5836.5943\n",
            "Epoch 165 -kfold 1-n_latent 4 Loss: -5555.7734\n",
            "Epoch 166 -kfold 1-n_latent 4 Loss: -5464.2280\n",
            "Epoch 167 -kfold 1-n_latent 4 Loss: -5543.5993\n",
            "Epoch 168 -kfold 1-n_latent 4 Loss: -5664.3243\n",
            "Epoch 169 -kfold 1-n_latent 4 Loss: -5846.0409\n",
            "Epoch 170 -kfold 1-n_latent 4 Loss: -5818.4259\n",
            "Epoch 171 -kfold 1-n_latent 4 Loss: -6031.1774\n",
            "Epoch 172 -kfold 1-n_latent 4 Loss: -6144.4173\n",
            "Epoch 173 -kfold 1-n_latent 4 Loss: -6280.9023\n",
            "Epoch 174 -kfold 1-n_latent 4 Loss: -6198.8112\n",
            "Epoch 175 -kfold 1-n_latent 4 Loss: -6337.8811\n",
            "Epoch 176 -kfold 1-n_latent 4 Loss: -6034.2959\n",
            "Epoch 177 -kfold 1-n_latent 4 Loss: -5840.0332\n",
            "Epoch 178 -kfold 1-n_latent 4 Loss: -5992.4282\n",
            "Epoch 179 -kfold 1-n_latent 4 Loss: -5630.1012\n",
            "Epoch 180 -kfold 1-n_latent 4 Loss: -5522.7635\n",
            "Epoch 181 -kfold 1-n_latent 4 Loss: -5797.8505\n",
            "Epoch 182 -kfold 1-n_latent 4 Loss: -5791.5349\n",
            "Epoch 183 -kfold 1-n_latent 4 Loss: -5916.1319\n",
            "Epoch 184 -kfold 1-n_latent 4 Loss: -6118.8997\n",
            "Epoch 185 -kfold 1-n_latent 4 Loss: -6210.0037\n",
            "Epoch 186 -kfold 1-n_latent 4 Loss: -6353.0217\n",
            "Epoch 187 -kfold 1-n_latent 4 Loss: -6249.9996\n",
            "Epoch 188 -kfold 1-n_latent 4 Loss: -6248.6168\n",
            "Epoch 189 -kfold 1-n_latent 4 Loss: -6255.3246\n",
            "Epoch 190 -kfold 1-n_latent 4 Loss: -6324.2161\n",
            "Epoch 191 -kfold 1-n_latent 4 Loss: -6321.0216\n",
            "Epoch 192 -kfold 1-n_latent 4 Loss: -6460.2932\n",
            "Epoch 193 -kfold 1-n_latent 4 Loss: -6551.4129\n",
            "Epoch 194 -kfold 1-n_latent 4 Loss: -6534.3545\n",
            "Epoch 195 -kfold 1-n_latent 4 Loss: -6587.0347\n",
            "Epoch 196 -kfold 1-n_latent 4 Loss: -6598.5385\n",
            "Epoch 197 -kfold 1-n_latent 4 Loss: -6752.6773\n",
            "Epoch 198 -kfold 1-n_latent 4 Loss: -6490.1934\n",
            "Epoch 199 -kfold 1-n_latent 4 Loss: -6610.0746\n",
            "Epoch 200 -kfold 1-n_latent 4 Loss: -6331.1206\n",
            "Epoch 201 -kfold 1-n_latent 4 Loss: -6309.3816\n",
            "Epoch 202 -kfold 1-n_latent 4 Loss: -6399.4275\n",
            "Epoch 203 -kfold 1-n_latent 4 Loss: -6481.6180\n",
            "Epoch 204 -kfold 1-n_latent 4 Loss: -6527.5079\n",
            "Epoch 205 -kfold 1-n_latent 4 Loss: -6556.9429\n",
            "Epoch 206 -kfold 1-n_latent 4 Loss: -6518.3781\n",
            "Epoch 207 -kfold 1-n_latent 4 Loss: -6500.7535\n",
            "Epoch 208 -kfold 1-n_latent 4 Loss: -6106.3069\n",
            "Epoch 209 -kfold 1-n_latent 4 Loss: -5968.0300\n",
            "Epoch 210 -kfold 1-n_latent 4 Loss: -6124.2783\n",
            "Epoch 211 -kfold 1-n_latent 4 Loss: -6173.2916\n",
            "Epoch 212 -kfold 1-n_latent 4 Loss: -6182.5538\n",
            "Epoch 213 -kfold 1-n_latent 4 Loss: -6299.1029\n",
            "Epoch 214 -kfold 1-n_latent 4 Loss: -6372.5504\n",
            "Epoch 215 -kfold 1-n_latent 4 Loss: -6608.0497\n",
            "Epoch 216 -kfold 1-n_latent 4 Loss: -6708.7895\n",
            "Epoch 217 -kfold 1-n_latent 4 Loss: -6679.9505\n",
            "Epoch 218 -kfold 1-n_latent 4 Loss: -6265.6961\n",
            "Epoch 219 -kfold 1-n_latent 4 Loss: -6453.6474\n",
            "Epoch 220 -kfold 1-n_latent 4 Loss: -6554.4718\n",
            "Epoch 221 -kfold 1-n_latent 4 Loss: -6437.9871\n",
            "Epoch 222 -kfold 1-n_latent 4 Loss: -6344.7016\n",
            "Epoch 223 -kfold 1-n_latent 4 Loss: -6280.0376\n",
            "Epoch 224 -kfold 1-n_latent 4 Loss: -6552.4867\n",
            "Epoch 225 -kfold 1-n_latent 4 Loss: -6714.1415\n",
            "Epoch 226 -kfold 1-n_latent 4 Loss: -6739.6559\n",
            "Epoch 227 -kfold 1-n_latent 4 Loss: -6925.0010\n",
            "Epoch 228 -kfold 1-n_latent 4 Loss: -6857.2544\n",
            "Epoch 229 -kfold 1-n_latent 4 Loss: -6839.9975\n",
            "Epoch 230 -kfold 1-n_latent 4 Loss: -6526.8841\n",
            "Epoch 231 -kfold 1-n_latent 4 Loss: -6329.0324\n",
            "Epoch 232 -kfold 1-n_latent 4 Loss: -6505.2913\n",
            "Epoch 233 -kfold 1-n_latent 4 Loss: -6379.4463\n",
            "Epoch 234 -kfold 1-n_latent 4 Loss: -6335.3796\n",
            "Epoch 235 -kfold 1-n_latent 4 Loss: -6560.4587\n",
            "Epoch 236 -kfold 1-n_latent 4 Loss: -6767.8184\n",
            "Epoch 237 -kfold 1-n_latent 4 Loss: -6835.5894\n",
            "Epoch 238 -kfold 1-n_latent 4 Loss: -7018.6070\n",
            "Epoch 239 -kfold 1-n_latent 4 Loss: -7082.7160\n",
            "Epoch 240 -kfold 1-n_latent 4 Loss: -6925.9680\n",
            "Epoch 241 -kfold 1-n_latent 4 Loss: -6258.7058\n",
            "Epoch 242 -kfold 1-n_latent 4 Loss: -5698.8640\n",
            "Epoch 243 -kfold 1-n_latent 4 Loss: -5490.9239\n",
            "Epoch 244 -kfold 1-n_latent 4 Loss: -5376.4290\n",
            "Epoch 245 -kfold 1-n_latent 4 Loss: -5327.5333\n",
            "Epoch 246 -kfold 1-n_latent 4 Loss: -5203.0241\n",
            "Epoch 247 -kfold 1-n_latent 4 Loss: -5463.0247\n",
            "Epoch 248 -kfold 1-n_latent 4 Loss: -5646.4940\n",
            "Epoch 249 -kfold 1-n_latent 4 Loss: -5867.7244\n",
            "Epoch 250 -kfold 1-n_latent 4 Loss: -6173.8625\n",
            "Epoch 251 -kfold 1-n_latent 4 Loss: -6565.6827\n",
            "Epoch 252 -kfold 1-n_latent 4 Loss: -6763.1752\n",
            "Epoch 253 -kfold 1-n_latent 4 Loss: -6612.4790\n",
            "Epoch 254 -kfold 1-n_latent 4 Loss: -6300.7902\n",
            "Epoch 255 -kfold 1-n_latent 4 Loss: -5874.4784\n",
            "Epoch 256 -kfold 1-n_latent 4 Loss: -5572.6819\n",
            "Epoch 257 -kfold 1-n_latent 4 Loss: -5670.9105\n",
            "Epoch 258 -kfold 1-n_latent 4 Loss: -5860.2989\n",
            "Epoch 259 -kfold 1-n_latent 4 Loss: -5864.4624\n",
            "Epoch 260 -kfold 1-n_latent 4 Loss: -5786.5864\n",
            "Epoch 261 -kfold 1-n_latent 4 Loss: -6040.4100\n",
            "Epoch 262 -kfold 1-n_latent 4 Loss: -6286.3510\n",
            "Epoch 263 -kfold 1-n_latent 4 Loss: -6509.6683\n",
            "Epoch 264 -kfold 1-n_latent 4 Loss: -6895.1237\n",
            "Epoch 265 -kfold 1-n_latent 4 Loss: -7095.0329\n",
            "Epoch 266 -kfold 1-n_latent 4 Loss: -7058.1828\n",
            "Epoch 267 -kfold 1-n_latent 4 Loss: -7004.6684\n",
            "Epoch 268 -kfold 1-n_latent 4 Loss: -6283.5357\n",
            "Epoch 269 -kfold 1-n_latent 4 Loss: -5567.5048\n",
            "Epoch 270 -kfold 1-n_latent 4 Loss: -5689.2059\n",
            "Epoch 271 -kfold 1-n_latent 4 Loss: -6076.0361\n",
            "Epoch 272 -kfold 1-n_latent 4 Loss: -6107.2969\n",
            "Epoch 273 -kfold 1-n_latent 4 Loss: -6072.5397\n",
            "Epoch 274 -kfold 1-n_latent 4 Loss: -6116.0333\n",
            "Epoch 275 -kfold 1-n_latent 4 Loss: -6282.5703\n",
            "Epoch 276 -kfold 1-n_latent 4 Loss: -6460.2940\n",
            "Epoch 277 -kfold 1-n_latent 4 Loss: -6665.8374\n",
            "Epoch 278 -kfold 1-n_latent 4 Loss: -6987.8412\n",
            "Epoch 279 -kfold 1-n_latent 4 Loss: -7114.8347\n",
            "Epoch 280 -kfold 1-n_latent 4 Loss: -7286.7937\n",
            "Epoch 281 -kfold 1-n_latent 4 Loss: -7098.2956\n",
            "Epoch 282 -kfold 1-n_latent 4 Loss: -6511.6836\n",
            "Epoch 283 -kfold 1-n_latent 4 Loss: -5952.2571\n",
            "Epoch 284 -kfold 1-n_latent 4 Loss: -5532.5988\n",
            "Epoch 285 -kfold 1-n_latent 4 Loss: -5264.7333\n",
            "Epoch 286 -kfold 1-n_latent 4 Loss: -5501.5708\n",
            "Epoch 287 -kfold 1-n_latent 4 Loss: -5714.3757\n",
            "Epoch 288 -kfold 1-n_latent 4 Loss: -5483.1737\n",
            "Epoch 289 -kfold 1-n_latent 4 Loss: -5661.6423\n",
            "Epoch 290 -kfold 1-n_latent 4 Loss: -5808.9920\n",
            "Epoch 291 -kfold 1-n_latent 4 Loss: -6058.0817\n",
            "Epoch 292 -kfold 1-n_latent 4 Loss: -6261.0488\n",
            "Epoch 293 -kfold 1-n_latent 4 Loss: -6653.3354\n",
            "Epoch 294 -kfold 1-n_latent 4 Loss: -6915.1408\n",
            "Epoch 295 -kfold 1-n_latent 4 Loss: -7057.6743\n",
            "Epoch 296 -kfold 1-n_latent 4 Loss: -6969.4919\n",
            "Epoch 297 -kfold 1-n_latent 4 Loss: -6595.4636\n",
            "Epoch 298 -kfold 1-n_latent 4 Loss: -6077.4780\n",
            "Epoch 299 -kfold 1-n_latent 4 Loss: -5905.5575\n",
            "Epoch 300 -kfold 1-n_latent 4 Loss: -6118.5294\n",
            "Epoch 301 -kfold 1-n_latent 4 Loss: -6262.3501\n",
            "Epoch 302 -kfold 1-n_latent 4 Loss: -6367.9617\n",
            "Epoch 303 -kfold 1-n_latent 4 Loss: -6513.6499\n",
            "Epoch 304 -kfold 1-n_latent 4 Loss: -6547.3761\n",
            "Epoch 305 -kfold 1-n_latent 4 Loss: -6682.5684\n",
            "Epoch 306 -kfold 1-n_latent 4 Loss: -6906.6123\n",
            "Epoch 307 -kfold 1-n_latent 4 Loss: -7006.0597\n",
            "Epoch 308 -kfold 1-n_latent 4 Loss: -7203.4770\n",
            "Epoch 309 -kfold 1-n_latent 4 Loss: -7389.7044\n",
            "Epoch 310 -kfold 1-n_latent 4 Loss: -7205.3459\n",
            "Epoch 311 -kfold 1-n_latent 4 Loss: -6679.1400\n",
            "Epoch 312 -kfold 1-n_latent 4 Loss: -6170.1070\n",
            "Epoch 313 -kfold 1-n_latent 4 Loss: -5629.6245\n",
            "Epoch 314 -kfold 1-n_latent 4 Loss: -5391.7163\n",
            "Epoch 315 -kfold 1-n_latent 4 Loss: -5393.6183\n",
            "Epoch 316 -kfold 1-n_latent 4 Loss: -5645.4509\n",
            "Epoch 317 -kfold 1-n_latent 4 Loss: -5657.9413\n",
            "Epoch 318 -kfold 1-n_latent 4 Loss: -5906.9527\n",
            "Epoch 319 -kfold 1-n_latent 4 Loss: -6097.1433\n",
            "Epoch 320 -kfold 1-n_latent 4 Loss: -6118.6782\n",
            "Epoch 321 -kfold 1-n_latent 4 Loss: -6371.5917\n",
            "Epoch 322 -kfold 1-n_latent 4 Loss: -6599.6470\n",
            "Epoch 323 -kfold 1-n_latent 4 Loss: -6854.8851\n",
            "Epoch 324 -kfold 1-n_latent 4 Loss: -7170.9187\n",
            "Epoch 325 -kfold 1-n_latent 4 Loss: -7382.4597\n",
            "Epoch 326 -kfold 1-n_latent 4 Loss: -7253.0717\n",
            "Epoch 327 -kfold 1-n_latent 4 Loss: -6952.6724\n",
            "Epoch 328 -kfold 1-n_latent 4 Loss: -6334.3788\n",
            "Epoch 329 -kfold 1-n_latent 4 Loss: -5708.3312\n",
            "Epoch 330 -kfold 1-n_latent 4 Loss: -5866.2560\n",
            "Epoch 331 -kfold 1-n_latent 4 Loss: -5936.3282\n",
            "Epoch 332 -kfold 1-n_latent 4 Loss: -6097.9936\n",
            "Epoch 333 -kfold 1-n_latent 4 Loss: -6239.1996\n",
            "Epoch 334 -kfold 1-n_latent 4 Loss: -6174.7363\n",
            "Epoch 335 -kfold 1-n_latent 4 Loss: -6268.5302\n",
            "Epoch 336 -kfold 1-n_latent 4 Loss: -6168.0552\n",
            "Epoch 337 -kfold 1-n_latent 4 Loss: -6382.5401\n",
            "Epoch 338 -kfold 1-n_latent 4 Loss: -6605.2237\n",
            "Epoch 339 -kfold 1-n_latent 4 Loss: -6937.8253\n",
            "Epoch 340 -kfold 1-n_latent 4 Loss: -7030.2554\n",
            "Epoch 341 -kfold 1-n_latent 4 Loss: -7079.7599\n",
            "Epoch 342 -kfold 1-n_latent 4 Loss: -7079.5924\n",
            "Epoch 343 -kfold 1-n_latent 4 Loss: -6862.0689\n",
            "Epoch 344 -kfold 1-n_latent 4 Loss: -6602.9240\n",
            "Epoch 345 -kfold 1-n_latent 4 Loss: -6671.2793\n",
            "Epoch 346 -kfold 1-n_latent 4 Loss: -6726.3015\n",
            "Epoch 347 -kfold 1-n_latent 4 Loss: -6843.3997\n",
            "Epoch 348 -kfold 1-n_latent 4 Loss: -6974.1019\n",
            "Epoch 349 -kfold 1-n_latent 4 Loss: -6954.6328\n",
            "Epoch 350 -kfold 1-n_latent 4 Loss: -7028.3679\n",
            "Epoch 351 -kfold 1-n_latent 4 Loss: -7155.2925\n",
            "Epoch 352 -kfold 1-n_latent 4 Loss: -7229.0019\n",
            "Epoch 353 -kfold 1-n_latent 4 Loss: -7469.6553\n",
            "Epoch 354 -kfold 1-n_latent 4 Loss: -7631.4730\n",
            "Epoch 355 -kfold 1-n_latent 4 Loss: -7563.9924\n",
            "Epoch 356 -kfold 1-n_latent 4 Loss: -7465.0158\n",
            "Epoch 357 -kfold 1-n_latent 4 Loss: -7364.0859\n",
            "Epoch 358 -kfold 1-n_latent 4 Loss: -7313.7165\n",
            "Epoch 359 -kfold 1-n_latent 4 Loss: -7040.4475\n",
            "Epoch 360 -kfold 1-n_latent 4 Loss: -7045.3275\n",
            "Epoch 361 -kfold 1-n_latent 4 Loss: -6931.2907\n",
            "Epoch 362 -kfold 1-n_latent 4 Loss: -6920.0866\n",
            "Epoch 363 -kfold 1-n_latent 4 Loss: -6899.6194\n",
            "Epoch 364 -kfold 1-n_latent 4 Loss: -6851.0701\n",
            "Epoch 365 -kfold 1-n_latent 4 Loss: -6936.5567\n",
            "Epoch 366 -kfold 1-n_latent 4 Loss: -7019.5662\n",
            "Epoch 367 -kfold 1-n_latent 4 Loss: -7230.1708\n",
            "Epoch 368 -kfold 1-n_latent 4 Loss: -7399.6293\n",
            "Epoch 369 -kfold 1-n_latent 4 Loss: -7493.5627\n",
            "Epoch 370 -kfold 1-n_latent 4 Loss: -7545.5637\n",
            "Epoch 371 -kfold 1-n_latent 4 Loss: -7408.2801\n",
            "Epoch 372 -kfold 1-n_latent 4 Loss: -7373.6958\n",
            "Epoch 373 -kfold 1-n_latent 4 Loss: -7154.2350\n",
            "Epoch 374 -kfold 1-n_latent 4 Loss: -7014.4466\n",
            "Epoch 375 -kfold 1-n_latent 4 Loss: -6805.0637\n",
            "Epoch 376 -kfold 1-n_latent 4 Loss: -6475.1214\n",
            "Epoch 377 -kfold 1-n_latent 4 Loss: -6851.2281\n",
            "Epoch 378 -kfold 1-n_latent 4 Loss: -6866.9201\n",
            "Epoch 379 -kfold 1-n_latent 4 Loss: -6909.6799\n",
            "Epoch 380 -kfold 1-n_latent 4 Loss: -6896.9810\n",
            "Epoch 381 -kfold 1-n_latent 4 Loss: -6873.7886\n",
            "Epoch 382 -kfold 1-n_latent 4 Loss: -7013.4980\n",
            "Epoch 383 -kfold 1-n_latent 4 Loss: -7197.2855\n",
            "Epoch 384 -kfold 1-n_latent 4 Loss: -7407.4200\n",
            "Epoch 385 -kfold 1-n_latent 4 Loss: -7426.4894\n",
            "Epoch 386 -kfold 1-n_latent 4 Loss: -7445.7103\n",
            "Epoch 387 -kfold 1-n_latent 4 Loss: -7436.5544\n",
            "Epoch 388 -kfold 1-n_latent 4 Loss: -7164.7307\n",
            "Epoch 389 -kfold 1-n_latent 4 Loss: -6856.4942\n",
            "Epoch 390 -kfold 1-n_latent 4 Loss: -6847.8481\n",
            "Epoch 391 -kfold 1-n_latent 4 Loss: -6659.5328\n",
            "Epoch 392 -kfold 1-n_latent 4 Loss: -6502.9101\n",
            "Epoch 393 -kfold 1-n_latent 4 Loss: -6545.0907\n",
            "Epoch 394 -kfold 1-n_latent 4 Loss: -6616.0907\n",
            "Epoch 395 -kfold 1-n_latent 4 Loss: -6413.1207\n",
            "Epoch 396 -kfold 1-n_latent 4 Loss: -6587.0176\n",
            "Epoch 397 -kfold 1-n_latent 4 Loss: -6781.9439\n",
            "Epoch 398 -kfold 1-n_latent 4 Loss: -6910.3100\n",
            "Epoch 399 -kfold 1-n_latent 4 Loss: -7072.3275\n",
            "Epoch 400 -kfold 1-n_latent 4 Loss: -7177.6214\n",
            "Epoch 401 -kfold 1-n_latent 4 Loss: -7359.7943\n",
            "Epoch 402 -kfold 1-n_latent 4 Loss: -7272.0373\n",
            "Epoch 403 -kfold 1-n_latent 4 Loss: -7184.2522\n",
            "Epoch 404 -kfold 1-n_latent 4 Loss: -7057.8444\n",
            "Epoch 405 -kfold 1-n_latent 4 Loss: -6666.2749\n",
            "Epoch 406 -kfold 1-n_latent 4 Loss: -6192.2596\n",
            "Epoch 407 -kfold 1-n_latent 4 Loss: -6529.6201\n",
            "Epoch 408 -kfold 1-n_latent 4 Loss: -6627.2510\n",
            "Epoch 409 -kfold 1-n_latent 4 Loss: -7001.1647\n",
            "Epoch 410 -kfold 1-n_latent 4 Loss: -7064.7835\n",
            "Epoch 411 -kfold 1-n_latent 4 Loss: -7036.9728\n",
            "Epoch 412 -kfold 1-n_latent 4 Loss: -6646.5499\n",
            "Epoch 413 -kfold 1-n_latent 4 Loss: -6477.6459\n",
            "Epoch 414 -kfold 1-n_latent 4 Loss: -6962.0899\n",
            "Epoch 415 -kfold 1-n_latent 4 Loss: -7285.4497\n",
            "Epoch 416 -kfold 1-n_latent 4 Loss: -7457.8697\n",
            "Epoch 417 -kfold 1-n_latent 4 Loss: -7316.3981\n",
            "Epoch 418 -kfold 1-n_latent 4 Loss: -7163.1226\n",
            "Epoch 419 -kfold 1-n_latent 4 Loss: -7051.6036\n",
            "Epoch 420 -kfold 1-n_latent 4 Loss: -6922.5764\n",
            "Epoch 421 -kfold 1-n_latent 4 Loss: -6883.8427\n",
            "Epoch 422 -kfold 1-n_latent 4 Loss: -6939.1930\n",
            "Epoch 423 -kfold 1-n_latent 4 Loss: -6703.8825\n",
            "Epoch 424 -kfold 1-n_latent 4 Loss: -6218.2895\n",
            "Epoch 425 -kfold 1-n_latent 4 Loss: -6350.8217\n",
            "Epoch 426 -kfold 1-n_latent 4 Loss: -6730.8089\n",
            "Epoch 427 -kfold 1-n_latent 4 Loss: -7023.6673\n",
            "Epoch 428 -kfold 1-n_latent 4 Loss: -6993.5584\n",
            "Epoch 429 -kfold 1-n_latent 4 Loss: -7160.9373\n",
            "Epoch 430 -kfold 1-n_latent 4 Loss: -7334.5201\n",
            "Epoch 431 -kfold 1-n_latent 4 Loss: -7382.4655\n",
            "Epoch 432 -kfold 1-n_latent 4 Loss: -7561.1135\n",
            "Epoch 433 -kfold 1-n_latent 4 Loss: -7581.7482\n",
            "Epoch 434 -kfold 1-n_latent 4 Loss: -7450.0665\n",
            "Epoch 435 -kfold 1-n_latent 4 Loss: -7470.2221\n",
            "Epoch 436 -kfold 1-n_latent 4 Loss: -7408.1277\n",
            "Epoch 437 -kfold 1-n_latent 4 Loss: -7321.0012\n",
            "Epoch 438 -kfold 1-n_latent 4 Loss: -7396.8402\n",
            "Epoch 439 -kfold 1-n_latent 4 Loss: -7342.9435\n",
            "Epoch 440 -kfold 1-n_latent 4 Loss: -7395.4649\n",
            "Epoch 441 -kfold 1-n_latent 4 Loss: -7630.2294\n",
            "Epoch 442 -kfold 1-n_latent 4 Loss: -7626.5405\n",
            "Epoch 443 -kfold 1-n_latent 4 Loss: -7769.3680\n",
            "Epoch 444 -kfold 1-n_latent 4 Loss: -7643.4036\n",
            "Epoch 445 -kfold 1-n_latent 4 Loss: -7824.7685\n",
            "Epoch 446 -kfold 1-n_latent 4 Loss: -7774.3982\n",
            "Epoch 447 -kfold 1-n_latent 4 Loss: -7575.8687\n",
            "Epoch 448 -kfold 1-n_latent 4 Loss: -7380.3193\n",
            "Epoch 449 -kfold 1-n_latent 4 Loss: -7276.1011\n",
            "Epoch 450 -kfold 1-n_latent 4 Loss: -7193.9618\n",
            "Epoch 451 -kfold 1-n_latent 4 Loss: -7216.9564\n",
            "Epoch 452 -kfold 1-n_latent 4 Loss: -7237.0448\n",
            "Epoch 453 -kfold 1-n_latent 4 Loss: -7374.5939\n",
            "Epoch 454 -kfold 1-n_latent 4 Loss: -7190.3927\n",
            "Epoch 455 -kfold 1-n_latent 4 Loss: -7100.7194\n",
            "Epoch 456 -kfold 1-n_latent 4 Loss: -7078.0018\n",
            "Epoch 457 -kfold 1-n_latent 4 Loss: -7021.6238\n",
            "Epoch 458 -kfold 1-n_latent 4 Loss: -7273.0879\n",
            "Epoch 459 -kfold 1-n_latent 4 Loss: -7217.8977\n",
            "Epoch 460 -kfold 1-n_latent 4 Loss: -7030.2648\n",
            "Epoch 461 -kfold 1-n_latent 4 Loss: -7195.5990\n",
            "Epoch 462 -kfold 1-n_latent 4 Loss: -7035.0242\n",
            "Epoch 463 -kfold 1-n_latent 4 Loss: -7143.5564\n",
            "Epoch 464 -kfold 1-n_latent 4 Loss: -7018.0315\n",
            "Epoch 465 -kfold 1-n_latent 4 Loss: -6808.6480\n",
            "Epoch 466 -kfold 1-n_latent 4 Loss: -6793.9469\n",
            "Epoch 467 -kfold 1-n_latent 4 Loss: -7234.6026\n",
            "Epoch 468 -kfold 1-n_latent 4 Loss: -7234.3567\n",
            "Epoch 469 -kfold 1-n_latent 4 Loss: -7227.4893\n",
            "Epoch 470 -kfold 1-n_latent 4 Loss: -7362.5358\n",
            "Epoch 471 -kfold 1-n_latent 4 Loss: -7545.1613\n",
            "Epoch 472 -kfold 1-n_latent 4 Loss: -7725.2478\n",
            "Epoch 473 -kfold 1-n_latent 4 Loss: -7728.9996\n",
            "Epoch 474 -kfold 1-n_latent 4 Loss: -7813.2297\n",
            "Epoch 475 -kfold 1-n_latent 4 Loss: -7890.4440\n",
            "Epoch 476 -kfold 1-n_latent 4 Loss: -7799.5860\n",
            "Epoch 477 -kfold 1-n_latent 4 Loss: -7807.9041\n",
            "Epoch 478 -kfold 1-n_latent 4 Loss: -7412.9774\n",
            "Epoch 479 -kfold 1-n_latent 4 Loss: -7343.2862\n",
            "Epoch 480 -kfold 1-n_latent 4 Loss: -7461.6516\n",
            "Epoch 481 -kfold 1-n_latent 4 Loss: -7620.8402\n",
            "Epoch 482 -kfold 1-n_latent 4 Loss: -7052.4051\n",
            "Epoch 483 -kfold 1-n_latent 4 Loss: -6956.5839\n",
            "Epoch 484 -kfold 1-n_latent 4 Loss: -7559.0095\n",
            "Epoch 485 -kfold 1-n_latent 4 Loss: -7615.2010\n",
            "Epoch 486 -kfold 1-n_latent 4 Loss: -7648.5385\n",
            "Epoch 487 -kfold 1-n_latent 4 Loss: -7627.0798\n",
            "Epoch 488 -kfold 1-n_latent 4 Loss: -7659.9107\n",
            "Epoch 489 -kfold 1-n_latent 4 Loss: -7776.0370\n",
            "Epoch 490 -kfold 1-n_latent 4 Loss: -7982.2521\n",
            "Epoch 491 -kfold 1-n_latent 4 Loss: -7540.6303\n",
            "Epoch 492 -kfold 1-n_latent 4 Loss: -7076.4303\n",
            "Epoch 493 -kfold 1-n_latent 4 Loss: -7224.6879\n",
            "Epoch 494 -kfold 1-n_latent 4 Loss: -6854.4100\n",
            "Epoch 495 -kfold 1-n_latent 4 Loss: -6827.4095\n",
            "Epoch 496 -kfold 1-n_latent 4 Loss: -6986.9076\n",
            "Epoch 497 -kfold 1-n_latent 4 Loss: -7188.1617\n",
            "Epoch 498 -kfold 1-n_latent 4 Loss: -7189.3759\n",
            "Epoch 499 -kfold 1-n_latent 4 Loss: -7361.1185\n",
            "Epoch 500 -kfold 1-n_latent 4 Loss: -7527.5803\n",
            "Epoch 501 -kfold 1-n_latent 4 Loss: -7578.5693\n",
            "Epoch 502 -kfold 1-n_latent 4 Loss: -7655.8115\n",
            "Epoch 503 -kfold 1-n_latent 4 Loss: -7743.2778\n",
            "Epoch 504 -kfold 1-n_latent 4 Loss: -7711.7027\n",
            "Epoch 505 -kfold 1-n_latent 4 Loss: -7575.8905\n",
            "Epoch 506 -kfold 1-n_latent 4 Loss: -7063.4670\n",
            "Epoch 507 -kfold 1-n_latent 4 Loss: -6416.7259\n",
            "Epoch 508 -kfold 1-n_latent 4 Loss: -5871.3668\n",
            "Epoch 509 -kfold 1-n_latent 4 Loss: -6202.6540\n",
            "Epoch 510 -kfold 1-n_latent 4 Loss: -6558.6507\n",
            "Epoch 511 -kfold 1-n_latent 4 Loss: -6718.8488\n",
            "Epoch 512 -kfold 1-n_latent 4 Loss: -6711.5272\n",
            "Epoch 513 -kfold 1-n_latent 4 Loss: -6691.5747\n",
            "Epoch 514 -kfold 1-n_latent 4 Loss: -6857.2082\n",
            "Epoch 515 -kfold 1-n_latent 4 Loss: -7130.3332\n",
            "Epoch 516 -kfold 1-n_latent 4 Loss: -7311.5733\n",
            "Epoch 517 -kfold 1-n_latent 4 Loss: -7442.2513\n",
            "Epoch 518 -kfold 1-n_latent 4 Loss: -7605.2764\n",
            "Epoch 519 -kfold 1-n_latent 4 Loss: -7643.3461\n",
            "Epoch 520 -kfold 1-n_latent 4 Loss: -7667.9541\n",
            "Epoch 521 -kfold 1-n_latent 4 Loss: -7561.3483\n",
            "Epoch 522 -kfold 1-n_latent 4 Loss: -7167.2673\n",
            "Epoch 523 -kfold 1-n_latent 4 Loss: -6667.0885\n",
            "Epoch 524 -kfold 1-n_latent 4 Loss: -6665.4349\n",
            "Epoch 525 -kfold 1-n_latent 4 Loss: -5384.7694\n",
            "Epoch 526 -kfold 1-n_latent 4 Loss: -4141.1998\n",
            "Epoch 527 -kfold 1-n_latent 4 Loss: -4374.8845\n",
            "Epoch 528 -kfold 1-n_latent 4 Loss: -4858.7580\n",
            "Epoch 529 -kfold 1-n_latent 4 Loss: -5026.1160\n",
            "Epoch 530 -kfold 1-n_latent 4 Loss: -4866.1638\n",
            "Epoch 531 -kfold 1-n_latent 4 Loss: -4735.0167\n",
            "Epoch 532 -kfold 1-n_latent 4 Loss: -4647.2012\n",
            "Epoch 533 -kfold 1-n_latent 4 Loss: -4613.2332\n",
            "Epoch 534 -kfold 1-n_latent 4 Loss: -4685.1888\n",
            "Epoch 535 -kfold 1-n_latent 4 Loss: -4860.2216\n",
            "Epoch 536 -kfold 1-n_latent 4 Loss: -5197.7889\n",
            "Epoch 537 -kfold 1-n_latent 4 Loss: -5570.5867\n",
            "Epoch 538 -kfold 1-n_latent 4 Loss: -6153.8607\n",
            "Epoch 539 -kfold 1-n_latent 4 Loss: -6724.8659\n",
            "Epoch 540 -kfold 1-n_latent 4 Loss: -7159.5736\n",
            "Epoch 541 -kfold 1-n_latent 4 Loss: -7483.1793\n",
            "Epoch 542 -kfold 1-n_latent 4 Loss: -7753.5718\n",
            "Epoch 543 -kfold 1-n_latent 4 Loss: -7878.1871\n",
            "Epoch 544 -kfold 1-n_latent 4 Loss: -7662.8746\n",
            "Epoch 545 -kfold 1-n_latent 4 Loss: -7133.3825\n",
            "Epoch 546 -kfold 1-n_latent 4 Loss: -6754.5922\n",
            "Epoch 547 -kfold 1-n_latent 4 Loss: -6788.8186\n",
            "Epoch 548 -kfold 1-n_latent 4 Loss: -6624.4935\n",
            "Epoch 549 -kfold 1-n_latent 4 Loss: -7231.3092\n",
            "Epoch 550 -kfold 1-n_latent 4 Loss: -7338.4690\n",
            "Epoch 551 -kfold 1-n_latent 4 Loss: -7472.5096\n",
            "Epoch 552 -kfold 1-n_latent 4 Loss: -7395.8207\n",
            "Epoch 553 -kfold 1-n_latent 4 Loss: -7363.0331\n",
            "Epoch 554 -kfold 1-n_latent 4 Loss: -7547.4511\n",
            "Epoch 555 -kfold 1-n_latent 4 Loss: -7660.1779\n",
            "Epoch 556 -kfold 1-n_latent 4 Loss: -7712.9620\n",
            "Epoch 557 -kfold 1-n_latent 4 Loss: -7800.8029\n",
            "Epoch 558 -kfold 1-n_latent 4 Loss: -7962.7302\n",
            "Epoch 559 -kfold 1-n_latent 4 Loss: -8127.1854\n",
            "Epoch 560 -kfold 1-n_latent 4 Loss: -8072.2985\n",
            "Epoch 561 -kfold 1-n_latent 4 Loss: -7950.4570\n",
            "Epoch 562 -kfold 1-n_latent 4 Loss: -7604.3674\n",
            "Epoch 563 -kfold 1-n_latent 4 Loss: -7294.2683\n",
            "Epoch 564 -kfold 1-n_latent 4 Loss: -7080.4648\n",
            "Epoch 565 -kfold 1-n_latent 4 Loss: -7102.5282\n",
            "Epoch 566 -kfold 1-n_latent 4 Loss: -6267.3557\n",
            "Epoch 567 -kfold 1-n_latent 4 Loss: -6298.5865\n",
            "Epoch 568 -kfold 1-n_latent 4 Loss: -6499.4621\n",
            "Epoch 569 -kfold 1-n_latent 4 Loss: -6595.2913\n",
            "Epoch 570 -kfold 1-n_latent 4 Loss: -6438.5369\n",
            "Epoch 571 -kfold 1-n_latent 4 Loss: -6487.5967\n",
            "Epoch 572 -kfold 1-n_latent 4 Loss: -6507.1990\n",
            "Epoch 573 -kfold 1-n_latent 4 Loss: -6283.2624\n",
            "Epoch 574 -kfold 1-n_latent 4 Loss: -6350.0057\n",
            "Epoch 575 -kfold 1-n_latent 4 Loss: -6435.8249\n",
            "Epoch 576 -kfold 1-n_latent 4 Loss: -6453.7361\n",
            "Epoch 577 -kfold 1-n_latent 4 Loss: -6500.9948\n",
            "Epoch 578 -kfold 1-n_latent 4 Loss: -6676.9376\n",
            "Epoch 579 -kfold 1-n_latent 4 Loss: -6986.8133\n",
            "Epoch 580 -kfold 1-n_latent 4 Loss: -7017.5807\n",
            "Epoch 581 -kfold 1-n_latent 4 Loss: -7327.5274\n",
            "Epoch 582 -kfold 1-n_latent 4 Loss: -7474.1613\n",
            "Epoch 583 -kfold 1-n_latent 4 Loss: -7576.1027\n",
            "Epoch 584 -kfold 1-n_latent 4 Loss: -7548.1211\n",
            "Epoch 585 -kfold 1-n_latent 4 Loss: -7745.3021\n",
            "Epoch 586 -kfold 1-n_latent 4 Loss: -7598.0937\n",
            "Epoch 587 -kfold 1-n_latent 4 Loss: -7646.3295\n",
            "Epoch 588 -kfold 1-n_latent 4 Loss: -7596.7604\n",
            "Epoch 589 -kfold 1-n_latent 4 Loss: -7495.8403\n",
            "Epoch 590 -kfold 1-n_latent 4 Loss: -7475.6297\n",
            "Epoch 591 -kfold 1-n_latent 4 Loss: -7476.8280\n",
            "Epoch 592 -kfold 1-n_latent 4 Loss: -7557.5124\n",
            "Epoch 593 -kfold 1-n_latent 4 Loss: -7415.9993\n",
            "Epoch 594 -kfold 1-n_latent 4 Loss: -7641.5073\n",
            "Epoch 595 -kfold 1-n_latent 4 Loss: -7823.9284\n",
            "Epoch 596 -kfold 1-n_latent 4 Loss: -7925.0259\n",
            "Epoch 597 -kfold 1-n_latent 4 Loss: -7919.8773\n",
            "Epoch 598 -kfold 1-n_latent 4 Loss: -8132.1489\n",
            "Epoch 599 -kfold 1-n_latent 4 Loss: -8205.8746\n",
            "Epoch 600 -kfold 1-n_latent 4 Loss: -8080.7019\n",
            "Epoch 601 -kfold 1-n_latent 4 Loss: -8014.2937\n",
            "Epoch 602 -kfold 1-n_latent 4 Loss: -7691.7646\n",
            "Epoch 603 -kfold 1-n_latent 4 Loss: -7589.0988\n",
            "Epoch 604 -kfold 1-n_latent 4 Loss: -7573.8628\n",
            "Epoch 605 -kfold 1-n_latent 4 Loss: -7122.6790\n",
            "Epoch 606 -kfold 1-n_latent 4 Loss: -7251.6651\n",
            "Epoch 607 -kfold 1-n_latent 4 Loss: -7561.5287\n",
            "Epoch 608 -kfold 1-n_latent 4 Loss: -7539.1950\n",
            "Epoch 609 -kfold 1-n_latent 4 Loss: -7533.9039\n",
            "Epoch 610 -kfold 1-n_latent 4 Loss: -7565.6107\n",
            "Epoch 611 -kfold 1-n_latent 4 Loss: -7721.0300\n",
            "Epoch 612 -kfold 1-n_latent 4 Loss: -7305.3534\n",
            "Epoch 613 -kfold 1-n_latent 4 Loss: -7211.4867\n",
            "Epoch 614 -kfold 1-n_latent 4 Loss: -7659.2121\n",
            "Epoch 615 -kfold 1-n_latent 4 Loss: -7783.5199\n",
            "Epoch 616 -kfold 1-n_latent 4 Loss: -7656.1426\n",
            "Epoch 617 -kfold 1-n_latent 4 Loss: -7618.3718\n",
            "Epoch 618 -kfold 1-n_latent 4 Loss: -7431.7671\n",
            "Epoch 619 -kfold 1-n_latent 4 Loss: -7276.9602\n",
            "Epoch 620 -kfold 1-n_latent 4 Loss: -7172.7750\n",
            "Epoch 621 -kfold 1-n_latent 4 Loss: -7188.9390\n",
            "Epoch 622 -kfold 1-n_latent 4 Loss: -7229.8752\n",
            "Epoch 623 -kfold 1-n_latent 4 Loss: -7286.2934\n",
            "Epoch 624 -kfold 1-n_latent 4 Loss: -7332.1996\n",
            "Epoch 625 -kfold 1-n_latent 4 Loss: -7493.7165\n",
            "Epoch 626 -kfold 1-n_latent 4 Loss: -7636.5201\n",
            "Epoch 627 -kfold 1-n_latent 4 Loss: -7655.4424\n",
            "Epoch 628 -kfold 1-n_latent 4 Loss: -7765.9734\n",
            "Epoch 629 -kfold 1-n_latent 4 Loss: -7790.7214\n",
            "Epoch 630 -kfold 1-n_latent 4 Loss: -7649.6155\n",
            "Epoch 631 -kfold 1-n_latent 4 Loss: -7491.8355\n",
            "Epoch 632 -kfold 1-n_latent 4 Loss: -7308.0470\n",
            "Epoch 633 -kfold 1-n_latent 4 Loss: -7094.4047\n",
            "Epoch 634 -kfold 1-n_latent 4 Loss: -7176.7466\n",
            "Epoch 635 -kfold 1-n_latent 4 Loss: -7568.8891\n",
            "Epoch 636 -kfold 1-n_latent 4 Loss: -7452.8419\n",
            "Epoch 637 -kfold 1-n_latent 4 Loss: -7482.2492\n",
            "Epoch 638 -kfold 1-n_latent 4 Loss: -7670.7715\n",
            "Epoch 639 -kfold 1-n_latent 4 Loss: -7583.0085\n",
            "Epoch 640 -kfold 1-n_latent 4 Loss: -7684.1158\n",
            "Epoch 641 -kfold 1-n_latent 4 Loss: -7968.2432\n",
            "Epoch 642 -kfold 1-n_latent 4 Loss: -7870.6705\n",
            "Epoch 643 -kfold 1-n_latent 4 Loss: -7795.9960\n",
            "Epoch 644 -kfold 1-n_latent 4 Loss: -7604.5021\n",
            "Epoch 645 -kfold 1-n_latent 4 Loss: -7449.8958\n",
            "Epoch 646 -kfold 1-n_latent 4 Loss: -7173.2363\n",
            "Epoch 647 -kfold 1-n_latent 4 Loss: -7285.3275\n",
            "Epoch 648 -kfold 1-n_latent 4 Loss: -7498.3227\n",
            "Epoch 649 -kfold 1-n_latent 4 Loss: -7640.2992\n",
            "Epoch 650 -kfold 1-n_latent 4 Loss: -7811.1549\n",
            "Epoch 651 -kfold 1-n_latent 4 Loss: -7890.5396\n",
            "Epoch 652 -kfold 1-n_latent 4 Loss: -7924.5490\n",
            "Epoch 653 -kfold 1-n_latent 4 Loss: -8124.8919\n",
            "Epoch 654 -kfold 1-n_latent 4 Loss: -8044.4221\n",
            "Epoch 655 -kfold 1-n_latent 4 Loss: -8142.7560\n",
            "Epoch 656 -kfold 1-n_latent 4 Loss: -7929.3016\n",
            "Epoch 657 -kfold 1-n_latent 4 Loss: -7455.2007\n",
            "Epoch 658 -kfold 1-n_latent 4 Loss: -6890.9207\n",
            "Epoch 659 -kfold 1-n_latent 4 Loss: -6686.9930\n",
            "Epoch 660 -kfold 1-n_latent 4 Loss: -6707.3409\n",
            "Epoch 661 -kfold 1-n_latent 4 Loss: -7151.5891\n",
            "Epoch 662 -kfold 1-n_latent 4 Loss: -7306.8776\n",
            "Epoch 663 -kfold 1-n_latent 4 Loss: -7377.9571\n",
            "Epoch 664 -kfold 1-n_latent 4 Loss: -7589.4394\n",
            "Epoch 665 -kfold 1-n_latent 4 Loss: -7626.6252\n",
            "Epoch 666 -kfold 1-n_latent 4 Loss: -7632.0153\n",
            "Epoch 667 -kfold 1-n_latent 4 Loss: -7827.5312\n",
            "Epoch 668 -kfold 1-n_latent 4 Loss: -7717.9632\n",
            "Epoch 669 -kfold 1-n_latent 4 Loss: -7623.7933\n",
            "Epoch 670 -kfold 1-n_latent 4 Loss: -7276.5496\n",
            "Epoch 671 -kfold 1-n_latent 4 Loss: -7082.9103\n",
            "Epoch 672 -kfold 1-n_latent 4 Loss: -7183.4277\n",
            "Epoch 673 -kfold 1-n_latent 4 Loss: -7110.7150\n",
            "Epoch 674 -kfold 1-n_latent 4 Loss: -7329.4845\n",
            "Epoch 675 -kfold 1-n_latent 4 Loss: -7134.8331\n",
            "Epoch 676 -kfold 1-n_latent 4 Loss: -6862.2577\n",
            "Epoch 677 -kfold 1-n_latent 4 Loss: -7161.2875\n",
            "Epoch 678 -kfold 1-n_latent 4 Loss: -7503.0541\n",
            "Epoch 679 -kfold 1-n_latent 4 Loss: -7677.2149\n",
            "Epoch 680 -kfold 1-n_latent 4 Loss: -7778.5515\n",
            "Epoch 681 -kfold 1-n_latent 4 Loss: -7781.5361\n",
            "Epoch 682 -kfold 1-n_latent 4 Loss: -7817.7374\n",
            "Epoch 683 -kfold 1-n_latent 4 Loss: -7778.3343\n",
            "Epoch 684 -kfold 1-n_latent 4 Loss: -7777.7889\n",
            "Epoch 685 -kfold 1-n_latent 4 Loss: -7798.9396\n",
            "Epoch 686 -kfold 1-n_latent 4 Loss: -7754.0616\n",
            "Epoch 687 -kfold 1-n_latent 4 Loss: -7712.2218\n",
            "Epoch 688 -kfold 1-n_latent 4 Loss: -7899.5368\n",
            "Epoch 689 -kfold 1-n_latent 4 Loss: -7935.3653\n",
            "Epoch 690 -kfold 1-n_latent 4 Loss: -8055.0676\n",
            "Epoch 691 -kfold 1-n_latent 4 Loss: -7958.3423\n",
            "Epoch 692 -kfold 1-n_latent 4 Loss: -8103.2838\n",
            "Epoch 693 -kfold 1-n_latent 4 Loss: -8046.9840\n",
            "Epoch 694 -kfold 1-n_latent 4 Loss: -8109.2692\n",
            "Epoch 695 -kfold 1-n_latent 4 Loss: -8035.5367\n",
            "Epoch 696 -kfold 1-n_latent 4 Loss: -8019.1063\n",
            "Epoch 697 -kfold 1-n_latent 4 Loss: -7954.9485\n",
            "Epoch 698 -kfold 1-n_latent 4 Loss: -7826.9696\n",
            "Epoch 699 -kfold 1-n_latent 4 Loss: -7904.0017\n",
            "5\n",
            "Neural: Sparse / Behavioral: Dense\n",
            "Epoch 1 -kfold 1-n_latent 5 Loss:  24407.9027\n",
            "Epoch 2 -kfold 1-n_latent 5 Loss:  22700.1493\n",
            "Epoch 3 -kfold 1-n_latent 5 Loss:  21185.5744\n",
            "Epoch 4 -kfold 1-n_latent 5 Loss:  19691.8150\n",
            "Epoch 5 -kfold 1-n_latent 5 Loss:  18192.9206\n",
            "Epoch 6 -kfold 1-n_latent 5 Loss:  16692.1744\n",
            "Epoch 7 -kfold 1-n_latent 5 Loss:  15207.6687\n",
            "Epoch 8 -kfold 1-n_latent 5 Loss:  13797.1545\n",
            "Epoch 9 -kfold 1-n_latent 5 Loss:  12482.0558\n",
            "Epoch 10 -kfold 1-n_latent 5 Loss:  11258.2460\n",
            "Epoch 11 -kfold 1-n_latent 5 Loss:  10116.1089\n",
            "Epoch 12 -kfold 1-n_latent 5 Loss:  9057.1377\n",
            "Epoch 13 -kfold 1-n_latent 5 Loss:  8117.3039\n",
            "Epoch 14 -kfold 1-n_latent 5 Loss:  7342.2227\n",
            "Epoch 15 -kfold 1-n_latent 5 Loss:  6712.0690\n",
            "Epoch 16 -kfold 1-n_latent 5 Loss:  6164.2822\n",
            "Epoch 17 -kfold 1-n_latent 5 Loss:  5607.9523\n",
            "Epoch 18 -kfold 1-n_latent 5 Loss:  5018.8059\n",
            "Epoch 19 -kfold 1-n_latent 5 Loss:  4368.5078\n",
            "Epoch 20 -kfold 1-n_latent 5 Loss:  3830.3583\n",
            "Epoch 21 -kfold 1-n_latent 5 Loss:  3243.5046\n",
            "Epoch 22 -kfold 1-n_latent 5 Loss:  2772.9623\n",
            "Epoch 23 -kfold 1-n_latent 5 Loss:  2266.4972\n",
            "Epoch 24 -kfold 1-n_latent 5 Loss:  1733.6951\n",
            "Epoch 25 -kfold 1-n_latent 5 Loss:  1262.5931\n",
            "Epoch 26 -kfold 1-n_latent 5 Loss:  788.9443\n",
            "Epoch 27 -kfold 1-n_latent 5 Loss:  296.9486\n",
            "Epoch 28 -kfold 1-n_latent 5 Loss: -136.9184\n",
            "Epoch 29 -kfold 1-n_latent 5 Loss: -524.1743\n",
            "Epoch 30 -kfold 1-n_latent 5 Loss: -903.3636\n",
            "Epoch 31 -kfold 1-n_latent 5 Loss: -1281.1550\n",
            "Epoch 32 -kfold 1-n_latent 5 Loss: -1605.2413\n",
            "Epoch 33 -kfold 1-n_latent 5 Loss: -1968.5658\n",
            "Epoch 34 -kfold 1-n_latent 5 Loss: -2268.7151\n",
            "Epoch 35 -kfold 1-n_latent 5 Loss: -2642.3439\n",
            "Epoch 36 -kfold 1-n_latent 5 Loss: -2927.0919\n",
            "Epoch 37 -kfold 1-n_latent 5 Loss: -3215.5360\n",
            "Epoch 38 -kfold 1-n_latent 5 Loss: -3403.2083\n",
            "Epoch 39 -kfold 1-n_latent 5 Loss: -3616.0231\n",
            "Epoch 40 -kfold 1-n_latent 5 Loss: -3840.3764\n",
            "Epoch 41 -kfold 1-n_latent 5 Loss: -4007.1837\n",
            "Epoch 42 -kfold 1-n_latent 5 Loss: -4148.5760\n",
            "Epoch 43 -kfold 1-n_latent 5 Loss: -4326.4256\n"
          ]
        }
      ],
      "source": [
        "#Optimización del modelo usando descenso por gradiente\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from numpy.linalg import inv, det\n",
        "import math as math\n",
        "\n",
        "#número de iteraciones\n",
        "max_iter = 700\n",
        "record_step = 20\n",
        "\n",
        "#Validación cruzada n_splits-fold\n",
        "n_splits=10\n",
        "latentes=7\n",
        "kf = KFold(n_splits=n_splits,shuffle=True, random_state=1)\n",
        "\n",
        "kf.get_n_splits(np.arange(1567))\n",
        "j=0\n",
        "ECM=np.zeros((n_splits,latentes-1))\n",
        "#ECM2=np.zeros((n_splits,latentes-1))\n",
        "log=np.zeros((n_splits,latentes-1))\n",
        "#log2=np.zeros((n_splits,latentes-1))\n",
        "\n",
        "#Ciclo para ajustar el modelo para cada muestra de la validación cruzada y para distintas dimensiones de la variable latente\n",
        "for train_index, test_index in kf.split(np.arange(1567)):\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  tsd_train, tsd_test = ts_dense[train_index,:], ts_dense[test_index,:]\n",
        "  tss_train, tss_test = ts_dense[train_index[train_index % 2 == 0],:], ts_dense[test_index[test_index % 2 == 0] ,:]\n",
        "  for n_latent in range(1,latentes):\n",
        "    print(n_latent)#Agregamos datos al modelo\n",
        "    r=n_latent-1\n",
        "    Y_N1_train=Y_N1_es[tss_train.astype(int)[:,0]]\n",
        "    Y_B1_train=Y_B1_es[train_index, :]\n",
        "    Y_N1_test=Y_N1_es[tss_test.astype(int)[:,0]]\n",
        "    Y_B1_test=Y_B1_es[test_index, :]\n",
        "    test= GPJMv3(Y_N1_train ,Y_B1_train, tss_train, tsd_train, n_latent, ss)\n",
        "    #test.likelihood_tX.variance = 1e-6\n",
        "    #gpflow.set_trainable(test.likelihood_tX, False)\n",
        "    \n",
        "    @tf.function\n",
        "    def optimisation_step():\n",
        "      adam_opt.minimize(test.training_loss, test.trainable_variables)\n",
        "\n",
        "\n",
        "    X_storage = np.zeros((test.X.shape[0], test.X.shape[1], (max_iter+1)))\n",
        "    llk_storage = np.zeros(max_iter+1)\n",
        "    learning_rate_vec_temp = 0.25/np.sqrt(np.arange(max_iter)+1)\n",
        "    learning_rate_vec = np.zeros_like(learning_rate_vec_temp)\n",
        "    for i in range(len(learning_rate_vec_temp)):\n",
        "      learning_rate_vec[i] = learning_rate_vec_temp[int(np.floor(i/record_step))] \n",
        "\n",
        "    for iter in range(max_iter):\n",
        "      l = learning_rate_vec[iter]\n",
        "      adam_opt = tf.optimizers.Adam(l)\n",
        "      optimisation_step()\n",
        "\n",
        "      if max_iter % record_step== 0 and iter > 0:\n",
        "        print(f\"Epoch {iter} -kfold {j}-n_latent {n_latent} Loss: {test.training_loss().numpy() : .4f}\")\n",
        "\n",
        "\n",
        "    #Calculo de la log-probabilidad\n",
        "\n",
        "    yhat_arr, yhat_v, yhat_sd = recover_neural(test, tss_test)\n",
        "    bmean, bV, bci = recover_behavioral(test, tsd_test)\n",
        "    \n",
        "    #calculo del error cuadrático médio\n",
        "    k1=len(tss_test)\n",
        "    k2=len(tsd_test)\n",
        "    k3=16\n",
        "    ECM[j,r]=(np.sum((yhat_arr-Y_N1_test)**2)+np.sum((bmean-Y_B1_test)**2))/(k1*k3+k2)\n",
        "   \n",
        "    #GUardamos los errores en una tabla\n",
        "    with open('ECM', 'wb') as output:\n",
        "      pickle.dump(ECM, output)\n",
        "    lxn=tf.linalg.cholesky(bV)\n",
        "    log2=gpflow.logdensities.multivariate_normal(Y_B1_test, bmean, lxn).numpy()\n",
        "    lxn=tf.linalg.cholesky(yhat_v)\n",
        "    log3=np.sum(gpflow.logdensities.multivariate_normal(yhat_arr,Y_N1_test, lxn).numpy())\n",
        "    log[j,r]=log2+log3\n",
        "    with open('LOG', 'wb') as output:\n",
        "      pickle.dump(log, output)\n",
        "\n",
        "\n",
        "  j=j+1\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}